WARNING:tensorflow:From src/translate.py:701: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.

Reading training data (seq_len_in: 50, seq_len_out 10).
Reading subject 1, action walking, subaction 1
Reading subject 1, action walking, subaction 2
Reading subject 6, action walking, subaction 1
Reading subject 6, action walking, subaction 2
Reading subject 7, action walking, subaction 1
Reading subject 7, action walking, subaction 2
Reading subject 8, action walking, subaction 1
Reading subject 8, action walking, subaction 2
Reading subject 9, action walking, subaction 1
Reading subject 9, action walking, subaction 2
Reading subject 11, action walking, subaction 1
Reading subject 11, action walking, subaction 2
Reading subject 5, action walking, subaction 1
Reading subject 5, action walking, subaction 2
done reading data.
WARNING:tensorflow:From src/translate.py:124: The name tf.GPUOptions is deprecated. Please use tf.compat.v1.GPUOptions instead.

W1130 12:53:34.821850 140668203612032 module_wrapper.py:139] From src/translate.py:124: The name tf.GPUOptions is deprecated. Please use tf.compat.v1.GPUOptions instead.

WARNING:tensorflow:From src/translate.py:127: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.

W1130 12:53:34.822172 140668203612032 module_wrapper.py:139] From src/translate.py:127: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.

WARNING:tensorflow:From src/translate.py:127: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.

W1130 12:53:34.822342 140668203612032 module_wrapper.py:139] From src/translate.py:127: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.

2019-11-30 12:53:34.827794: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2200000000 Hz
2019-11-30 12:53:34.827967: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x27bf100 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2019-11-30 12:53:34.827995: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2019-11-30 12:53:34.829801: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2019-11-30 12:53:34.923740: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-11-30 12:53:34.924465: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x27bf9c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2019-11-30 12:53:34.924494: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0
2019-11-30 12:53:34.924668: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-11-30 12:53:34.925181: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285
pciBusID: 0000:00:04.0
2019-11-30 12:53:34.925514: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2019-11-30 12:53:34.927012: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2019-11-30 12:53:34.928601: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2019-11-30 12:53:34.928933: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2019-11-30 12:53:34.930464: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2019-11-30 12:53:34.931125: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2019-11-30 12:53:34.934104: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2019-11-30 12:53:34.934234: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-11-30 12:53:34.934775: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-11-30 12:53:34.935256: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2019-11-30 12:53:34.935321: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2019-11-30 12:53:34.936448: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-11-30 12:53:34.936474: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 
2019-11-30 12:53:34.936485: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N 
2019-11-30 12:53:34.936596: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-11-30 12:53:34.937133: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-11-30 12:53:34.937632: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2019-11-30 12:53:34.937672: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 16280 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)
Creating 1 layers of 1024 units.
One hot is  True
Input size is 55
WARNING:tensorflow:From /content/ml2test/human-motion-prediction/src/seq2seq_model.py:70: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.

W1130 12:53:34.938601 140668203612032 module_wrapper.py:139] From /content/ml2test/human-motion-prediction/src/seq2seq_model.py:70: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.

rnn_size = 1024
WARNING:tensorflow:
The TensorFlow contrib module will not be included in TensorFlow 2.0.
For more information, please see:
  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md
  * https://github.com/tensorflow/addons
  * https://github.com/tensorflow/io (for I/O related ops)
If you depend on functionality not listed there, please file an issue.

W1130 12:53:34.947603 140668203612032 lazy_loader.py:50] 
The TensorFlow contrib module will not be included in TensorFlow 2.0.
For more information, please see:
  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md
  * https://github.com/tensorflow/addons
  * https://github.com/tensorflow/io (for I/O related ops)
If you depend on functionality not listed there, please file an issue.

WARNING:tensorflow:From /content/ml2test/human-motion-prediction/src/seq2seq_model.py:83: GRUCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.
Instructions for updating:
This class is equivalent as tf.keras.layers.GRUCell, and will be replaced by that in Tensorflow 2.0.
W1130 12:53:34.947799 140668203612032 deprecation.py:323] From /content/ml2test/human-motion-prediction/src/seq2seq_model.py:83: GRUCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.
Instructions for updating:
This class is equivalent as tf.keras.layers.GRUCell, and will be replaced by that in Tensorflow 2.0.
WARNING:tensorflow:From /content/ml2test/human-motion-prediction/src/seq2seq_model.py:91: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

W1130 12:53:34.948817 140668203612032 module_wrapper.py:139] From /content/ml2test/human-motion-prediction/src/seq2seq_model.py:91: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

output_size = 55
 state_size = 1024
WARNING:tensorflow:From /content/ml2test/human-motion-prediction/src/rnn_cell_extensions.py:95: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.

W1130 12:53:34.959232 140668203612032 module_wrapper.py:139] From /content/ml2test/human-motion-prediction/src/rnn_cell_extensions.py:95: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.

WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/legacy_seq2seq/python/ops/seq2seq.py:223: static_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell, unroll=True)`, which is equivalent to this API
W1130 12:53:34.972216 140668203612032 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/legacy_seq2seq/python/ops/seq2seq.py:223: static_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell, unroll=True)`, which is equivalent to this API
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/rnn_cell_impl.py:559: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `layer.add_weight` method instead.
W1130 12:53:34.981517 140668203612032 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/rnn_cell_impl.py:559: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `layer.add_weight` method instead.
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/rnn_cell_impl.py:565: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W1130 12:53:34.987687 140668203612032 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/rnn_cell_impl.py:565: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/rnn_cell_impl.py:575: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W1130 12:53:34.996054 140668203612032 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/rnn_cell_impl.py:575: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:From /content/ml2test/human-motion-prediction/src/seq2seq_model.py:148: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.

W1130 12:53:35.632881 140668203612032 module_wrapper.py:139] From /content/ml2test/human-motion-prediction/src/seq2seq_model.py:148: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.

WARNING:tensorflow:From /content/ml2test/human-motion-prediction/src/seq2seq_model.py:151: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.

W1130 12:53:35.634181 140668203612032 module_wrapper.py:139] From /content/ml2test/human-motion-prediction/src/seq2seq_model.py:151: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.

WARNING:tensorflow:From /content/ml2test/human-motion-prediction/src/seq2seq_model.py:153: The name tf.train.GradientDescentOptimizer is deprecated. Please use tf.compat.v1.train.GradientDescentOptimizer instead.

W1130 12:53:35.634429 140668203612032 module_wrapper.py:139] From /content/ml2test/human-motion-prediction/src/seq2seq_model.py:153: The name tf.train.GradientDescentOptimizer is deprecated. Please use tf.compat.v1.train.GradientDescentOptimizer instead.

WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/clip_ops.py:301: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
W1130 12:53:37.878907 140668203612032 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/clip_ops.py:301: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
WARNING:tensorflow:From /content/ml2test/human-motion-prediction/src/seq2seq_model.py:378: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.

W1130 12:53:37.993997 140668203612032 module_wrapper.py:139] From /content/ml2test/human-motion-prediction/src/seq2seq_model.py:378: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.

WARNING:tensorflow:From /content/ml2test/human-motion-prediction/src/seq2seq_model.py:378: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.

W1130 12:53:37.994208 140668203612032 module_wrapper.py:139] From /content/ml2test/human-motion-prediction/src/seq2seq_model.py:378: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.

Creating model with fresh parameters.
WARNING:tensorflow:From src/translate.py:87: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.

W1130 12:53:38.004201 140668203612032 module_wrapper.py:139] From src/translate.py:87: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.

Model created
2019-11-30 12:53:40.125547: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
step 0000; step_loss: 2.1196
step 0010; step_loss: 1.0557
step 0020; step_loss: 0.7806
step 0030; step_loss: 0.6066
step 0040; step_loss: 0.5081
step 0050; step_loss: 0.4449
step 0060; step_loss: 0.5448
step 0070; step_loss: 0.3731
step 0080; step_loss: 0.3831
step 0090; step_loss: 0.3308
step 0100; step_loss: 0.3452
step 0110; step_loss: 0.2831
step 0120; step_loss: 0.3281
step 0130; step_loss: 0.3514
step 0140; step_loss: 0.3455
step 0150; step_loss: 0.2545
step 0160; step_loss: 0.2663
step 0170; step_loss: 0.2396
step 0180; step_loss: 0.2277
step 0190; step_loss: 0.2228
step 0200; step_loss: 0.2373
step 0210; step_loss: 0.2107
step 0220; step_loss: 0.2309
step 0230; step_loss: 0.2886
step 0240; step_loss: 0.1922
step 0250; step_loss: 0.2501
step 0260; step_loss: 0.2407
step 0270; step_loss: 0.2669
step 0280; step_loss: 0.2281
step 0290; step_loss: 0.2958
step 0300; step_loss: 0.2927
step 0310; step_loss: 0.1719
step 0320; step_loss: 0.2760
step 0330; step_loss: 0.2435
step 0340; step_loss: 0.2352
step 0350; step_loss: 0.2488
step 0360; step_loss: 0.2787
step 0370; step_loss: 0.1888
step 0380; step_loss: 0.1795
step 0390; step_loss: 0.2471
step 0400; step_loss: 0.3367
step 0410; step_loss: 0.1718
step 0420; step_loss: 0.2037
step 0430; step_loss: 0.3368
step 0440; step_loss: 0.2461
step 0450; step_loss: 0.2343
step 0460; step_loss: 0.2259
step 0470; step_loss: 0.2447
step 0480; step_loss: 0.2295
step 0490; step_loss: 0.2157
step 0500; step_loss: 0.2531
step 0510; step_loss: 0.2374
step 0520; step_loss: 0.1977
step 0530; step_loss: 0.2020
step 0540; step_loss: 0.2588
step 0550; step_loss: 0.2141
step 0560; step_loss: 0.1773
step 0570; step_loss: 0.2473
step 0580; step_loss: 0.2038
step 0590; step_loss: 0.2293
step 0600; step_loss: 0.2828
step 0610; step_loss: 0.2524
step 0620; step_loss: 0.2123
step 0630; step_loss: 0.1599
step 0640; step_loss: 0.2059
step 0650; step_loss: 0.1573
step 0660; step_loss: 0.1357
step 0670; step_loss: 0.1592
step 0680; step_loss: 0.2736
step 0690; step_loss: 0.2591
step 0700; step_loss: 0.1675
step 0710; step_loss: 0.1629
step 0720; step_loss: 0.1999
step 0730; step_loss: 0.2707
step 0740; step_loss: 0.2187
step 0750; step_loss: 0.1642
step 0760; step_loss: 0.1523
step 0770; step_loss: 0.1943
step 0780; step_loss: 0.1357
step 0790; step_loss: 0.3323
step 0800; step_loss: 0.2127
step 0810; step_loss: 0.1517
step 0820; step_loss: 0.1677
step 0830; step_loss: 0.2114
step 0840; step_loss: 0.1878
step 0850; step_loss: 0.1258
step 0860; step_loss: 0.1843
step 0870; step_loss: 0.2227
step 0880; step_loss: 0.2916
step 0890; step_loss: 0.1690
step 0900; step_loss: 0.2286
step 0910; step_loss: 0.2029
step 0920; step_loss: 0.2096
step 0930; step_loss: 0.2217
step 0940; step_loss: 0.2051
step 0950; step_loss: 0.1458
step 0960; step_loss: 0.1449
step 0970; step_loss: 0.2069
step 0980; step_loss: 0.2701
step 0990; step_loss: 0.1802

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 0.374 | 0.660 | 1.007 | 1.141 |   n/a |   n/a |

============================
Global step:         1000
Learning rate:       0.0050
Step-time (ms):     28.6207
Train loss avg:      0.2728
--------------------------
Val loss:            0.3827
srnn loss:           0.3195
============================

Saving the model...
done in 302.56 ms
step 1000; step_loss: 0.2242
step 1010; step_loss: 0.1806
step 1020; step_loss: 0.3066
step 1030; step_loss: 0.2262
step 1040; step_loss: 0.1616
step 1050; step_loss: 0.1974
step 1060; step_loss: 0.2520
step 1070; step_loss: 0.1391
step 1080; step_loss: 0.2192
step 1090; step_loss: 0.1686
step 1100; step_loss: 0.1710
step 1110; step_loss: 0.1842
step 1120; step_loss: 0.1798
step 1130; step_loss: 0.2211
step 1140; step_loss: 0.1462
step 1150; step_loss: 0.2445
step 1160; step_loss: 0.2072
step 1170; step_loss: 0.2790
step 1180; step_loss: 0.1759
step 1190; step_loss: 0.1570
step 1200; step_loss: 0.1717
step 1210; step_loss: 0.1724
step 1220; step_loss: 0.1435
step 1230; step_loss: 0.2186
step 1240; step_loss: 0.1579
step 1250; step_loss: 0.1389
step 1260; step_loss: 0.1545
step 1270; step_loss: 0.2554
step 1280; step_loss: 0.4830
step 1290; step_loss: 0.2022
step 1300; step_loss: 0.2624
step 1310; step_loss: 0.4373
step 1320; step_loss: 0.1873
step 1330; step_loss: 0.1343
step 1340; step_loss: 0.2690
step 1350; step_loss: 0.2087
step 1360; step_loss: 0.1381
step 1370; step_loss: 0.1848
step 1380; step_loss: 0.1305
step 1390; step_loss: 0.2288
step 1400; step_loss: 0.1666
step 1410; step_loss: 0.2338
step 1420; step_loss: 0.1500
step 1430; step_loss: 0.2194
step 1440; step_loss: 0.1718
step 1450; step_loss: 0.1933
step 1460; step_loss: 0.1544
step 1470; step_loss: 0.1681
step 1480; step_loss: 0.2175
step 1490; step_loss: 0.1841
step 1500; step_loss: 0.1806
step 1510; step_loss: 0.1975
step 1520; step_loss: 0.1556
step 1530; step_loss: 0.1861
step 1540; step_loss: 0.1618
step 1550; step_loss: 0.1698
step 1560; step_loss: 0.2124
step 1570; step_loss: 0.1517
step 1580; step_loss: 0.2531
step 1590; step_loss: 0.2007
step 1600; step_loss: 0.2072
step 1610; step_loss: 0.1761
step 1620; step_loss: 0.3101
step 1630; step_loss: 0.2397
step 1640; step_loss: 0.5784
step 1650; step_loss: 0.1379
step 1660; step_loss: 0.1516
step 1670; step_loss: 0.1530
step 1680; step_loss: 0.1548
step 1690; step_loss: 0.1723
step 1700; step_loss: 0.1495
step 1710; step_loss: 0.1393
step 1720; step_loss: 0.1772
step 1730; step_loss: 0.1755
step 1740; step_loss: 0.1682
step 1750; step_loss: 0.3008
step 1760; step_loss: 0.1767
step 1770; step_loss: 0.1305
step 1780; step_loss: 0.2643
step 1790; step_loss: 0.1696
step 1800; step_loss: 0.1415
step 1810; step_loss: 0.2436
step 1820; step_loss: 0.1613
step 1830; step_loss: 0.1934
step 1840; step_loss: 0.1967
step 1850; step_loss: 0.1913
step 1860; step_loss: 0.2722
step 1870; step_loss: 0.1313
step 1880; step_loss: 0.2078
step 1890; step_loss: 0.2098
step 1900; step_loss: 0.1286
step 1910; step_loss: 0.1291
step 1920; step_loss: 0.1684
step 1930; step_loss: 0.2682
step 1940; step_loss: 0.1493
step 1950; step_loss: 0.1455
step 1960; step_loss: 0.1958
step 1970; step_loss: 0.1376
step 1980; step_loss: 0.1624
step 1990; step_loss: 0.1593

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 0.367 | 0.633 | 0.926 | 1.026 |   n/a |   n/a |

============================
Global step:         2000
Learning rate:       0.0050
Step-time (ms):     27.1364
Train loss avg:      0.1893
--------------------------
Val loss:            0.3910
srnn loss:           0.3110
============================

Saving the model...
done in 262.61 ms
step 2000; step_loss: 0.1668
step 2010; step_loss: 0.2608
step 2020; step_loss: 0.1752
step 2030; step_loss: 0.1648
step 2040; step_loss: 0.1638
step 2050; step_loss: 0.1466
step 2060; step_loss: 0.1645
step 2070; step_loss: 0.1566
step 2080; step_loss: 0.1358
step 2090; step_loss: 0.1731
step 2100; step_loss: 0.0957
step 2110; step_loss: 0.1491
step 2120; step_loss: 0.2759
step 2130; step_loss: 0.2336
step 2140; step_loss: 0.2429
step 2150; step_loss: 0.1743
step 2160; step_loss: 0.1330
step 2170; step_loss: 0.1582
step 2180; step_loss: 0.1415
step 2190; step_loss: 0.1659
step 2200; step_loss: 0.2476
step 2210; step_loss: 0.1446
step 2220; step_loss: 0.1591
step 2230; step_loss: 0.2286
step 2240; step_loss: 0.1725
step 2250; step_loss: 0.2078
step 2260; step_loss: 0.1813
step 2270; step_loss: 0.1537
step 2280; step_loss: 0.1592
step 2290; step_loss: 0.1386
step 2300; step_loss: 0.1603
step 2310; step_loss: 0.1524
step 2320; step_loss: 0.2118
step 2330; step_loss: 0.1957
step 2340; step_loss: 0.1730
step 2350; step_loss: 0.2059
step 2360; step_loss: 0.1564
step 2370; step_loss: 0.1507
step 2380; step_loss: 0.2011
step 2390; step_loss: 0.2041
step 2400; step_loss: 0.1558
step 2410; step_loss: 0.2578
step 2420; step_loss: 0.1740
step 2430; step_loss: 0.1145
step 2440; step_loss: 0.2009
step 2450; step_loss: 0.2057
step 2460; step_loss: 0.1704
step 2470; step_loss: 0.1595
step 2480; step_loss: 0.1567
step 2490; step_loss: 0.1854
step 2500; step_loss: 0.1559
step 2510; step_loss: 0.1990
step 2520; step_loss: 0.1448
step 2530; step_loss: 0.1769
step 2540; step_loss: 0.2306
step 2550; step_loss: 0.1388
step 2560; step_loss: 0.1307
step 2570; step_loss: 0.3773
step 2580; step_loss: 0.1567
step 2590; step_loss: 0.1275
step 2600; step_loss: 0.2126
step 2610; step_loss: 0.1322
step 2620; step_loss: 0.1172
step 2630; step_loss: 0.1612
step 2640; step_loss: 0.1245
step 2650; step_loss: 0.1790
step 2660; step_loss: 0.1608
step 2670; step_loss: 0.1929
step 2680; step_loss: 0.1423
step 2690; step_loss: 0.1784
step 2700; step_loss: 0.1826
step 2710; step_loss: 0.1954
step 2720; step_loss: 0.2345
step 2730; step_loss: 0.1366
step 2740; step_loss: 0.1331
step 2750; step_loss: 0.1661
step 2760; step_loss: 0.1572
step 2770; step_loss: 0.2019
step 2780; step_loss: 0.1538
step 2790; step_loss: 0.1648
step 2800; step_loss: 0.1695
step 2810; step_loss: 0.1491
step 2820; step_loss: 0.1710
step 2830; step_loss: 0.1023
step 2840; step_loss: 0.1258
step 2850; step_loss: 0.1180
step 2860; step_loss: 0.1405
step 2870; step_loss: 0.1958
step 2880; step_loss: 0.1388
step 2890; step_loss: 0.1340
step 2900; step_loss: 0.1482
step 2910; step_loss: 0.1655
step 2920; step_loss: 0.1510
step 2930; step_loss: 0.1591
step 2940; step_loss: 0.3386
step 2950; step_loss: 0.1689
step 2960; step_loss: 0.2028
step 2970; step_loss: 0.2344
step 2980; step_loss: 0.1746
step 2990; step_loss: 0.1433

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 0.370 | 0.633 | 0.934 | 1.039 |   n/a |   n/a |

============================
Global step:         3000
Learning rate:       0.0050
Step-time (ms):     27.1145
Train loss avg:      0.1740
--------------------------
Val loss:            0.4112
srnn loss:           0.3120
============================

Saving the model...
done in 257.95 ms
step 3000; step_loss: 0.1569
step 3010; step_loss: 0.1588
step 3020; step_loss: 0.1541
step 3030; step_loss: 0.1622
step 3040; step_loss: 0.1774
step 3050; step_loss: 0.1634
step 3060; step_loss: 0.1673
step 3070; step_loss: 0.2315
step 3080; step_loss: 0.1503
step 3090; step_loss: 0.1337
step 3100; step_loss: 0.1483
step 3110; step_loss: 0.1336
step 3120; step_loss: 0.1772
step 3130; step_loss: 0.1487
step 3140; step_loss: 0.2140
step 3150; step_loss: 0.1640
step 3160; step_loss: 0.1078
step 3170; step_loss: 0.1303
step 3180; step_loss: 0.1291
step 3190; step_loss: 0.1571
step 3200; step_loss: 0.1252
step 3210; step_loss: 0.1420
step 3220; step_loss: 0.1456
step 3230; step_loss: 0.1131
step 3240; step_loss: 0.1523
step 3250; step_loss: 0.1338
step 3260; step_loss: 0.1508
step 3270; step_loss: 0.1222
step 3280; step_loss: 0.2097
step 3290; step_loss: 0.1492
step 3300; step_loss: 0.1469
step 3310; step_loss: 0.1280
step 3320; step_loss: 0.1519
step 3330; step_loss: 0.1990
step 3340; step_loss: 0.1537
step 3350; step_loss: 0.1284
step 3360; step_loss: 0.1580
step 3370; step_loss: 0.1003
step 3380; step_loss: 0.1316
step 3390; step_loss: 0.1206
step 3400; step_loss: 0.1499
step 3410; step_loss: 0.2041
step 3420; step_loss: 0.1896
step 3430; step_loss: 0.1820
step 3440; step_loss: 0.1590
step 3450; step_loss: 0.1322
step 3460; step_loss: 0.2238
step 3470; step_loss: 0.1369
step 3480; step_loss: 0.1551
step 3490; step_loss: 0.1220
step 3500; step_loss: 0.1723
step 3510; step_loss: 0.1376
step 3520; step_loss: 0.1864
step 3530; step_loss: 0.1341
step 3540; step_loss: 0.1556
step 3550; step_loss: 0.2243
step 3560; step_loss: 0.1685
step 3570; step_loss: 0.1215
step 3580; step_loss: 0.1893
step 3590; step_loss: 0.1853
step 3600; step_loss: 0.1565
step 3610; step_loss: 0.1372
step 3620; step_loss: 0.1370
step 3630; step_loss: 0.1363
step 3640; step_loss: 0.1187
step 3650; step_loss: 0.1341
step 3660; step_loss: 0.1509
step 3670; step_loss: 0.1591
step 3680; step_loss: 0.1986
step 3690; step_loss: 0.1867
step 3700; step_loss: 0.1478
step 3710; step_loss: 0.1393
step 3720; step_loss: 0.1544
step 3730; step_loss: 0.1221
step 3740; step_loss: 0.2515
step 3750; step_loss: 0.1569
step 3760; step_loss: 0.1282
step 3770; step_loss: 0.1247
step 3780; step_loss: 0.1593
step 3790; step_loss: 0.1577
step 3800; step_loss: 0.1449
step 3810; step_loss: 0.1528
step 3820; step_loss: 0.2998
step 3830; step_loss: 0.2081
step 3840; step_loss: 0.1251
step 3850; step_loss: 0.1324
step 3860; step_loss: 0.1178
step 3870; step_loss: 0.1242
step 3880; step_loss: 0.1262
step 3890; step_loss: 0.1602
step 3900; step_loss: 0.1235
step 3910; step_loss: 0.2027
step 3920; step_loss: 0.1071
step 3930; step_loss: 0.1469
step 3940; step_loss: 0.1346
step 3950; step_loss: 0.1788
step 3960; step_loss: 0.1310
step 3970; step_loss: 0.1710
step 3980; step_loss: 0.4322
step 3990; step_loss: 0.1936

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 0.368 | 0.625 | 0.917 | 1.020 |   n/a |   n/a |

============================
Global step:         4000
Learning rate:       0.0050
Step-time (ms):     27.1272
Train loss avg:      0.1636
--------------------------
Val loss:            0.3221
srnn loss:           0.3028
============================

Saving the model...
done in 260.04 ms
step 4000; step_loss: 0.1361
step 4010; step_loss: 0.1307
step 4020; step_loss: 0.1047
step 4030; step_loss: 0.2482
step 4040; step_loss: 0.1532
step 4050; step_loss: 0.1892
step 4060; step_loss: 0.1128
step 4070; step_loss: 0.1611
step 4080; step_loss: 0.1512
step 4090; step_loss: 0.1889
step 4100; step_loss: 0.1294
step 4110; step_loss: 0.2115
step 4120; step_loss: 0.1296
step 4130; step_loss: 0.1830
step 4140; step_loss: 0.1337
step 4150; step_loss: 0.1594
step 4160; step_loss: 0.1602
step 4170; step_loss: 0.1278
step 4180; step_loss: 0.1389
step 4190; step_loss: 0.1144
step 4200; step_loss: 0.1717
step 4210; step_loss: 0.1884
step 4220; step_loss: 0.1561
step 4230; step_loss: 0.1659
step 4240; step_loss: 0.1194
step 4250; step_loss: 0.1311
step 4260; step_loss: 0.1313
step 4270; step_loss: 0.2096
step 4280; step_loss: 0.2068
step 4290; step_loss: 0.1542
step 4300; step_loss: 0.1905
step 4310; step_loss: 0.1809
step 4320; step_loss: 0.3641
step 4330; step_loss: 0.1219
step 4340; step_loss: 0.1471
step 4350; step_loss: 0.1950
step 4360; step_loss: 0.2246
step 4370; step_loss: 0.1530
step 4380; step_loss: 0.1314
step 4390; step_loss: 0.2070
step 4400; step_loss: 0.1468
step 4410; step_loss: 0.1219
step 4420; step_loss: 0.2579
step 4430; step_loss: 0.1305
step 4440; step_loss: 0.1593
step 4450; step_loss: 0.1685
step 4460; step_loss: 0.2224
step 4470; step_loss: 0.1177
step 4480; step_loss: 0.0938
step 4490; step_loss: 0.2040
step 4500; step_loss: 0.1219
step 4510; step_loss: 0.1519
step 4520; step_loss: 0.1289
step 4530; step_loss: 0.1132
step 4540; step_loss: 0.1882
step 4550; step_loss: 0.1570
step 4560; step_loss: 0.1190
step 4570; step_loss: 0.1686
step 4580; step_loss: 0.1259
step 4590; step_loss: 0.1300
step 4600; step_loss: 0.0993
step 4610; step_loss: 0.1638
step 4620; step_loss: 0.1149
step 4630; step_loss: 0.1866
step 4640; step_loss: 0.1488
step 4650; step_loss: 0.1556
step 4660; step_loss: 0.1600
step 4670; step_loss: 0.1731
step 4680; step_loss: 0.2689
step 4690; step_loss: 0.1167
step 4700; step_loss: 0.1232
step 4710; step_loss: 0.1464
step 4720; step_loss: 0.1855
step 4730; step_loss: 0.1443
step 4740; step_loss: 0.1325
step 4750; step_loss: 0.1617
step 4760; step_loss: 0.2851
step 4770; step_loss: 0.1583
step 4780; step_loss: 0.2039
step 4790; step_loss: 0.2230
step 4800; step_loss: 0.2257
step 4810; step_loss: 0.1074
step 4820; step_loss: 0.1180
step 4830; step_loss: 0.1406
step 4840; step_loss: 0.1132
step 4850; step_loss: 0.1319
step 4860; step_loss: 0.1961
step 4870; step_loss: 0.2354
step 4880; step_loss: 0.1372
step 4890; step_loss: 0.1536
step 4900; step_loss: 0.1930
step 4910; step_loss: 0.1226
step 4920; step_loss: 0.1418
step 4930; step_loss: 0.1403
step 4940; step_loss: 0.1920
step 4950; step_loss: 0.1049
step 4960; step_loss: 0.1692
step 4970; step_loss: 0.1366
step 4980; step_loss: 0.1852
step 4990; step_loss: 0.1571

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 0.372 | 0.628 | 0.904 | 0.997 |   n/a |   n/a |

============================
Global step:         5000
Learning rate:       0.0050
Step-time (ms):     27.2100
Train loss avg:      0.1582
--------------------------
Val loss:            0.3029
srnn loss:           0.3050
============================

Saving the model...
done in 261.57 ms
step 5000; step_loss: 0.2386
step 5010; step_loss: 0.1174
step 5020; step_loss: 0.1199
step 5030; step_loss: 0.1573
step 5040; step_loss: 0.1651
step 5050; step_loss: 0.1703
step 5060; step_loss: 0.1588
step 5070; step_loss: 0.1053
step 5080; step_loss: 0.1576
step 5090; step_loss: 0.1450
step 5100; step_loss: 0.1150
step 5110; step_loss: 0.1618
step 5120; step_loss: 0.1454
step 5130; step_loss: 0.1309
step 5140; step_loss: 0.1791
step 5150; step_loss: 0.1297
step 5160; step_loss: 0.1287
step 5170; step_loss: 0.2007
step 5180; step_loss: 0.1551
step 5190; step_loss: 0.1604
step 5200; step_loss: 0.1082
step 5210; step_loss: 0.1359
step 5220; step_loss: 0.1037
step 5230; step_loss: 0.1279
step 5240; step_loss: 0.1266
step 5250; step_loss: 0.1685
step 5260; step_loss: 0.1814
step 5270; step_loss: 0.1278
step 5280; step_loss: 0.1680
step 5290; step_loss: 0.1842
step 5300; step_loss: 0.1516
step 5310; step_loss: 0.1525
step 5320; step_loss: 0.1105
step 5330; step_loss: 0.1174
step 5340; step_loss: 0.1039
step 5350; step_loss: 0.1310
step 5360; step_loss: 0.1418
step 5370; step_loss: 0.1433
step 5380; step_loss: 0.1492
step 5390; step_loss: 0.2991
step 5400; step_loss: 0.1262
step 5410; step_loss: 0.1619
step 5420; step_loss: 0.0902
step 5430; step_loss: 0.0964
step 5440; step_loss: 0.1596
step 5450; step_loss: 0.1902
step 5460; step_loss: 0.1408
step 5470; step_loss: 0.1437
step 5480; step_loss: 0.1440
step 5490; step_loss: 0.1167
step 5500; step_loss: 0.1307
step 5510; step_loss: 0.1675
step 5520; step_loss: 0.1373
step 5530; step_loss: 0.1552
step 5540; step_loss: 0.1156
step 5550; step_loss: 0.1831
step 5560; step_loss: 0.3093
step 5570; step_loss: 0.1820
step 5580; step_loss: 0.2502
step 5590; step_loss: 0.1053
step 5600; step_loss: 0.1665
step 5610; step_loss: 0.3478
step 5620; step_loss: 0.1485
step 5630; step_loss: 0.1130
step 5640; step_loss: 0.1826
step 5650; step_loss: 0.1565
step 5660; step_loss: 0.1393
step 5670; step_loss: 0.0895
step 5680; step_loss: 0.1962
step 5690; step_loss: 0.1393
step 5700; step_loss: 0.1325
step 5710; step_loss: 0.1714
step 5720; step_loss: 0.1698
step 5730; step_loss: 0.1168
step 5740; step_loss: 0.1315
step 5750; step_loss: 0.0981
step 5760; step_loss: 0.1278
step 5770; step_loss: 0.1167
step 5780; step_loss: 0.1299
step 5790; step_loss: 0.2144
step 5800; step_loss: 0.1635
step 5810; step_loss: 0.1313
step 5820; step_loss: 0.1383
step 5830; step_loss: 0.1195
step 5840; step_loss: 0.1430
step 5850; step_loss: 0.1585
step 5860; step_loss: 0.1569
step 5870; step_loss: 0.0980
step 5880; step_loss: 0.1211
step 5890; step_loss: 0.1414
step 5900; step_loss: 0.1160
step 5910; step_loss: 0.1266
step 5920; step_loss: 0.1204
step 5930; step_loss: 0.1240
step 5940; step_loss: 0.2353
step 5950; step_loss: 0.1083
step 5960; step_loss: 0.1741
step 5970; step_loss: 0.2554
step 5980; step_loss: 0.1299
step 5990; step_loss: 0.1480

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 0.386 | 0.655 | 0.949 | 1.043 |   n/a |   n/a |

============================
Global step:         6000
Learning rate:       0.0050
Step-time (ms):     27.1177
Train loss avg:      0.1554
--------------------------
Val loss:            0.3602
srnn loss:           0.3174
============================

Saving the model...
done in 261.32 ms
step 6000; step_loss: 0.1328
step 6010; step_loss: 0.1339
step 6020; step_loss: 0.1120
step 6030; step_loss: 0.1387
step 6040; step_loss: 0.1543
step 6050; step_loss: 0.1447
step 6060; step_loss: 0.2676
step 6070; step_loss: 0.1664
step 6080; step_loss: 0.1231
step 6090; step_loss: 0.1274
step 6100; step_loss: 0.1772
step 6110; step_loss: 0.1414
step 6120; step_loss: 0.1323
step 6130; step_loss: 0.1269
step 6140; step_loss: 0.1888
step 6150; step_loss: 0.1145
step 6160; step_loss: 0.1697
step 6170; step_loss: 0.2128
step 6180; step_loss: 0.2259
step 6190; step_loss: 0.1716
step 6200; step_loss: 0.1495
step 6210; step_loss: 0.1170
step 6220; step_loss: 0.1336
step 6230; step_loss: 0.1803
step 6240; step_loss: 0.1503
step 6250; step_loss: 0.1247
step 6260; step_loss: 0.1279
step 6270; step_loss: 0.1372
step 6280; step_loss: 0.2524
step 6290; step_loss: 0.1896
step 6300; step_loss: 0.1598
step 6310; step_loss: 0.1462
step 6320; step_loss: 0.0932
step 6330; step_loss: 0.1948
step 6340; step_loss: 0.1461
step 6350; step_loss: 0.1172
step 6360; step_loss: 0.1459
step 6370; step_loss: 0.1237
step 6380; step_loss: 0.1598
step 6390; step_loss: 0.1408
step 6400; step_loss: 0.1076
step 6410; step_loss: 0.1419
step 6420; step_loss: 0.1703
step 6430; step_loss: 0.2299
step 6440; step_loss: 0.1442
step 6450; step_loss: 0.2266
step 6460; step_loss: 0.2216
step 6470; step_loss: 0.0982
step 6480; step_loss: 0.0989
step 6490; step_loss: 0.1315
step 6500; step_loss: 0.1780
step 6510; step_loss: 0.0898
step 6520; step_loss: 0.1532
step 6530; step_loss: 0.2543
step 6540; step_loss: 0.1400
step 6550; step_loss: 0.1508
step 6560; step_loss: 0.1246
step 6570; step_loss: 0.1182
step 6580; step_loss: 0.1480
step 6590; step_loss: 0.1790
step 6600; step_loss: 0.1639
step 6610; step_loss: 0.1423
step 6620; step_loss: 0.1205
step 6630; step_loss: 0.1198
step 6640; step_loss: 0.1241
step 6650; step_loss: 0.1441
step 6660; step_loss: 0.1577
step 6670; step_loss: 0.1382
step 6680; step_loss: 0.2909
step 6690; step_loss: 0.2303
step 6700; step_loss: 0.1063
step 6710; step_loss: 0.1193
step 6720; step_loss: 0.1190
step 6730; step_loss: 0.1480
step 6740; step_loss: 0.1393
step 6750; step_loss: 0.1240
step 6760; step_loss: 0.0975
step 6770; step_loss: 0.1140
step 6780; step_loss: 0.1241
step 6790; step_loss: 0.2198
step 6800; step_loss: 0.1550
step 6810; step_loss: 0.1117
step 6820; step_loss: 0.1561
step 6830; step_loss: 0.1403
step 6840; step_loss: 0.1333
step 6850; step_loss: 0.0993
step 6860; step_loss: 0.1403
step 6870; step_loss: 0.1036
step 6880; step_loss: 0.1104
step 6890; step_loss: 0.2928
step 6900; step_loss: 0.1553
step 6910; step_loss: 0.1373
step 6920; step_loss: 0.1641
step 6930; step_loss: 0.1479
step 6940; step_loss: 0.1020
step 6950; step_loss: 0.1498
step 6960; step_loss: 0.1374
step 6970; step_loss: 0.1834
step 6980; step_loss: 0.1190
step 6990; step_loss: 0.1277

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 0.381 | 0.645 | 0.934 | 1.024 |   n/a |   n/a |

============================
Global step:         7000
Learning rate:       0.0050
Step-time (ms):     27.1560
Train loss avg:      0.1508
--------------------------
Val loss:            0.3395
srnn loss:           0.3083
============================

Saving the model...
done in 266.41 ms
step 7000; step_loss: 0.1563
step 7010; step_loss: 0.1400
step 7020; step_loss: 0.1236
step 7030; step_loss: 0.1720
step 7040; step_loss: 0.2456
step 7050; step_loss: 0.1352
step 7060; step_loss: 0.1068
step 7070; step_loss: 0.1159
step 7080; step_loss: 0.1427
step 7090; step_loss: 0.1156
step 7100; step_loss: 0.1463
step 7110; step_loss: 0.1511
step 7120; step_loss: 0.1550
step 7130; step_loss: 0.1518
step 7140; step_loss: 0.2189
step 7150; step_loss: 0.2353
step 7160; step_loss: 0.1221
step 7170; step_loss: 0.0969
step 7180; step_loss: 0.1456
step 7190; step_loss: 0.1351
step 7200; step_loss: 0.1739
step 7210; step_loss: 0.1283
step 7220; step_loss: 0.1925
step 7230; step_loss: 0.2173
step 7240; step_loss: 0.1425
step 7250; step_loss: 0.2488
step 7260; step_loss: 0.1014
step 7270; step_loss: 0.1009
step 7280; step_loss: 0.1388
step 7290; step_loss: 0.1405
step 7300; step_loss: 0.1150
step 7310; step_loss: 0.1151
step 7320; step_loss: 0.1502
step 7330; step_loss: 0.1133
step 7340; step_loss: 0.1432
step 7350; step_loss: 0.1646
step 7360; step_loss: 0.1489
step 7370; step_loss: 0.1395
step 7380; step_loss: 0.2806
step 7390; step_loss: 0.1031
step 7400; step_loss: 0.1537
step 7410; step_loss: 0.1382
step 7420; step_loss: 0.1195
step 7430; step_loss: 0.1345
step 7440; step_loss: 0.1518
step 7450; step_loss: 0.1685
step 7460; step_loss: 0.1354
step 7470; step_loss: 0.1186
step 7480; step_loss: 0.1366
step 7490; step_loss: 0.1616
step 7500; step_loss: 0.1435
step 7510; step_loss: 0.1433
step 7520; step_loss: 0.1678
step 7530; step_loss: 0.1769
step 7540; step_loss: 0.1624
step 7550; step_loss: 0.1208
step 7560; step_loss: 0.1575
step 7570; step_loss: 0.1083
step 7580; step_loss: 0.2056
step 7590; step_loss: 0.1669
step 7600; step_loss: 0.2079
step 7610; step_loss: 0.1388
step 7620; step_loss: 0.1324
step 7630; step_loss: 0.1324
step 7640; step_loss: 0.1307
step 7650; step_loss: 0.1167
step 7660; step_loss: 0.1472
step 7670; step_loss: 0.1003
step 7680; step_loss: 0.1737
step 7690; step_loss: 0.1262
step 7700; step_loss: 0.1199
step 7710; step_loss: 0.1068
step 7720; step_loss: 0.1354
step 7730; step_loss: 0.1420
step 7740; step_loss: 0.1490
step 7750; step_loss: 0.1343
step 7760; step_loss: 0.1247
step 7770; step_loss: 0.1222
step 7780; step_loss: 0.1479
step 7790; step_loss: 0.1175
step 7800; step_loss: 0.1524
step 7810; step_loss: 0.1098
step 7820; step_loss: 0.1104
step 7830; step_loss: 0.2230
step 7840; step_loss: 0.1190
step 7850; step_loss: 0.1031
step 7860; step_loss: 0.1354
step 7870; step_loss: 0.1339
step 7880; step_loss: 0.2369
step 7890; step_loss: 0.2497
step 7900; step_loss: 0.2599
step 7910; step_loss: 0.2650
step 7920; step_loss: 0.1461
step 7930; step_loss: 0.1244
step 7940; step_loss: 0.1150
step 7950; step_loss: 0.1119
step 7960; step_loss: 0.1976
step 7970; step_loss: 0.1886
step 7980; step_loss: 0.0999
step 7990; step_loss: 0.1832

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 0.380 | 0.643 | 0.939 | 1.035 |   n/a |   n/a |

============================
Global step:         8000
Learning rate:       0.0050
Step-time (ms):     27.1824
Train loss avg:      0.1473
--------------------------
Val loss:            0.3462
srnn loss:           0.3013
============================

Saving the model...
done in 271.55 ms
step 8000; step_loss: 0.1232
step 8010; step_loss: 0.1826
step 8020; step_loss: 0.1393
step 8030; step_loss: 0.1068
step 8040; step_loss: 0.1927
step 8050; step_loss: 0.1220
step 8060; step_loss: 0.1214
step 8070; step_loss: 0.1352
step 8080; step_loss: 0.1326
step 8090; step_loss: 0.1374
step 8100; step_loss: 0.1401
step 8110; step_loss: 0.1176
step 8120; step_loss: 0.1256
step 8130; step_loss: 0.1201
step 8140; step_loss: 0.1330
step 8150; step_loss: 0.1381
step 8160; step_loss: 0.1537
step 8170; step_loss: 0.1042
step 8180; step_loss: 0.2679
step 8190; step_loss: 0.1389
step 8200; step_loss: 0.1262
step 8210; step_loss: 0.1414
step 8220; step_loss: 0.1377
step 8230; step_loss: 0.0935
step 8240; step_loss: 0.1096
step 8250; step_loss: 0.1282
step 8260; step_loss: 0.1109
step 8270; step_loss: 0.0913
step 8280; step_loss: 0.1166
step 8290; step_loss: 0.2173
step 8300; step_loss: 0.1615
step 8310; step_loss: 0.1564
step 8320; step_loss: 0.1164
step 8330; step_loss: 0.1124
step 8340; step_loss: 0.1093
step 8350; step_loss: 0.1068
step 8360; step_loss: 0.2766
step 8370; step_loss: 0.2039
step 8380; step_loss: 0.1460
step 8390; step_loss: 0.1084
step 8400; step_loss: 0.1027
step 8410; step_loss: 0.1601
step 8420; step_loss: 0.2229
step 8430; step_loss: 0.2254
step 8440; step_loss: 0.1585
step 8450; step_loss: 0.2509
step 8460; step_loss: 0.2185
step 8470; step_loss: 0.1675
step 8480; step_loss: 0.2863
step 8490; step_loss: 0.1317
step 8500; step_loss: 0.1414
step 8510; step_loss: 0.1778
step 8520; step_loss: 0.1210
step 8530; step_loss: 0.1307
step 8540; step_loss: 0.1423
step 8550; step_loss: 0.1275
step 8560; step_loss: 0.1191
step 8570; step_loss: 0.1425
step 8580; step_loss: 0.1322
step 8590; step_loss: 0.1247
step 8600; step_loss: 0.1303
step 8610; step_loss: 0.1216
step 8620; step_loss: 0.1842
step 8630; step_loss: 0.2241
step 8640; step_loss: 0.1321
step 8650; step_loss: 0.1345
step 8660; step_loss: 0.1335
step 8670; step_loss: 0.1976
step 8680; step_loss: 0.1680
step 8690; step_loss: 0.0994
step 8700; step_loss: 0.2458
step 8710; step_loss: 0.1293
step 8720; step_loss: 0.1405
step 8730; step_loss: 0.1013
step 8740; step_loss: 0.2125
step 8750; step_loss: 0.1382
step 8760; step_loss: 0.1263
step 8770; step_loss: 0.1265
step 8780; step_loss: 0.1500
step 8790; step_loss: 0.1015
step 8800; step_loss: 0.2453
step 8810; step_loss: 0.1693
step 8820; step_loss: 0.1225
step 8830; step_loss: 0.1418
step 8840; step_loss: 0.1300
step 8850; step_loss: 0.1141
step 8860; step_loss: 0.1337
step 8870; step_loss: 0.1461
step 8880; step_loss: 0.1303
step 8890; step_loss: 0.2904
step 8900; step_loss: 0.1291
step 8910; step_loss: 0.1212
step 8920; step_loss: 0.1156
step 8930; step_loss: 0.2359
step 8940; step_loss: 0.1265
step 8950; step_loss: 0.2264
step 8960; step_loss: 0.1867
step 8970; step_loss: 0.0984
step 8980; step_loss: 0.1321
step 8990; step_loss: 0.1433

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 0.380 | 0.644 | 0.935 | 1.032 |   n/a |   n/a |

============================
Global step:         9000
Learning rate:       0.0050
Step-time (ms):     27.0992
Train loss avg:      0.1457
--------------------------
Val loss:            0.4350
srnn loss:           0.2987
============================

Saving the model...
done in 261.96 ms
step 9000; step_loss: 0.2247
step 9010; step_loss: 0.1359
step 9020; step_loss: 0.1723
step 9030; step_loss: 0.0827
step 9040; step_loss: 0.1745
step 9050; step_loss: 0.1189
step 9060; step_loss: 0.1055
step 9070; step_loss: 0.1596
step 9080; step_loss: 0.1064
step 9090; step_loss: 0.1462
step 9100; step_loss: 0.1824
step 9110; step_loss: 0.1283
step 9120; step_loss: 0.2049
step 9130; step_loss: 0.1821
step 9140; step_loss: 0.1694
step 9150; step_loss: 0.1586
step 9160; step_loss: 0.1260
step 9170; step_loss: 0.1146
step 9180; step_loss: 0.0826
step 9190; step_loss: 0.2105
step 9200; step_loss: 0.1639
step 9210; step_loss: 0.1144
step 9220; step_loss: 0.1485
step 9230; step_loss: 0.1363
step 9240; step_loss: 0.1562
step 9250; step_loss: 0.1226
step 9260; step_loss: 0.1138
step 9270; step_loss: 0.2045
step 9280; step_loss: 0.1566
step 9290; step_loss: 0.1206
step 9300; step_loss: 0.1514
step 9310; step_loss: 0.1548
step 9320; step_loss: 0.1358
step 9330; step_loss: 0.1388
step 9340; step_loss: 0.1933
step 9350; step_loss: 0.0916
step 9360; step_loss: 0.1495
step 9370; step_loss: 0.1281
step 9380; step_loss: 0.1173
step 9390; step_loss: 0.1225
step 9400; step_loss: 0.1793
step 9410; step_loss: 0.1246
step 9420; step_loss: 0.1191
step 9430; step_loss: 0.1204
step 9440; step_loss: 0.1956
step 9450; step_loss: 0.3133
step 9460; step_loss: 0.2195
step 9470; step_loss: 0.1333
step 9480; step_loss: 0.1285
step 9490; step_loss: 0.1261
step 9500; step_loss: 0.1381
step 9510; step_loss: 0.1622
step 9520; step_loss: 0.1136
step 9530; step_loss: 0.1740
step 9540; step_loss: 0.1388
step 9550; step_loss: 0.1328
step 9560; step_loss: 0.0975
step 9570; step_loss: 0.0959
step 9580; step_loss: 0.1373
step 9590; step_loss: 0.1505
step 9600; step_loss: 0.1372
step 9610; step_loss: 0.2124
step 9620; step_loss: 0.1214
step 9630; step_loss: 0.1535
step 9640; step_loss: 0.1008
step 9650; step_loss: 0.3599
step 9660; step_loss: 0.1462
step 9670; step_loss: 0.0983
step 9680; step_loss: 0.0938
step 9690; step_loss: 0.1283
step 9700; step_loss: 0.1310
step 9710; step_loss: 0.1714
step 9720; step_loss: 0.0922
step 9730; step_loss: 0.1628
step 9740; step_loss: 0.1059
step 9750; step_loss: 0.1416
step 9760; step_loss: 0.1004
step 9770; step_loss: 0.1146
step 9780; step_loss: 0.1576
step 9790; step_loss: 0.1049
step 9800; step_loss: 0.1531
step 9810; step_loss: 0.1539
step 9820; step_loss: 0.1418
step 9830; step_loss: 0.1140
step 9840; step_loss: 0.0966
step 9850; step_loss: 0.1368
step 9860; step_loss: 0.1435
step 9870; step_loss: 0.1108
step 9880; step_loss: 0.0997
step 9890; step_loss: 0.1186
step 9900; step_loss: 0.1198
step 9910; step_loss: 0.1695
step 9920; step_loss: 0.1153
step 9930; step_loss: 0.1693
step 9940; step_loss: 0.2231
step 9950; step_loss: 0.1351
step 9960; step_loss: 0.1920
step 9970; step_loss: 0.1638
step 9980; step_loss: 0.1138
step 9990; step_loss: 0.1602

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 0.388 | 0.657 | 0.951 | 1.048 |   n/a |   n/a |

============================
Global step:         10000
Learning rate:       0.0047
Step-time (ms):     27.0949
Train loss avg:      0.1399
--------------------------
Val loss:            0.3430
srnn loss:           0.3053
============================

Saving the model...
done in 249.92 ms
step 10000; step_loss: 0.1103
step 10010; step_loss: 0.1085
step 10020; step_loss: 0.1079
step 10030; step_loss: 0.1246
step 10040; step_loss: 0.1315
step 10050; step_loss: 0.1022
step 10060; step_loss: 0.0976
step 10070; step_loss: 0.1369
step 10080; step_loss: 0.1181
step 10090; step_loss: 0.0988
step 10100; step_loss: 0.1276
step 10110; step_loss: 0.1392
step 10120; step_loss: 0.2068
step 10130; step_loss: 0.1124
step 10140; step_loss: 0.1199
step 10150; step_loss: 0.1128
step 10160; step_loss: 0.1044
step 10170; step_loss: 0.1674
step 10180; step_loss: 0.1149
step 10190; step_loss: 0.1713
step 10200; step_loss: 0.1718
step 10210; step_loss: 0.1569
step 10220; step_loss: 0.1622
step 10230; step_loss: 0.1261
step 10240; step_loss: 0.1903
step 10250; step_loss: 0.1030
step 10260; step_loss: 0.1751
step 10270; step_loss: 0.1496
step 10280; step_loss: 0.1016
step 10290; step_loss: 0.1261
step 10300; step_loss: 0.1526
step 10310; step_loss: 0.1502
step 10320; step_loss: 0.1131
step 10330; step_loss: 0.1324
step 10340; step_loss: 0.0936
step 10350; step_loss: 0.1008
step 10360; step_loss: 0.1053
step 10370; step_loss: 0.1989
step 10380; step_loss: 0.1201
step 10390; step_loss: 0.1171
step 10400; step_loss: 0.1537
step 10410; step_loss: 0.1231
step 10420; step_loss: 0.1388
step 10430; step_loss: 0.1373
step 10440; step_loss: 0.1600
step 10450; step_loss: 0.1375
step 10460; step_loss: 0.1649
step 10470; step_loss: 0.1139
step 10480; step_loss: 0.1335
step 10490; step_loss: 0.1997
step 10500; step_loss: 0.2053
step 10510; step_loss: 0.1003
step 10520; step_loss: 0.1627
step 10530; step_loss: 0.1403
step 10540; step_loss: 0.1519
step 10550; step_loss: 0.1453
step 10560; step_loss: 0.1629
step 10570; step_loss: 0.1286
step 10580; step_loss: 0.1418
step 10590; step_loss: 0.2200
step 10600; step_loss: 0.1871
step 10610; step_loss: 0.0948
step 10620; step_loss: 0.0999
step 10630; step_loss: 0.0797
step 10640; step_loss: 0.1007
step 10650; step_loss: 0.1075
step 10660; step_loss: 0.1293
step 10670; step_loss: 0.2680
step 10680; step_loss: 0.1416
step 10690; step_loss: 0.1516
step 10700; step_loss: 0.1355
step 10710; step_loss: 0.1855
step 10720; step_loss: 0.1547
step 10730; step_loss: 0.1333
step 10740; step_loss: 0.0937
step 10750; step_loss: 0.1253
step 10760; step_loss: 0.1212
step 10770; step_loss: 0.1786
step 10780; step_loss: 0.1010
step 10790; step_loss: 0.1302
step 10800; step_loss: 0.2079
step 10810; step_loss: 0.1263
step 10820; step_loss: 0.1720
step 10830; step_loss: 0.1951
step 10840; step_loss: 0.1478
step 10850; step_loss: 0.1564
step 10860; step_loss: 0.1143
step 10870; step_loss: 0.1283
step 10880; step_loss: 0.1022
step 10890; step_loss: 0.1374
step 10900; step_loss: 0.1119
step 10910; step_loss: 0.1153
step 10920; step_loss: 0.2100
step 10930; step_loss: 0.1060
step 10940; step_loss: 0.1537
step 10950; step_loss: 0.1638
step 10960; step_loss: 0.1448
step 10970; step_loss: 0.1021
step 10980; step_loss: 0.1992
step 10990; step_loss: 0.1155

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 0.379 | 0.643 | 0.920 | 1.006 |   n/a |   n/a |

============================
Global step:         11000
Learning rate:       0.0047
Step-time (ms):     27.1388
Train loss avg:      0.1390
--------------------------
Val loss:            0.3171
srnn loss:           0.2902
============================

Saving the model...
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py:963: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to delete files with this prefix.
W1130 12:58:44.345453 140668203612032 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py:963: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to delete files with this prefix.
done in 260.81 ms
step 11000; step_loss: 0.1816
step 11010; step_loss: 0.1413
step 11020; step_loss: 0.1463
step 11030; step_loss: 0.2326
step 11040; step_loss: 0.0947
step 11050; step_loss: 0.1184
step 11060; step_loss: 0.1030
step 11070; step_loss: 0.1161
step 11080; step_loss: 0.2102
step 11090; step_loss: 0.1104
step 11100; step_loss: 0.1288
step 11110; step_loss: 0.1318
step 11120; step_loss: 0.1212
step 11130; step_loss: 0.1140
step 11140; step_loss: 0.1451
step 11150; step_loss: 0.2015
step 11160; step_loss: 0.1453
step 11170; step_loss: 0.1497
step 11180; step_loss: 0.1128
step 11190; step_loss: 0.1649
step 11200; step_loss: 0.2856
step 11210; step_loss: 0.1267
step 11220; step_loss: 0.1517
step 11230; step_loss: 0.1117
step 11240; step_loss: 0.0973
step 11250; step_loss: 0.2253
step 11260; step_loss: 0.1225
step 11270; step_loss: 0.1156
step 11280; step_loss: 0.1489
step 11290; step_loss: 0.1794
step 11300; step_loss: 0.1139
step 11310; step_loss: 0.1135
step 11320; step_loss: 0.1025
step 11330; step_loss: 0.1502
step 11340; step_loss: 0.1363
step 11350; step_loss: 0.1609
step 11360; step_loss: 0.1121
step 11370; step_loss: 0.1370
step 11380; step_loss: 0.1260
step 11390; step_loss: 0.1439
step 11400; step_loss: 0.1530
step 11410; step_loss: 0.1348
step 11420; step_loss: 0.1491
step 11430; step_loss: 0.1530
step 11440; step_loss: 0.1952
step 11450; step_loss: 0.1067
step 11460; step_loss: 0.1064
step 11470; step_loss: 0.0817
step 11480; step_loss: 0.1162
step 11490; step_loss: 0.1307
step 11500; step_loss: 0.1105
step 11510; step_loss: 0.1183
step 11520; step_loss: 0.1541
step 11530; step_loss: 0.1960
step 11540; step_loss: 0.1077
step 11550; step_loss: 0.1721
step 11560; step_loss: 0.1725
step 11570; step_loss: 0.1337
step 11580; step_loss: 0.1038
step 11590; step_loss: 0.1234
step 11600; step_loss: 0.1207
step 11610; step_loss: 0.1728
step 11620; step_loss: 0.1252
step 11630; step_loss: 0.1959
step 11640; step_loss: 0.1217
step 11650; step_loss: 0.1076
step 11660; step_loss: 0.1867
step 11670; step_loss: 0.1363
step 11680; step_loss: 0.1349
step 11690; step_loss: 0.1277
step 11700; step_loss: 0.0957
step 11710; step_loss: 0.1734
step 11720; step_loss: 0.0960
step 11730; step_loss: 0.1254
step 11740; step_loss: 0.1144
step 11750; step_loss: 0.2285
step 11760; step_loss: 0.1135
step 11770; step_loss: 0.1338
step 11780; step_loss: 0.1506
step 11790; step_loss: 0.1164
step 11800; step_loss: 0.1389
step 11810; step_loss: 0.1330
step 11820; step_loss: 0.1528
step 11830; step_loss: 0.1454
step 11840; step_loss: 0.1472
step 11850; step_loss: 0.1434
step 11860; step_loss: 0.1915
step 11870; step_loss: 0.1014
step 11880; step_loss: 0.0931
step 11890; step_loss: 0.1050
step 11900; step_loss: 0.1591
step 11910; step_loss: 0.1011
step 11920; step_loss: 0.1379
step 11930; step_loss: 0.1778
step 11940; step_loss: 0.0919
step 11950; step_loss: 0.1185
step 11960; step_loss: 0.2339
step 11970; step_loss: 0.1256
step 11980; step_loss: 0.1101
step 11990; step_loss: 0.1030

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 0.386 | 0.654 | 0.943 | 1.036 |   n/a |   n/a |

============================
Global step:         12000
Learning rate:       0.0047
Step-time (ms):     27.2110
Train loss avg:      0.1390
--------------------------
Val loss:            0.3272
srnn loss:           0.2906
============================

Saving the model...
done in 282.88 ms
step 12000; step_loss: 0.0933
step 12010; step_loss: 0.1434
step 12020; step_loss: 0.1408
step 12030; step_loss: 0.1391
step 12040; step_loss: 0.1002
step 12050; step_loss: 0.1444
step 12060; step_loss: 0.1079
step 12070; step_loss: 0.2160
step 12080; step_loss: 0.1235
step 12090; step_loss: 0.1249
step 12100; step_loss: 0.1325
step 12110; step_loss: 0.1415
step 12120; step_loss: 0.1203
step 12130; step_loss: 0.2512
step 12140; step_loss: 0.1180
step 12150; step_loss: 0.1432
step 12160; step_loss: 0.1242
step 12170; step_loss: 0.1784
step 12180; step_loss: 0.1465
step 12190; step_loss: 0.2823
step 12200; step_loss: 0.1543
step 12210; step_loss: 0.1247
step 12220; step_loss: 0.1250
step 12230; step_loss: 0.1028
step 12240; step_loss: 0.1228
step 12250; step_loss: 0.1878
step 12260; step_loss: 0.1319
step 12270; step_loss: 0.1369
step 12280; step_loss: 0.0993
step 12290; step_loss: 0.1174
step 12300; step_loss: 0.1762
step 12310; step_loss: 0.1031
step 12320; step_loss: 0.1006
step 12330; step_loss: 0.1284
step 12340; step_loss: 0.1681
step 12350; step_loss: 0.1000
step 12360; step_loss: 0.1205
step 12370; step_loss: 0.1883
step 12380; step_loss: 0.1822
step 12390; step_loss: 0.1299
step 12400; step_loss: 0.0896
step 12410; step_loss: 0.1156
step 12420; step_loss: 0.1041
step 12430; step_loss: 0.1436
step 12440; step_loss: 0.2274
step 12450; step_loss: 0.1391
step 12460; step_loss: 0.1143
step 12470; step_loss: 0.0803
step 12480; step_loss: 0.1262
step 12490; step_loss: 0.1360
step 12500; step_loss: 0.1217
step 12510; step_loss: 0.1796
step 12520; step_loss: 0.1098
step 12530; step_loss: 0.1045
step 12540; step_loss: 0.1480
step 12550; step_loss: 0.1635
step 12560; step_loss: 0.1486
step 12570; step_loss: 0.3665
step 12580; step_loss: 0.1759
step 12590; step_loss: 0.2187
step 12600; step_loss: 0.2120
step 12610; step_loss: 0.1435
step 12620; step_loss: 0.1181
step 12630; step_loss: 0.1726
step 12640; step_loss: 0.1656
step 12650; step_loss: 0.1143
step 12660; step_loss: 0.1007
step 12670; step_loss: 0.1189
step 12680; step_loss: 0.1439
step 12690; step_loss: 0.0963
step 12700; step_loss: 0.1173
step 12710; step_loss: 0.1073
step 12720; step_loss: 0.1586
step 12730; step_loss: 0.2951
step 12740; step_loss: 0.1314
step 12750; step_loss: 0.0949
step 12760; step_loss: 0.1955
step 12770; step_loss: 0.1201
step 12780; step_loss: 0.1115
step 12790; step_loss: 0.1516
step 12800; step_loss: 0.1286
step 12810; step_loss: 0.1026
step 12820; step_loss: 0.1787
step 12830; step_loss: 0.1176
step 12840; step_loss: 0.1018
step 12850; step_loss: 0.1589
step 12860; step_loss: 0.0981
step 12870; step_loss: 0.1189
step 12880; step_loss: 0.1022
step 12890; step_loss: 0.1098
step 12900; step_loss: 0.1356
step 12910; step_loss: 0.0897
step 12920; step_loss: 0.1455
step 12930; step_loss: 0.1166
step 12940; step_loss: 0.1388
step 12950; step_loss: 0.1471
step 12960; step_loss: 0.1121
step 12970; step_loss: 0.1054
step 12980; step_loss: 0.0933
step 12990; step_loss: 0.1421

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 0.383 | 0.655 | 0.948 | 1.042 |   n/a |   n/a |

============================
Global step:         13000
Learning rate:       0.0047
Step-time (ms):     27.1468
Train loss avg:      0.1349
--------------------------
Val loss:            0.3659
srnn loss:           0.2900
============================

Saving the model...
done in 269.19 ms
step 13000; step_loss: 0.1321
step 13010; step_loss: 0.0943
step 13020; step_loss: 0.1186
step 13030; step_loss: 0.1678
step 13040; step_loss: 0.1639
step 13050; step_loss: 0.1628
step 13060; step_loss: 0.1390
step 13070; step_loss: 0.1658
step 13080; step_loss: 0.1734
step 13090; step_loss: 0.1618
step 13100; step_loss: 0.1884
step 13110; step_loss: 0.2347
step 13120; step_loss: 0.1828
step 13130; step_loss: 0.1139
step 13140; step_loss: 0.1735
step 13150; step_loss: 0.0921
step 13160; step_loss: 0.1181
step 13170; step_loss: 0.1801
step 13180; step_loss: 0.1777
step 13190; step_loss: 0.1087
step 13200; step_loss: 0.1326
step 13210; step_loss: 0.1080
step 13220; step_loss: 0.0820
step 13230; step_loss: 0.1006
step 13240; step_loss: 0.1385
step 13250; step_loss: 0.1001
step 13260; step_loss: 0.1525
step 13270; step_loss: 0.1214
step 13280; step_loss: 0.1458
step 13290; step_loss: 0.1591
step 13300; step_loss: 0.2099
step 13310; step_loss: 0.1187
step 13320; step_loss: 0.1364
step 13330; step_loss: 0.1285
step 13340; step_loss: 0.1571
step 13350; step_loss: 0.0819
step 13360; step_loss: 0.0907
step 13370; step_loss: 0.0858
step 13380; step_loss: 0.1115
step 13390; step_loss: 0.0798
step 13400; step_loss: 0.0878
step 13410; step_loss: 0.0865
step 13420; step_loss: 0.0982
step 13430; step_loss: 0.1608
step 13440; step_loss: 0.0985
step 13450; step_loss: 0.1332
step 13460; step_loss: 0.1315
step 13470; step_loss: 0.2525
step 13480; step_loss: 0.2021
step 13490; step_loss: 0.1541
step 13500; step_loss: 0.1231
step 13510; step_loss: 0.0951
step 13520; step_loss: 0.1548
step 13530; step_loss: 0.1210
step 13540; step_loss: 0.0940
step 13550; step_loss: 0.1541
step 13560; step_loss: 0.1392
step 13570; step_loss: 0.1752
step 13580; step_loss: 0.1709
step 13590; step_loss: 0.1056
step 13600; step_loss: 0.1224
step 13610; step_loss: 0.2098
step 13620; step_loss: 0.1512
step 13630; step_loss: 0.1292
step 13640; step_loss: 0.0808
step 13650; step_loss: 0.1245
step 13660; step_loss: 0.1415
step 13670; step_loss: 0.1028
step 13680; step_loss: 0.1194
step 13690; step_loss: 0.1905
step 13700; step_loss: 0.1609
step 13710; step_loss: 0.0967
step 13720; step_loss: 0.0816
step 13730; step_loss: 0.1375
step 13740; step_loss: 0.2221
step 13750; step_loss: 0.1569
step 13760; step_loss: 0.1117
step 13770; step_loss: 0.1261
step 13780; step_loss: 0.1121
step 13790; step_loss: 0.1370
step 13800; step_loss: 0.1346
step 13810; step_loss: 0.1062
step 13820; step_loss: 0.1334
step 13830; step_loss: 0.1734
step 13840; step_loss: 0.0862
step 13850; step_loss: 0.1635
step 13860; step_loss: 0.2542
step 13870; step_loss: 0.2078
step 13880; step_loss: 0.1316
step 13890; step_loss: 0.1285
step 13900; step_loss: 0.1075
step 13910; step_loss: 0.1953
step 13920; step_loss: 0.1319
step 13930; step_loss: 0.1064
step 13940; step_loss: 0.1501
step 13950; step_loss: 0.1290
step 13960; step_loss: 0.2500
step 13970; step_loss: 0.0994
step 13980; step_loss: 0.2197
step 13990; step_loss: 0.1210

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 0.383 | 0.652 | 0.938 | 1.024 |   n/a |   n/a |

============================
Global step:         14000
Learning rate:       0.0047
Step-time (ms):     27.1457
Train loss avg:      0.1338
--------------------------
Val loss:            0.3298
srnn loss:           0.2860
============================

Saving the model...
done in 268.55 ms
step 14000; step_loss: 0.1535
step 14010; step_loss: 0.1088
step 14020; step_loss: 0.1002
step 14030; step_loss: 0.1286
step 14040; step_loss: 0.0839
step 14050; step_loss: 0.1103
step 14060; step_loss: 0.0977
step 14070; step_loss: 0.1025
step 14080; step_loss: 0.1991
step 14090; step_loss: 0.1386
step 14100; step_loss: 0.1193
step 14110; step_loss: 0.1263
step 14120; step_loss: 0.1063
step 14130; step_loss: 0.1543
step 14140; step_loss: 0.1564
step 14150; step_loss: 0.1853
step 14160; step_loss: 0.1232
step 14170; step_loss: 0.1714
step 14180; step_loss: 0.2078
step 14190; step_loss: 0.0982
step 14200; step_loss: 0.1134
step 14210; step_loss: 0.1269
step 14220; step_loss: 0.1357
step 14230; step_loss: 0.1311
step 14240; step_loss: 0.1711
step 14250; step_loss: 0.1000
step 14260; step_loss: 0.1016
step 14270; step_loss: 0.1373
step 14280; step_loss: 0.0949
step 14290; step_loss: 0.0850
step 14300; step_loss: 0.1424
step 14310; step_loss: 0.1365
step 14320; step_loss: 0.1263
step 14330; step_loss: 0.0935
step 14340; step_loss: 0.1582
step 14350; step_loss: 0.1052
step 14360; step_loss: 0.1462
step 14370; step_loss: 0.2353
step 14380; step_loss: 0.0847
step 14390; step_loss: 0.1572
step 14400; step_loss: 0.1214
step 14410; step_loss: 0.2332
step 14420; step_loss: 0.3348
step 14430; step_loss: 0.0926
step 14440; step_loss: 0.1062
step 14450; step_loss: 0.0916
step 14460; step_loss: 0.1510
step 14470; step_loss: 0.1444
step 14480; step_loss: 0.1070
step 14490; step_loss: 0.1417
step 14500; step_loss: 0.1233
step 14510; step_loss: 0.0987
step 14520; step_loss: 0.1492
step 14530; step_loss: 0.1198
step 14540; step_loss: 0.1428
step 14550; step_loss: 0.1482
step 14560; step_loss: 0.2246
step 14570; step_loss: 0.1817
step 14580; step_loss: 0.1089
step 14590; step_loss: 0.0772
step 14600; step_loss: 0.1761
step 14610; step_loss: 0.1012
step 14620; step_loss: 0.1100
step 14630; step_loss: 0.1439
step 14640; step_loss: 0.0999
step 14650; step_loss: 0.1248
step 14660; step_loss: 0.1267
step 14670; step_loss: 0.0893
step 14680; step_loss: 0.1206
step 14690; step_loss: 0.0919
step 14700; step_loss: 0.1146
step 14710; step_loss: 0.1165
step 14720; step_loss: 0.1131
step 14730; step_loss: 0.1120
step 14740; step_loss: 0.1659
step 14750; step_loss: 0.1074
step 14760; step_loss: 0.0977
step 14770; step_loss: 0.0935
step 14780; step_loss: 0.1519
step 14790; step_loss: 0.1900
step 14800; step_loss: 0.0953
step 14810; step_loss: 0.1600
step 14820; step_loss: 0.1818
step 14830; step_loss: 0.1149
step 14840; step_loss: 0.2043
step 14850; step_loss: 0.1047
step 14860; step_loss: 0.1278
step 14870; step_loss: 0.1102
step 14880; step_loss: 0.2816
step 14890; step_loss: 0.1219
step 14900; step_loss: 0.1045
step 14910; step_loss: 0.2219
step 14920; step_loss: 0.2097
step 14930; step_loss: 0.1102
step 14940; step_loss: 0.0965
step 14950; step_loss: 0.1268
step 14960; step_loss: 0.0861
step 14970; step_loss: 0.1011
step 14980; step_loss: 0.1278
step 14990; step_loss: 0.1228

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 0.370 | 0.630 | 0.910 | 0.986 |   n/a |   n/a |

============================
Global step:         15000
Learning rate:       0.0047
Step-time (ms):     27.1259
Train loss avg:      0.1292
--------------------------
Val loss:            0.3334
srnn loss:           0.2784
============================

Saving the model...
done in 272.62 ms
step 15000; step_loss: 0.1452
step 15010; step_loss: 0.1325
step 15020; step_loss: 0.1003
step 15030; step_loss: 0.1143
step 15040; step_loss: 0.1440
step 15050; step_loss: 0.0912
step 15060; step_loss: 0.0766
step 15070; step_loss: 0.2008
step 15080; step_loss: 0.1154
step 15090; step_loss: 0.0722
step 15100; step_loss: 0.1408
step 15110; step_loss: 0.1324
step 15120; step_loss: 0.0857
step 15130; step_loss: 0.1758
step 15140; step_loss: 0.0911
step 15150; step_loss: 0.1313
step 15160; step_loss: 0.1263
step 15170; step_loss: 0.1383
step 15180; step_loss: 0.0825
step 15190; step_loss: 0.0940
step 15200; step_loss: 0.1054
step 15210; step_loss: 0.1882
step 15220; step_loss: 0.1120
step 15230; step_loss: 0.0821
step 15240; step_loss: 0.1452
step 15250; step_loss: 0.1466
step 15260; step_loss: 0.0980
step 15270; step_loss: 0.0986
step 15280; step_loss: 0.0828
step 15290; step_loss: 0.0931
step 15300; step_loss: 0.1286
step 15310; step_loss: 0.0857
step 15320; step_loss: 0.1050
step 15330; step_loss: 0.1229
step 15340; step_loss: 0.1004
step 15350; step_loss: 0.0863
step 15360; step_loss: 0.0883
step 15370; step_loss: 0.1090
step 15380; step_loss: 0.2184
step 15390; step_loss: 0.1659
step 15400; step_loss: 0.1112
step 15410; step_loss: 0.1938
step 15420; step_loss: 0.0779
step 15430; step_loss: 0.1727
step 15440; step_loss: 0.2481
step 15450; step_loss: 0.1158
step 15460; step_loss: 0.1420
step 15470; step_loss: 0.0923
step 15480; step_loss: 0.2033
step 15490; step_loss: 0.0930
step 15500; step_loss: 0.1187
step 15510; step_loss: 0.1099
step 15520; step_loss: 0.1305
step 15530; step_loss: 0.1343
step 15540; step_loss: 0.0822
step 15550; step_loss: 0.1136
step 15560; step_loss: 0.1392
step 15570; step_loss: 0.0830
step 15580; step_loss: 0.0926
step 15590; step_loss: 0.2687
step 15600; step_loss: 0.1213
step 15610; step_loss: 0.1151
step 15620; step_loss: 0.1488
step 15630; step_loss: 0.2859
step 15640; step_loss: 0.1133
step 15650; step_loss: 0.1600
step 15660; step_loss: 0.0951
step 15670; step_loss: 0.0973
step 15680; step_loss: 0.1107
step 15690; step_loss: 0.1177
step 15700; step_loss: 0.1107
step 15710; step_loss: 0.1615
step 15720; step_loss: 0.1952
step 15730; step_loss: 0.0849
step 15740; step_loss: 0.1115
step 15750; step_loss: 0.1384
step 15760; step_loss: 0.0899
step 15770; step_loss: 0.1447
step 15780; step_loss: 0.1153
step 15790; step_loss: 0.1159
step 15800; step_loss: 0.1024
step 15810; step_loss: 0.1029
step 15820; step_loss: 0.0947
step 15830; step_loss: 0.1141
step 15840; step_loss: 0.0896
step 15850; step_loss: 0.1991
step 15860; step_loss: 0.1711
step 15870; step_loss: 0.0963
step 15880; step_loss: 0.1219
step 15890; step_loss: 0.1798
step 15900; step_loss: 0.1539
step 15910; step_loss: 0.1439
step 15920; step_loss: 0.1665
step 15930; step_loss: 0.0674
step 15940; step_loss: 0.1018
step 15950; step_loss: 0.1348
step 15960; step_loss: 0.1142
step 15970; step_loss: 0.1433
step 15980; step_loss: 0.1516
step 15990; step_loss: 0.1845

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 0.377 | 0.643 | 0.923 | 1.005 |   n/a |   n/a |

============================
Global step:         16000
Learning rate:       0.0047
Step-time (ms):     27.2464
Train loss avg:      0.1300
--------------------------
Val loss:            0.3447
srnn loss:           0.2784
============================

Saving the model...
done in 266.90 ms
step 16000; step_loss: 0.1025
step 16010; step_loss: 0.0858
step 16020; step_loss: 0.1234
step 16030; step_loss: 0.1180
step 16040; step_loss: 0.1159
step 16050; step_loss: 0.1302
step 16060; step_loss: 0.0997
step 16070; step_loss: 0.1298
step 16080; step_loss: 0.1189
step 16090; step_loss: 0.1535
step 16100; step_loss: 0.1395
step 16110; step_loss: 0.1360
step 16120; step_loss: 0.1977
step 16130; step_loss: 0.1163
step 16140; step_loss: 0.1348
step 16150; step_loss: 0.1035
step 16160; step_loss: 0.1171
step 16170; step_loss: 0.0984
step 16180; step_loss: 0.1642
step 16190; step_loss: 0.1496
step 16200; step_loss: 0.0932
step 16210; step_loss: 0.0959
step 16220; step_loss: 0.0939
step 16230; step_loss: 0.1256
step 16240; step_loss: 0.1476
step 16250; step_loss: 0.0961
step 16260; step_loss: 0.1317
step 16270; step_loss: 0.1253
step 16280; step_loss: 0.1371
step 16290; step_loss: 0.0906
step 16300; step_loss: 0.1254
step 16310; step_loss: 0.0858
step 16320; step_loss: 0.1207
step 16330; step_loss: 0.1257
step 16340; step_loss: 0.1022
step 16350; step_loss: 0.1256
step 16360; step_loss: 0.1304
step 16370; step_loss: 0.1207
step 16380; step_loss: 0.1198
step 16390; step_loss: 0.1385
step 16400; step_loss: 0.1247
step 16410; step_loss: 0.1537
step 16420; step_loss: 0.0782
step 16430; step_loss: 0.1468
step 16440; step_loss: 0.1375
step 16450; step_loss: 0.1056
step 16460; step_loss: 0.1323
step 16470; step_loss: 0.1181
step 16480; step_loss: 0.1020
step 16490; step_loss: 0.1135
step 16500; step_loss: 0.1086
step 16510; step_loss: 0.2319
step 16520; step_loss: 0.1040
step 16530; step_loss: 0.1011
step 16540; step_loss: 0.1614
step 16550; step_loss: 0.1262
step 16560; step_loss: 0.1017
step 16570; step_loss: 0.1225
step 16580; step_loss: 0.1411
step 16590; step_loss: 0.0997
step 16600; step_loss: 0.1502
step 16610; step_loss: 0.1984
step 16620; step_loss: 0.1485
step 16630; step_loss: 0.1153
step 16640; step_loss: 0.1956
step 16650; step_loss: 0.1164
step 16660; step_loss: 0.1354
step 16670; step_loss: 0.0841
step 16680; step_loss: 0.1278
step 16690; step_loss: 0.1014
step 16700; step_loss: 0.1951
step 16710; step_loss: 0.2824
step 16720; step_loss: 0.1240
step 16730; step_loss: 0.1340
step 16740; step_loss: 0.1233
step 16750; step_loss: 0.0968
step 16760; step_loss: 0.1259
step 16770; step_loss: 0.1939
step 16780; step_loss: 0.1342
step 16790; step_loss: 0.1052
step 16800; step_loss: 0.1074
step 16810; step_loss: 0.1116
step 16820; step_loss: 0.1760
step 16830; step_loss: 0.1249
step 16840; step_loss: 0.1159
step 16850; step_loss: 0.0853
step 16860; step_loss: 0.0875
step 16870; step_loss: 0.1044
step 16880; step_loss: 0.1064
step 16890; step_loss: 0.0902
step 16900; step_loss: 0.1237
step 16910; step_loss: 0.0724
step 16920; step_loss: 0.1109
step 16930; step_loss: 0.1750
step 16940; step_loss: 0.0944
step 16950; step_loss: 0.1534
step 16960; step_loss: 0.1323
step 16970; step_loss: 0.1493
step 16980; step_loss: 0.0790
step 16990; step_loss: 0.1256

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 0.379 | 0.646 | 0.932 | 1.016 |   n/a |   n/a |

============================
Global step:         17000
Learning rate:       0.0047
Step-time (ms):     27.1364
Train loss avg:      0.1277
--------------------------
Val loss:            0.3639
srnn loss:           0.2906
============================

Saving the model...
done in 273.01 ms
step 17000; step_loss: 0.0961
step 17010; step_loss: 0.1437
step 17020; step_loss: 0.0836
step 17030; step_loss: 0.1392
step 17040; step_loss: 0.1400
step 17050; step_loss: 0.1149
step 17060; step_loss: 0.0961
step 17070; step_loss: 0.0943
step 17080; step_loss: 0.1292
step 17090; step_loss: 0.1964
step 17100; step_loss: 0.1193
step 17110; step_loss: 0.1349
step 17120; step_loss: 0.0975
step 17130; step_loss: 0.1072
step 17140; step_loss: 0.1465
step 17150; step_loss: 0.1283
step 17160; step_loss: 0.1038
step 17170; step_loss: 0.2477
step 17180; step_loss: 0.1299
step 17190; step_loss: 0.2626
step 17200; step_loss: 0.1230
step 17210; step_loss: 0.1244
step 17220; step_loss: 0.1580
step 17230; step_loss: 0.1151
step 17240; step_loss: 0.1943
step 17250; step_loss: 0.1262
step 17260; step_loss: 0.1062
step 17270; step_loss: 0.1029
step 17280; step_loss: 0.1274
step 17290; step_loss: 0.1175
step 17300; step_loss: 0.1188
step 17310; step_loss: 0.1210
step 17320; step_loss: 0.0823
step 17330; step_loss: 0.2379
step 17340; step_loss: 0.2221
step 17350; step_loss: 0.1898
step 17360; step_loss: 0.0940
step 17370; step_loss: 0.1007
step 17380; step_loss: 0.1078
step 17390; step_loss: 0.0933
step 17400; step_loss: 0.0993
step 17410; step_loss: 0.0986
step 17420; step_loss: 0.1367
step 17430; step_loss: 0.0915
step 17440; step_loss: 0.1479
step 17450; step_loss: 0.1074
step 17460; step_loss: 0.0968
step 17470; step_loss: 0.1037
step 17480; step_loss: 0.0893
step 17490; step_loss: 0.1121
step 17500; step_loss: 0.1188
step 17510; step_loss: 0.0898
step 17520; step_loss: 0.1239
step 17530; step_loss: 0.1879
step 17540; step_loss: 0.1039
step 17550; step_loss: 0.1045
step 17560; step_loss: 0.1035
step 17570; step_loss: 0.0922
step 17580; step_loss: 0.1131
step 17590; step_loss: 0.1549
step 17600; step_loss: 0.1242
step 17610; step_loss: 0.1314
step 17620; step_loss: 0.0923
step 17630; step_loss: 0.1701
step 17640; step_loss: 0.0988
step 17650; step_loss: 0.1846
step 17660; step_loss: 0.0856
step 17670; step_loss: 0.0840
step 17680; step_loss: 0.1551
step 17690; step_loss: 0.2426
step 17700; step_loss: 0.1207
step 17710; step_loss: 0.1047
step 17720; step_loss: 0.1289
step 17730; step_loss: 0.1371
step 17740; step_loss: 0.0825
step 17750; step_loss: 0.0890
step 17760; step_loss: 0.2038
step 17770; step_loss: 0.0860
step 17780; step_loss: 0.1846
step 17790; step_loss: 0.0822
step 17800; step_loss: 0.1057
step 17810; step_loss: 0.0929
step 17820; step_loss: 0.1149
step 17830; step_loss: 0.0849
step 17840; step_loss: 0.2154
step 17850; step_loss: 0.1779
step 17860; step_loss: 0.0734
step 17870; step_loss: 0.1299
step 17880; step_loss: 0.1051
step 17890; step_loss: 0.1082
step 17900; step_loss: 0.1003
step 17910; step_loss: 0.0915
step 17920; step_loss: 0.1717
step 17930; step_loss: 0.1886
step 17940; step_loss: 0.0858
step 17950; step_loss: 0.1262
step 17960; step_loss: 0.1365
step 17970; step_loss: 0.1021
step 17980; step_loss: 0.1584
step 17990; step_loss: 0.1100

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 0.378 | 0.651 | 0.956 | 1.049 |   n/a |   n/a |

============================
Global step:         18000
Learning rate:       0.0047
Step-time (ms):     27.1216
Train loss avg:      0.1253
--------------------------
Val loss:            0.3178
srnn loss:           0.2861
============================

Saving the model...
done in 263.40 ms
step 18000; step_loss: 0.1720
step 18010; step_loss: 0.1156
step 18020; step_loss: 0.1277
step 18030; step_loss: 0.0843
step 18040; step_loss: 0.1108
step 18050; step_loss: 0.1339
step 18060; step_loss: 0.1571
step 18070; step_loss: 0.0976
step 18080; step_loss: 0.0900
step 18090; step_loss: 0.1780
step 18100; step_loss: 0.1146
step 18110; step_loss: 0.0996
step 18120; step_loss: 0.1606
step 18130; step_loss: 0.1343
step 18140; step_loss: 0.1756
step 18150; step_loss: 0.2862
step 18160; step_loss: 0.1908
step 18170; step_loss: 0.2256
step 18180; step_loss: 0.1811
step 18190; step_loss: 0.1102
step 18200; step_loss: 0.1089
step 18210; step_loss: 0.0746
step 18220; step_loss: 0.1309
step 18230; step_loss: 0.0815
step 18240; step_loss: 0.1485
step 18250; step_loss: 0.0992
step 18260; step_loss: 0.1239
step 18270; step_loss: 0.1747
step 18280; step_loss: 0.1060
step 18290; step_loss: 0.1315
step 18300; step_loss: 0.1627
step 18310; step_loss: 0.1000
step 18320; step_loss: 0.0947
step 18330; step_loss: 0.1241
step 18340; step_loss: 0.0915
step 18350; step_loss: 0.1222
step 18360; step_loss: 0.1340
step 18370; step_loss: 0.1106
step 18380; step_loss: 0.1684
step 18390; step_loss: 0.1151
step 18400; step_loss: 0.1056
step 18410; step_loss: 0.0995
step 18420; step_loss: 0.1230
step 18430; step_loss: 0.0919
step 18440; step_loss: 0.0943
step 18450; step_loss: 0.0846
step 18460; step_loss: 0.0910
step 18470; step_loss: 0.0982
step 18480; step_loss: 0.0791
step 18490; step_loss: 0.1515
step 18500; step_loss: 0.0916
step 18510; step_loss: 0.0929
step 18520; step_loss: 0.0938
step 18530; step_loss: 0.0924
step 18540; step_loss: 0.1124
step 18550; step_loss: 0.1050
step 18560; step_loss: 0.1082
step 18570; step_loss: 0.2270
step 18580; step_loss: 0.1290
step 18590; step_loss: 0.1710
step 18600; step_loss: 0.1733
step 18610; step_loss: 0.1241
step 18620; step_loss: 0.1131
step 18630; step_loss: 0.0873
step 18640; step_loss: 0.1078
step 18650; step_loss: 0.0883
step 18660; step_loss: 0.1183
step 18670; step_loss: 0.1061
step 18680; step_loss: 0.2036
step 18690; step_loss: 0.2265
step 18700; step_loss: 0.1083
step 18710; step_loss: 0.1015
step 18720; step_loss: 0.1416
step 18730; step_loss: 0.2246
step 18740; step_loss: 0.0814
step 18750; step_loss: 0.1128
step 18760; step_loss: 0.1234
step 18770; step_loss: 0.1231
step 18780; step_loss: 0.1470
step 18790; step_loss: 0.1040
step 18800; step_loss: 0.1302
step 18810; step_loss: 0.1176
step 18820; step_loss: 0.1134
step 18830; step_loss: 0.0819
step 18840; step_loss: 0.1575
step 18850; step_loss: 0.0690
step 18860; step_loss: 0.1610
step 18870; step_loss: 0.1038
step 18880; step_loss: 0.1042
step 18890; step_loss: 0.1270
step 18900; step_loss: 0.1065
step 18910; step_loss: 0.1190
step 18920; step_loss: 0.1813
step 18930; step_loss: 0.1331
step 18940; step_loss: 0.1228
step 18950; step_loss: 0.0863
step 18960; step_loss: 0.1734
step 18970; step_loss: 0.1340
step 18980; step_loss: 0.0801
step 18990; step_loss: 0.1182

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 0.378 | 0.648 | 0.944 | 1.041 |   n/a |   n/a |

============================
Global step:         19000
Learning rate:       0.0047
Step-time (ms):     27.1332
Train loss avg:      0.1243
--------------------------
Val loss:            0.3450
srnn loss:           0.2843
============================

Saving the model...
done in 269.65 ms
step 19000; step_loss: 0.1139
step 19010; step_loss: 0.0928
step 19020; step_loss: 0.1160
step 19030; step_loss: 0.0930
step 19040; step_loss: 0.1692
step 19050; step_loss: 0.1126
step 19060; step_loss: 0.1959
step 19070; step_loss: 0.1315
step 19080; step_loss: 0.1054
step 19090; step_loss: 0.0855
step 19100; step_loss: 0.0911
step 19110; step_loss: 0.2367
step 19120; step_loss: 0.1110
step 19130; step_loss: 0.0836
step 19140; step_loss: 0.0990
step 19150; step_loss: 0.1358
step 19160; step_loss: 0.0937
step 19170; step_loss: 0.0944
step 19180; step_loss: 0.0692
step 19190; step_loss: 0.1032
step 19200; step_loss: 0.1143
step 19210; step_loss: 0.1026
step 19220; step_loss: 0.1365
step 19230; step_loss: 0.1408
step 19240; step_loss: 0.1012
step 19250; step_loss: 0.1115
step 19260; step_loss: 0.1132
step 19270; step_loss: 0.0939
step 19280; step_loss: 0.0816
step 19290; step_loss: 0.0827
step 19300; step_loss: 0.0944
step 19310; step_loss: 0.1117
step 19320; step_loss: 0.1170
step 19330; step_loss: 0.1106
step 19340; step_loss: 0.1510
step 19350; step_loss: 0.0922
step 19360; step_loss: 0.0826
step 19370; step_loss: 0.0769
step 19380; step_loss: 0.0869
step 19390; step_loss: 0.1298
step 19400; step_loss: 0.1104
step 19410; step_loss: 0.0973
step 19420; step_loss: 0.2212
step 19430; step_loss: 0.1365
step 19440; step_loss: 0.1082
step 19450; step_loss: 0.0852
step 19460; step_loss: 0.1634
step 19470; step_loss: 0.1491
step 19480; step_loss: 0.1176
step 19490; step_loss: 0.1227
step 19500; step_loss: 0.2343
step 19510; step_loss: 0.1145
step 19520; step_loss: 0.0789
step 19530; step_loss: 0.1240
step 19540; step_loss: 0.1001
step 19550; step_loss: 0.1096
step 19560; step_loss: 0.1226
step 19570; step_loss: 0.1186
step 19580; step_loss: 0.1815
step 19590; step_loss: 0.0991
step 19600; step_loss: 0.0968
step 19610; step_loss: 0.2033
step 19620; step_loss: 0.1147
step 19630; step_loss: 0.1132
step 19640; step_loss: 0.1347
step 19650; step_loss: 0.1367
step 19660; step_loss: 0.1082
step 19670; step_loss: 0.2395
step 19680; step_loss: 0.1025
step 19690; step_loss: 0.1203
step 19700; step_loss: 0.0920
step 19710; step_loss: 0.1996
step 19720; step_loss: 0.1012
step 19730; step_loss: 0.1007
step 19740; step_loss: 0.1182
step 19750; step_loss: 0.1084
step 19760; step_loss: 0.1137
step 19770; step_loss: 0.1310
step 19780; step_loss: 0.1249
step 19790; step_loss: 0.1500
step 19800; step_loss: 0.1336
step 19810; step_loss: 0.1388
step 19820; step_loss: 0.0932
step 19830; step_loss: 0.1602
step 19840; step_loss: 0.0983
step 19850; step_loss: 0.1596
step 19860; step_loss: 0.1116
step 19870; step_loss: 0.0945
step 19880; step_loss: 0.0949
step 19890; step_loss: 0.1750
step 19900; step_loss: 0.1271
step 19910; step_loss: 0.0957
step 19920; step_loss: 0.1047
step 19930; step_loss: 0.1036
step 19940; step_loss: 0.1456
step 19950; step_loss: 0.0805
step 19960; step_loss: 0.0879
step 19970; step_loss: 0.1295
step 19980; step_loss: 0.0892
step 19990; step_loss: 0.1481

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 0.379 | 0.653 | 0.960 | 1.057 |   n/a |   n/a |

============================
Global step:         20000
Learning rate:       0.0045
Step-time (ms):     27.1605
Train loss avg:      0.1223
--------------------------
Val loss:            0.2894
srnn loss:           0.2864
============================

Saving the model...
done in 271.23 ms
step 20000; step_loss: 0.1700
step 20010; step_loss: 0.0837
step 20020; step_loss: 0.1057
step 20030; step_loss: 0.0867
step 20040; step_loss: 0.0971
step 20050; step_loss: 0.2662
step 20060; step_loss: 0.1315
step 20070; step_loss: 0.1035
step 20080; step_loss: 0.1103
step 20090; step_loss: 0.0966
step 20100; step_loss: 0.1019
step 20110; step_loss: 0.1162
step 20120; step_loss: 0.0752
step 20130; step_loss: 0.1147
step 20140; step_loss: 0.0921
step 20150; step_loss: 0.1142
step 20160; step_loss: 0.1800
step 20170; step_loss: 0.0850
step 20180; step_loss: 0.1472
step 20190; step_loss: 0.1441
step 20200; step_loss: 0.1331
step 20210; step_loss: 0.2521
step 20220; step_loss: 0.2097
step 20230; step_loss: 0.1217
step 20240; step_loss: 0.1338
step 20250; step_loss: 0.0928
step 20260; step_loss: 0.1054
step 20270; step_loss: 0.0872
step 20280; step_loss: 0.1954
step 20290; step_loss: 0.1122
step 20300; step_loss: 0.1150
step 20310; step_loss: 0.1173
step 20320; step_loss: 0.1322
step 20330; step_loss: 0.1053
step 20340; step_loss: 0.0780
step 20350; step_loss: 0.1358
step 20360; step_loss: 0.0820
step 20370; step_loss: 0.0822
step 20380; step_loss: 0.0994
step 20390; step_loss: 0.0945
step 20400; step_loss: 0.1275
step 20410; step_loss: 0.1194
step 20420; step_loss: 0.0973
step 20430; step_loss: 0.0903
step 20440; step_loss: 0.1490
step 20450; step_loss: 0.0897
step 20460; step_loss: 0.0991
step 20470; step_loss: 0.0999
step 20480; step_loss: 0.1348
step 20490; step_loss: 0.1249
step 20500; step_loss: 0.1440
step 20510; step_loss: 0.1336
step 20520; step_loss: 0.1682
step 20530; step_loss: 0.2050
step 20540; step_loss: 0.1845
step 20550; step_loss: 0.1964
step 20560; step_loss: 0.1622
step 20570; step_loss: 0.0818
step 20580; step_loss: 0.1014
step 20590; step_loss: 0.0895
step 20600; step_loss: 0.2336
step 20610; step_loss: 0.1633
step 20620; step_loss: 0.1565
step 20630; step_loss: 0.1003
step 20640; step_loss: 0.1155
step 20650; step_loss: 0.0883
step 20660; step_loss: 0.1032
step 20670; step_loss: 0.0828
step 20680; step_loss: 0.0858
step 20690; step_loss: 0.1116
step 20700; step_loss: 0.0700
step 20710; step_loss: 0.0968
step 20720; step_loss: 0.0726
step 20730; step_loss: 0.1082
step 20740; step_loss: 0.1018
step 20750; step_loss: 0.0948
step 20760; step_loss: 0.1075
step 20770; step_loss: 0.1418
step 20780; step_loss: 0.1217
step 20790; step_loss: 0.0915
step 20800; step_loss: 0.0921
step 20810; step_loss: 0.1046
step 20820; step_loss: 0.2161
step 20830; step_loss: 0.1386
step 20840; step_loss: 0.1151
step 20850; step_loss: 0.1723
step 20860; step_loss: 0.0831
step 20870; step_loss: 0.0846
step 20880; step_loss: 0.1248
step 20890; step_loss: 0.1669
step 20900; step_loss: 0.1656
step 20910; step_loss: 0.1884
step 20920; step_loss: 0.0933
step 20930; step_loss: 0.1030
step 20940; step_loss: 0.1095
step 20950; step_loss: 0.1103
step 20960; step_loss: 0.1236
step 20970; step_loss: 0.0894
step 20980; step_loss: 0.1596
step 20990; step_loss: 0.1349

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 0.372 | 0.640 | 0.940 | 1.037 |   n/a |   n/a |

============================
Global step:         21000
Learning rate:       0.0045
Step-time (ms):     27.1313
Train loss avg:      0.1218
--------------------------
Val loss:            0.2790
srnn loss:           0.2828
============================

Saving the model...
done in 262.33 ms
step 21000; step_loss: 0.1570
step 21010; step_loss: 0.0742
step 21020; step_loss: 0.0874
step 21030; step_loss: 0.1364
step 21040; step_loss: 0.1364
step 21050; step_loss: 0.1199
step 21060; step_loss: 0.1040
step 21070; step_loss: 0.1084
step 21080; step_loss: 0.0891
step 21090; step_loss: 0.1725
step 21100; step_loss: 0.1106
step 21110; step_loss: 0.1184
step 21120; step_loss: 0.1320
step 21130; step_loss: 0.1533
step 21140; step_loss: 0.1408
step 21150; step_loss: 0.1940
step 21160; step_loss: 0.1253
step 21170; step_loss: 0.1328
step 21180; step_loss: 0.1590
step 21190; step_loss: 0.1286
step 21200; step_loss: 0.0927
step 21210; step_loss: 0.0921
step 21220; step_loss: 0.0893
step 21230; step_loss: 0.1199
step 21240; step_loss: 0.1484
step 21250; step_loss: 0.0923
step 21260; step_loss: 0.0962
step 21270; step_loss: 0.0901
step 21280; step_loss: 0.0952
step 21290; step_loss: 0.1587
step 21300; step_loss: 0.1100
step 21310; step_loss: 0.1334
step 21320; step_loss: 0.1013
step 21330; step_loss: 0.1219
step 21340; step_loss: 0.1288
step 21350; step_loss: 0.1374
step 21360; step_loss: 0.1335
step 21370; step_loss: 0.1082
step 21380; step_loss: 0.1302
step 21390; step_loss: 0.1157
step 21400; step_loss: 0.2554
step 21410; step_loss: 0.1106
step 21420; step_loss: 0.1744
step 21430; step_loss: 0.0967
step 21440; step_loss: 0.1092
step 21450; step_loss: 0.1302
step 21460; step_loss: 0.1138
step 21470; step_loss: 0.0731
step 21480; step_loss: 0.0955
step 21490; step_loss: 0.0896
step 21500; step_loss: 0.1218
step 21510; step_loss: 0.1688
step 21520; step_loss: 0.0975
step 21530; step_loss: 0.1027
step 21540; step_loss: 0.1374
step 21550; step_loss: 0.2461
step 21560; step_loss: 0.0997
step 21570; step_loss: 0.1234
step 21580; step_loss: 0.0937
step 21590; step_loss: 0.0922
step 21600; step_loss: 0.1406
step 21610; step_loss: 0.2465
step 21620; step_loss: 0.1262
step 21630; step_loss: 0.1703
step 21640; step_loss: 0.1949
step 21650; step_loss: 0.1294
step 21660; step_loss: 0.1825
step 21670; step_loss: 0.0886
step 21680; step_loss: 0.1966
step 21690; step_loss: 0.1451
step 21700; step_loss: 0.1092
step 21710; step_loss: 0.1463
step 21720; step_loss: 0.0830
step 21730; step_loss: 0.1575
step 21740; step_loss: 0.0949
step 21750; step_loss: 0.1063
step 21760; step_loss: 0.0938
step 21770; step_loss: 0.1215
step 21780; step_loss: 0.2958
step 21790; step_loss: 0.0945
step 21800; step_loss: 0.0985
step 21810; step_loss: 0.0853
step 21820; step_loss: 0.0786
step 21830; step_loss: 0.1044
step 21840; step_loss: 0.0909
step 21850; step_loss: 0.0973
step 21860; step_loss: 0.0941
step 21870; step_loss: 0.1159
step 21880; step_loss: 0.0765
step 21890; step_loss: 0.0799
step 21900; step_loss: 0.1194
step 21910; step_loss: 0.1571
step 21920; step_loss: 0.0846
step 21930; step_loss: 0.1452
step 21940; step_loss: 0.1623
step 21950; step_loss: 0.1246
step 21960; step_loss: 0.1032
step 21970; step_loss: 0.1317
step 21980; step_loss: 0.1660
step 21990; step_loss: 0.1248

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 0.374 | 0.643 | 0.944 | 1.039 |   n/a |   n/a |

============================
Global step:         22000
Learning rate:       0.0045
Step-time (ms):     27.1016
Train loss avg:      0.1223
--------------------------
Val loss:            0.3745
srnn loss:           0.2925
============================

Saving the model...
done in 271.43 ms
step 22000; step_loss: 0.0849
step 22010; step_loss: 0.1318
step 22020; step_loss: 0.1226
step 22030; step_loss: 0.1056
step 22040; step_loss: 0.0988
step 22050; step_loss: 0.0904
step 22060; step_loss: 0.1574
step 22070; step_loss: 0.1571
step 22080; step_loss: 0.0867
step 22090; step_loss: 0.1123
step 22100; step_loss: 0.1473
step 22110; step_loss: 0.1001
step 22120; step_loss: 0.0950
step 22130; step_loss: 0.1406
step 22140; step_loss: 0.1894
step 22150; step_loss: 0.1088
step 22160; step_loss: 0.1058
step 22170; step_loss: 0.1456
step 22180; step_loss: 0.1134
step 22190; step_loss: 0.1006
step 22200; step_loss: 0.1033
step 22210; step_loss: 0.1409
step 22220; step_loss: 0.1699
step 22230; step_loss: 0.0736
step 22240; step_loss: 0.0858
step 22250; step_loss: 0.2345
step 22260; step_loss: 0.1083
step 22270; step_loss: 0.1642
step 22280; step_loss: 0.1145
step 22290; step_loss: 0.1195
step 22300; step_loss: 0.0962
step 22310; step_loss: 0.0735
step 22320; step_loss: 0.1162
step 22330; step_loss: 0.1247
step 22340; step_loss: 0.1056
step 22350; step_loss: 0.0948
step 22360; step_loss: 0.1155
step 22370; step_loss: 0.0941
step 22380; step_loss: 0.1118
step 22390; step_loss: 0.0982
step 22400; step_loss: 0.1262
step 22410; step_loss: 0.0777
step 22420; step_loss: 0.1747
step 22430; step_loss: 0.1013
step 22440; step_loss: 0.1467
step 22450; step_loss: 0.1070
step 22460; step_loss: 0.1299
step 22470; step_loss: 0.1285
step 22480; step_loss: 0.1127
step 22490; step_loss: 0.0903
step 22500; step_loss: 0.2179
step 22510; step_loss: 0.1205
step 22520; step_loss: 0.1225
step 22530; step_loss: 0.0994
step 22540; step_loss: 0.1464
step 22550; step_loss: 0.1052
step 22560; step_loss: 0.1045
step 22570; step_loss: 0.1057
step 22580; step_loss: 0.1314
step 22590; step_loss: 0.1479
step 22600; step_loss: 0.1109
step 22610; step_loss: 0.1383
step 22620; step_loss: 0.1147
step 22630; step_loss: 0.0947
step 22640; step_loss: 0.0851
step 22650; step_loss: 0.0800
step 22660; step_loss: 0.1102
step 22670; step_loss: 0.1061
step 22680; step_loss: 0.0854
step 22690; step_loss: 0.1330
step 22700; step_loss: 0.1131
step 22710; step_loss: 0.1141
step 22720; step_loss: 0.1038
step 22730; step_loss: 0.1343
step 22740; step_loss: 0.1428
step 22750; step_loss: 0.1334
step 22760; step_loss: 0.0837
step 22770; step_loss: 0.1136
step 22780; step_loss: 0.1020
step 22790; step_loss: 0.0861
step 22800; step_loss: 0.1127
step 22810; step_loss: 0.1420
step 22820; step_loss: 0.0801
step 22830; step_loss: 0.0772
step 22840; step_loss: 0.0907
step 22850; step_loss: 0.0886
step 22860; step_loss: 0.0830
step 22870; step_loss: 0.0919
step 22880; step_loss: 0.1088
step 22890; step_loss: 0.1248
step 22900; step_loss: 0.1107
step 22910; step_loss: 0.1154
step 22920; step_loss: 0.0913
step 22930; step_loss: 0.1238
step 22940; step_loss: 0.1327
step 22950; step_loss: 0.0904
step 22960; step_loss: 0.0889
step 22970; step_loss: 0.1513
step 22980; step_loss: 0.1238
step 22990; step_loss: 0.1498

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 0.373 | 0.641 | 0.937 | 1.029 |   n/a |   n/a |

============================
Global step:         23000
Learning rate:       0.0045
Step-time (ms):     27.1249
Train loss avg:      0.1194
--------------------------
Val loss:            0.3476
srnn loss:           0.2873
============================

Saving the model...
done in 275.57 ms
step 23000; step_loss: 0.1046
step 23010; step_loss: 0.2041
step 23020; step_loss: 0.0868
step 23030; step_loss: 0.1253
step 23040; step_loss: 0.0886
step 23050; step_loss: 0.1261
step 23060; step_loss: 0.1254
step 23070; step_loss: 0.1129
step 23080; step_loss: 0.1111
step 23090; step_loss: 0.1064
step 23100; step_loss: 0.1036
step 23110; step_loss: 0.1166
step 23120; step_loss: 0.0876
step 23130; step_loss: 0.1070
step 23140; step_loss: 0.0759
step 23150; step_loss: 0.1112
step 23160; step_loss: 0.1176
step 23170; step_loss: 0.1491
step 23180; step_loss: 0.1158
step 23190; step_loss: 0.2162
step 23200; step_loss: 0.0846
step 23210; step_loss: 0.1228
step 23220; step_loss: 0.1469
step 23230; step_loss: 0.1167
step 23240; step_loss: 0.1297
step 23250; step_loss: 0.1469
step 23260; step_loss: 0.1052
step 23270; step_loss: 0.1026
step 23280; step_loss: 0.0801
step 23290; step_loss: 0.1074
step 23300; step_loss: 0.1653
step 23310; step_loss: 0.2120
step 23320; step_loss: 0.1283
step 23330; step_loss: 0.0884
step 23340; step_loss: 0.1033
step 23350; step_loss: 0.0921
step 23360; step_loss: 0.1299
step 23370; step_loss: 0.1731
step 23380; step_loss: 0.0712
step 23390; step_loss: 0.1516
step 23400; step_loss: 0.1155
step 23410; step_loss: 0.2726
step 23420; step_loss: 0.1071
step 23430; step_loss: 0.0697
step 23440; step_loss: 0.0953
step 23450; step_loss: 0.1067
step 23460; step_loss: 0.1180
step 23470; step_loss: 0.0930
step 23480; step_loss: 0.1054
step 23490; step_loss: 0.0764
step 23500; step_loss: 0.0998
step 23510; step_loss: 0.0818
step 23520; step_loss: 0.0766
step 23530; step_loss: 0.0827
step 23540; step_loss: 0.1154
step 23550; step_loss: 0.1060
step 23560; step_loss: 0.1381
step 23570; step_loss: 0.0952
step 23580; step_loss: 0.1322
step 23590; step_loss: 0.1139
step 23600; step_loss: 0.0850
step 23610; step_loss: 0.1402
step 23620; step_loss: 0.0913
step 23630; step_loss: 0.1510
step 23640; step_loss: 0.1178
step 23650; step_loss: 0.0865
step 23660; step_loss: 0.1282
step 23670; step_loss: 0.0999
step 23680; step_loss: 0.1184
step 23690; step_loss: 0.1279
step 23700; step_loss: 0.1327
step 23710; step_loss: 0.1021
step 23720; step_loss: 0.0976
step 23730; step_loss: 0.1075
step 23740; step_loss: 0.1476
step 23750; step_loss: 0.1149
step 23760; step_loss: 0.1025
step 23770; step_loss: 0.0968
step 23780; step_loss: 0.0844
step 23790; step_loss: 0.1046
step 23800; step_loss: 0.1485
step 23810; step_loss: 0.1458
step 23820; step_loss: 0.0912
step 23830; step_loss: 0.1133
step 23840; step_loss: 0.1072
step 23850; step_loss: 0.1237
step 23860; step_loss: 0.0924
step 23870; step_loss: 0.1940
step 23880; step_loss: 0.0960
step 23890; step_loss: 0.1022
step 23900; step_loss: 0.0955
step 23910; step_loss: 0.1124
step 23920; step_loss: 0.1064
step 23930; step_loss: 0.0895
step 23940; step_loss: 0.1378
step 23950; step_loss: 0.1044
step 23960; step_loss: 0.1202
step 23970; step_loss: 0.0879
step 23980; step_loss: 0.1231
step 23990; step_loss: 0.0820

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 0.375 | 0.649 | 0.959 | 1.061 |   n/a |   n/a |

============================
Global step:         24000
Learning rate:       0.0045
Step-time (ms):     27.1089
Train loss avg:      0.1169
--------------------------
Val loss:            0.3563
srnn loss:           0.2891
============================

Saving the model...
done in 265.80 ms
step 24000; step_loss: 0.1237
step 24010; step_loss: 0.1200
step 24020; step_loss: 0.0880
step 24030; step_loss: 0.0723
step 24040; step_loss: 0.1018
step 24050; step_loss: 0.1186
step 24060; step_loss: 0.1254
step 24070; step_loss: 0.0783
step 24080; step_loss: 0.0767
step 24090; step_loss: 0.0881
step 24100; step_loss: 0.1358
step 24110; step_loss: 0.1192
step 24120; step_loss: 0.0999
step 24130; step_loss: 0.1002
step 24140; step_loss: 0.0949
step 24150; step_loss: 0.2203
step 24160; step_loss: 0.0776
step 24170; step_loss: 0.1302
step 24180; step_loss: 0.0837
step 24190; step_loss: 0.1106
step 24200; step_loss: 0.0962
step 24210; step_loss: 0.1859
step 24220; step_loss: 0.1592
step 24230; step_loss: 0.1425
step 24240; step_loss: 0.1090
step 24250; step_loss: 0.1090
step 24260; step_loss: 0.1029
step 24270; step_loss: 0.1357
step 24280; step_loss: 0.0745
step 24290; step_loss: 0.1302
step 24300; step_loss: 0.0802
step 24310; step_loss: 0.0964
step 24320; step_loss: 0.1014
step 24330; step_loss: 0.0851
step 24340; step_loss: 0.1440
step 24350; step_loss: 0.0845
step 24360; step_loss: 0.0909
step 24370; step_loss: 0.1417
step 24380; step_loss: 0.1068
step 24390; step_loss: 0.0856
step 24400; step_loss: 0.1409
step 24410; step_loss: 0.1316
step 24420; step_loss: 0.1171
step 24430; step_loss: 0.1048
step 24440; step_loss: 0.1017
step 24450; step_loss: 0.1133
step 24460; step_loss: 0.0905
step 24470; step_loss: 0.0809
step 24480; step_loss: 0.1282
step 24490; step_loss: 0.1173
step 24500; step_loss: 0.0987
step 24510; step_loss: 0.0922
step 24520; step_loss: 0.1056
step 24530; step_loss: 0.1105
step 24540; step_loss: 0.0947
step 24550; step_loss: 0.1786
step 24560; step_loss: 0.1202
step 24570; step_loss: 0.0745
step 24580; step_loss: 0.0971
step 24590; step_loss: 0.0833
step 24600; step_loss: 0.0905
step 24610; step_loss: 0.2659
step 24620; step_loss: 0.1220
step 24630; step_loss: 0.0758
step 24640; step_loss: 0.1062
step 24650; step_loss: 0.0800
step 24660; step_loss: 0.1210
step 24670; step_loss: 0.1392
step 24680; step_loss: 0.0961
step 24690; step_loss: 0.1377
step 24700; step_loss: 0.1315
step 24710; step_loss: 0.1370
step 24720; step_loss: 0.0845
step 24730; step_loss: 0.2293
step 24740; step_loss: 0.1740
step 24750; step_loss: 0.0792
step 24760; step_loss: 0.0940
step 24770; step_loss: 0.0887
step 24780; step_loss: 0.1353
step 24790; step_loss: 0.1090
step 24800; step_loss: 0.1938
step 24810; step_loss: 0.0699
step 24820; step_loss: 0.2106
step 24830; step_loss: 0.1278
step 24840; step_loss: 0.1111
step 24850; step_loss: 0.0633
step 24860; step_loss: 0.1070
step 24870; step_loss: 0.1473
step 24880; step_loss: 0.1087
step 24890; step_loss: 0.1066
step 24900; step_loss: 0.0972
step 24910; step_loss: 0.0788
step 24920; step_loss: 0.2340
step 24930; step_loss: 0.1667
step 24940; step_loss: 0.1341
step 24950; step_loss: 0.0701
step 24960; step_loss: 0.1054
step 24970; step_loss: 0.0862
step 24980; step_loss: 0.1110
step 24990; step_loss: 0.0953

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 0.371 | 0.639 | 0.935 | 1.028 |   n/a |   n/a |

============================
Global step:         25000
Learning rate:       0.0045
Step-time (ms):     27.1425
Train loss avg:      0.1180
--------------------------
Val loss:            0.3348
srnn loss:           0.2850
============================

Saving the model...
done in 275.34 ms
step 25000; step_loss: 0.0821
step 25010; step_loss: 0.0912
step 25020; step_loss: 0.1394
step 25030; step_loss: 0.1049
step 25040; step_loss: 0.0851
step 25050; step_loss: 0.0908
step 25060; step_loss: 0.0950
step 25070; step_loss: 0.1246
step 25080; step_loss: 0.1154
step 25090; step_loss: 0.1264
step 25100; step_loss: 0.0922
step 25110; step_loss: 0.1050
step 25120; step_loss: 0.0697
step 25130; step_loss: 0.0920
step 25140; step_loss: 0.1010
step 25150; step_loss: 0.0924
step 25160; step_loss: 0.0993
step 25170; step_loss: 0.0788
step 25180; step_loss: 0.1261
step 25190; step_loss: 0.1104
step 25200; step_loss: 0.0940
step 25210; step_loss: 0.1832
step 25220; step_loss: 0.0979
step 25230; step_loss: 0.0925
step 25240; step_loss: 0.1171
step 25250; step_loss: 0.1220
step 25260; step_loss: 0.0899
step 25270; step_loss: 0.0858
step 25280; step_loss: 0.1242
step 25290; step_loss: 0.1391
step 25300; step_loss: 0.1261
step 25310; step_loss: 0.1334
step 25320; step_loss: 0.1068
step 25330; step_loss: 0.1221
step 25340; step_loss: 0.1069
step 25350; step_loss: 0.1068
step 25360; step_loss: 0.1101
step 25370; step_loss: 0.0778
step 25380; step_loss: 0.1239
step 25390; step_loss: 0.1540
step 25400; step_loss: 0.0980
step 25410; step_loss: 0.1558
step 25420; step_loss: 0.2179
step 25430; step_loss: 0.2357
step 25440; step_loss: 0.1227
step 25450; step_loss: 0.1733
step 25460; step_loss: 0.1315
step 25470; step_loss: 0.2499
step 25480; step_loss: 0.2218
step 25490; step_loss: 0.3191
step 25500; step_loss: 0.0868
step 25510; step_loss: 0.2231
step 25520; step_loss: 0.2196
step 25530; step_loss: 0.1309
step 25540; step_loss: 0.1119
step 25550; step_loss: 0.1469
step 25560; step_loss: 0.1091
step 25570; step_loss: 0.1147
step 25580; step_loss: 0.0946
step 25590; step_loss: 0.0840
step 25600; step_loss: 0.0797
step 25610; step_loss: 0.1135
step 25620; step_loss: 0.0946
step 25630; step_loss: 0.1024
step 25640; step_loss: 0.1243
step 25650; step_loss: 0.0932
step 25660; step_loss: 0.0888
step 25670; step_loss: 0.0882
step 25680; step_loss: 0.0842
step 25690; step_loss: 0.1187
step 25700; step_loss: 0.1107
step 25710; step_loss: 0.0722
step 25720; step_loss: 0.2397
step 25730; step_loss: 0.1282
step 25740; step_loss: 0.1858
step 25750; step_loss: 0.1363
step 25760; step_loss: 0.0804
step 25770; step_loss: 0.0717
step 25780; step_loss: 0.1041
step 25790; step_loss: 0.0901
step 25800; step_loss: 0.2106
step 25810; step_loss: 0.1535
step 25820; step_loss: 0.0890
step 25830; step_loss: 0.1053
step 25840; step_loss: 0.0873
step 25850; step_loss: 0.1032
step 25860; step_loss: 0.0988
step 25870; step_loss: 0.1320
step 25880; step_loss: 0.0956
step 25890; step_loss: 0.1815
step 25900; step_loss: 0.0834
step 25910; step_loss: 0.1356
step 25920; step_loss: 0.0991
step 25930; step_loss: 0.1138
step 25940; step_loss: 0.1584
step 25950; step_loss: 0.1294
step 25960; step_loss: 0.0963
step 25970; step_loss: 0.1116
step 25980; step_loss: 0.0822
step 25990; step_loss: 0.1322

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 0.369 | 0.636 | 0.943 | 1.041 |   n/a |   n/a |

============================
Global step:         26000
Learning rate:       0.0045
Step-time (ms):     27.1879
Train loss avg:      0.1168
--------------------------
Val loss:            0.3732
srnn loss:           0.2845
============================

Saving the model...
done in 270.04 ms
step 26000; step_loss: 0.1013
step 26010; step_loss: 0.0895
step 26020; step_loss: 0.0897
step 26030; step_loss: 0.1285
step 26040; step_loss: 0.1764
step 26050; step_loss: 0.1233
step 26060; step_loss: 0.1401
step 26070; step_loss: 0.1499
step 26080; step_loss: 0.1080
step 26090; step_loss: 0.1429
step 26100; step_loss: 0.1050
step 26110; step_loss: 0.1623
step 26120; step_loss: 0.0774
step 26130; step_loss: 0.0685
step 26140; step_loss: 0.1793
step 26150; step_loss: 0.0819
step 26160; step_loss: 0.1020
step 26170; step_loss: 0.1646
step 26180; step_loss: 0.0653
step 26190; step_loss: 0.0876
step 26200; step_loss: 0.1028
step 26210; step_loss: 0.0804
step 26220; step_loss: 0.1272
step 26230; step_loss: 0.1393
step 26240; step_loss: 0.1014
step 26250; step_loss: 0.0718
step 26260; step_loss: 0.1321
step 26270; step_loss: 0.0829
step 26280; step_loss: 0.1100
step 26290; step_loss: 0.1179
step 26300; step_loss: 0.1380
step 26310; step_loss: 0.1175
step 26320; step_loss: 0.1083
step 26330; step_loss: 0.0834
step 26340; step_loss: 0.1452
step 26350; step_loss: 0.0808
step 26360; step_loss: 0.1144
step 26370; step_loss: 0.0917
step 26380; step_loss: 0.1669
step 26390; step_loss: 0.1061
step 26400; step_loss: 0.1135
step 26410; step_loss: 0.0998
step 26420; step_loss: 0.0953
step 26430; step_loss: 0.1038
step 26440; step_loss: 0.1033
step 26450; step_loss: 0.1317
step 26460; step_loss: 0.1156
step 26470; step_loss: 0.1212
step 26480; step_loss: 0.1737
step 26490; step_loss: 0.1265
step 26500; step_loss: 0.1891
step 26510; step_loss: 0.2219
step 26520; step_loss: 0.0828
step 26530; step_loss: 0.0771
step 26540; step_loss: 0.1016
step 26550; step_loss: 0.0942
step 26560; step_loss: 0.0813
step 26570; step_loss: 0.1723
step 26580; step_loss: 0.1490
step 26590; step_loss: 0.1447
step 26600; step_loss: 0.1667
step 26610; step_loss: 0.1036
step 26620; step_loss: 0.0849
step 26630; step_loss: 0.0910
step 26640; step_loss: 0.1431
step 26650; step_loss: 0.1014
step 26660; step_loss: 0.0903
step 26670; step_loss: 0.0825
step 26680; step_loss: 0.1195
step 26690; step_loss: 0.0916
step 26700; step_loss: 0.1069
step 26710; step_loss: 0.0998
step 26720; step_loss: 0.0788
step 26730; step_loss: 0.0744
step 26740; step_loss: 0.1571
step 26750; step_loss: 0.0774
step 26760; step_loss: 0.0944
step 26770; step_loss: 0.0770
step 26780; step_loss: 0.0876
step 26790; step_loss: 0.0853
step 26800; step_loss: 0.0992
step 26810; step_loss: 0.0847
step 26820; step_loss: 0.1181
step 26830; step_loss: 0.0783
step 26840; step_loss: 0.1805
step 26850; step_loss: 0.0912
step 26860; step_loss: 0.0949
step 26870; step_loss: 0.1214
step 26880; step_loss: 0.1025
step 26890; step_loss: 0.1145
step 26900; step_loss: 0.1087
step 26910; step_loss: 0.1213
step 26920; step_loss: 0.1161
step 26930; step_loss: 0.0963
step 26940; step_loss: 0.1210
step 26950; step_loss: 0.0895
step 26960; step_loss: 0.1123
step 26970; step_loss: 0.0938
step 26980; step_loss: 0.0786
step 26990; step_loss: 0.0839

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 0.374 | 0.647 | 0.955 | 1.051 |   n/a |   n/a |

============================
Global step:         27000
Learning rate:       0.0045
Step-time (ms):     27.2265
Train loss avg:      0.1169
--------------------------
Val loss:            0.3597
srnn loss:           0.2933
============================

Saving the model...
done in 276.48 ms
step 27000; step_loss: 0.0752
step 27010; step_loss: 0.1030
step 27020; step_loss: 0.0877
step 27030; step_loss: 0.0737
step 27040; step_loss: 0.1069
step 27050; step_loss: 0.0858
step 27060; step_loss: 0.1029
step 27070; step_loss: 0.0928
step 27080; step_loss: 0.2253
step 27090; step_loss: 0.0946
step 27100; step_loss: 0.0674
step 27110; step_loss: 0.1021
step 27120; step_loss: 0.0870
step 27130; step_loss: 0.1108
step 27140; step_loss: 0.0917
step 27150; step_loss: 0.1591
step 27160; step_loss: 0.0982
step 27170; step_loss: 0.0827
step 27180; step_loss: 0.0939
step 27190; step_loss: 0.0894
step 27200; step_loss: 0.1141
step 27210; step_loss: 0.0977
step 27220; step_loss: 0.1231
step 27230; step_loss: 0.2304
step 27240; step_loss: 0.0834
step 27250; step_loss: 0.1279
step 27260; step_loss: 0.0748
step 27270; step_loss: 0.1349
step 27280; step_loss: 0.1052
step 27290; step_loss: 0.1127
step 27300; step_loss: 0.1105
step 27310; step_loss: 0.0904
step 27320; step_loss: 0.1094
step 27330; step_loss: 0.1085
step 27340; step_loss: 0.0792
step 27350; step_loss: 0.1041
step 27360; step_loss: 0.1233
step 27370; step_loss: 0.1287
step 27380; step_loss: 0.1475
step 27390; step_loss: 0.2425
step 27400; step_loss: 0.0966
step 27410; step_loss: 0.0941
step 27420; step_loss: 0.1014
step 27430; step_loss: 0.0942
step 27440; step_loss: 0.1184
step 27450; step_loss: 0.1023
step 27460; step_loss: 0.1073
step 27470; step_loss: 0.1026
step 27480; step_loss: 0.1026
step 27490; step_loss: 0.0984
step 27500; step_loss: 0.1358
step 27510; step_loss: 0.1421
step 27520; step_loss: 0.0985
step 27530; step_loss: 0.1153
step 27540; step_loss: 0.1102
step 27550; step_loss: 0.1126
step 27560; step_loss: 0.0818
step 27570; step_loss: 0.0976
step 27580; step_loss: 0.1132
step 27590; step_loss: 0.1150
step 27600; step_loss: 0.1473
step 27610; step_loss: 0.0842
step 27620; step_loss: 0.0890
step 27630; step_loss: 0.1178
step 27640; step_loss: 0.2264
step 27650; step_loss: 0.1669
step 27660; step_loss: 0.1091
step 27670; step_loss: 0.0928
step 27680; step_loss: 0.1138
step 27690; step_loss: 0.1078
step 27700; step_loss: 0.1276
step 27710; step_loss: 0.0682
step 27720; step_loss: 0.0952
step 27730; step_loss: 0.1757
step 27740; step_loss: 0.1407
step 27750; step_loss: 0.1013
step 27760; step_loss: 0.1232
step 27770; step_loss: 0.1030
step 27780; step_loss: 0.1393
step 27790; step_loss: 0.0771
step 27800; step_loss: 0.0997
step 27810; step_loss: 0.1288
step 27820; step_loss: 0.0725
step 27830; step_loss: 0.1772
step 27840; step_loss: 0.0872
step 27850; step_loss: 0.0876
step 27860; step_loss: 0.1421
step 27870; step_loss: 0.1164
step 27880; step_loss: 0.1051
step 27890; step_loss: 0.1070
step 27900; step_loss: 0.0746
step 27910; step_loss: 0.0741
step 27920; step_loss: 0.1353
step 27930; step_loss: 0.1146
step 27940; step_loss: 0.0812
step 27950; step_loss: 0.1298
step 27960; step_loss: 0.1616
step 27970; step_loss: 0.0800
step 27980; step_loss: 0.1328
step 27990; step_loss: 0.0894

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 0.375 | 0.647 | 0.962 | 1.069 |   n/a |   n/a |

============================
Global step:         28000
Learning rate:       0.0045
Step-time (ms):     27.1066
Train loss avg:      0.1142
--------------------------
Val loss:            0.3394
srnn loss:           0.2871
============================

Saving the model...
done in 272.44 ms
step 28000; step_loss: 0.1148
step 28010; step_loss: 0.1116
step 28020; step_loss: 0.1191
step 28030; step_loss: 0.1037
step 28040; step_loss: 0.1219
step 28050; step_loss: 0.0957
step 28060; step_loss: 0.0960
step 28070; step_loss: 0.0764
step 28080; step_loss: 0.0941
step 28090; step_loss: 0.0737
step 28100; step_loss: 0.0886
step 28110; step_loss: 0.1383
step 28120; step_loss: 0.0866
step 28130; step_loss: 0.1363
step 28140; step_loss: 0.0864
step 28150; step_loss: 0.1019
step 28160; step_loss: 0.0956
step 28170; step_loss: 0.1130
step 28180; step_loss: 0.0887
step 28190; step_loss: 0.1015
step 28200; step_loss: 0.1323
step 28210; step_loss: 0.0779
step 28220; step_loss: 0.1092
step 28230; step_loss: 0.0860
step 28240; step_loss: 0.1068
step 28250; step_loss: 0.0567
step 28260; step_loss: 0.0756
step 28270; step_loss: 0.0985
step 28280; step_loss: 0.0773
step 28290; step_loss: 0.0979
step 28300; step_loss: 0.0979
step 28310; step_loss: 0.0819
step 28320; step_loss: 0.1629
step 28330; step_loss: 0.0834
step 28340; step_loss: 0.0757
step 28350; step_loss: 0.1064
step 28360; step_loss: 0.1893
step 28370; step_loss: 0.0946
step 28380; step_loss: 0.1906
step 28390; step_loss: 0.1530
step 28400; step_loss: 0.0713
step 28410; step_loss: 0.1105
step 28420; step_loss: 0.1094
step 28430; step_loss: 0.1149
step 28440; step_loss: 0.1565
step 28450; step_loss: 0.1043
step 28460; step_loss: 0.1175
step 28470; step_loss: 0.1635
step 28480; step_loss: 0.0815
step 28490; step_loss: 0.1304
step 28500; step_loss: 0.1369
step 28510; step_loss: 0.1948
step 28520; step_loss: 0.1249
step 28530; step_loss: 0.0630
step 28540; step_loss: 0.1495
step 28550; step_loss: 0.0962
step 28560; step_loss: 0.0838
step 28570; step_loss: 0.0834
step 28580; step_loss: 0.1466
step 28590; step_loss: 0.0877
step 28600; step_loss: 0.0777
step 28610; step_loss: 0.2484
step 28620; step_loss: 0.0940
step 28630; step_loss: 0.0945
step 28640; step_loss: 0.2237
step 28650; step_loss: 0.1734
step 28660; step_loss: 0.1163
step 28670; step_loss: 0.1163
step 28680; step_loss: 0.1063
step 28690; step_loss: 0.1984
step 28700; step_loss: 0.1286
step 28710; step_loss: 0.1173
step 28720; step_loss: 0.0903
step 28730; step_loss: 0.1116
step 28740; step_loss: 0.1057
step 28750; step_loss: 0.2005
step 28760; step_loss: 0.0928
step 28770; step_loss: 0.0958
step 28780; step_loss: 0.1014
step 28790; step_loss: 0.1196
step 28800; step_loss: 0.1413
step 28810; step_loss: 0.1033
step 28820; step_loss: 0.1342
step 28830; step_loss: 0.1437
step 28840; step_loss: 0.0927
step 28850; step_loss: 0.1124
step 28860; step_loss: 0.0807
step 28870; step_loss: 0.1134
step 28880; step_loss: 0.1378
step 28890; step_loss: 0.0913
step 28900; step_loss: 0.0938
step 28910; step_loss: 0.1000
step 28920; step_loss: 0.0970
step 28930; step_loss: 0.1086
step 28940; step_loss: 0.1640
step 28950; step_loss: 0.0935
step 28960; step_loss: 0.0939
step 28970; step_loss: 0.0977
step 28980; step_loss: 0.1704
step 28990; step_loss: 0.1249

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 0.373 | 0.643 | 0.946 | 1.044 |   n/a |   n/a |

============================
Global step:         29000
Learning rate:       0.0045
Step-time (ms):     27.1505
Train loss avg:      0.1132
--------------------------
Val loss:            0.3766
srnn loss:           0.2870
============================

Saving the model...
done in 262.18 ms
step 29000; step_loss: 0.1529
step 29010; step_loss: 0.0975
step 29020; step_loss: 0.0868
step 29030; step_loss: 0.0912
step 29040; step_loss: 0.1145
step 29050; step_loss: 0.1442
step 29060; step_loss: 0.0825
step 29070; step_loss: 0.1795
step 29080; step_loss: 0.1270
step 29090; step_loss: 0.1180
step 29100; step_loss: 0.1036
step 29110; step_loss: 0.1215
step 29120; step_loss: 0.1215
step 29130; step_loss: 0.2234
step 29140; step_loss: 0.0751
step 29150; step_loss: 0.1081
step 29160; step_loss: 0.0964
step 29170; step_loss: 0.0877
step 29180; step_loss: 0.1150
step 29190; step_loss: 0.1726
step 29200; step_loss: 0.1022
step 29210; step_loss: 0.1024
step 29220; step_loss: 0.0943
step 29230; step_loss: 0.0826
step 29240; step_loss: 0.1251
step 29250; step_loss: 0.1372
step 29260; step_loss: 0.0988
step 29270; step_loss: 0.1053
step 29280; step_loss: 0.1948
step 29290; step_loss: 0.0855
step 29300; step_loss: 0.1169
step 29310; step_loss: 0.1043
step 29320; step_loss: 0.1450
step 29330; step_loss: 0.0768
step 29340; step_loss: 0.1380
step 29350; step_loss: 0.1023
step 29360; step_loss: 0.1035
step 29370; step_loss: 0.0879
step 29380; step_loss: 0.1175
step 29390; step_loss: 0.1331
step 29400; step_loss: 0.0866
step 29410; step_loss: 0.0926
step 29420; step_loss: 0.1237
step 29430; step_loss: 0.1036
step 29440; step_loss: 0.1576
step 29450; step_loss: 0.1452
step 29460; step_loss: 0.1004
step 29470; step_loss: 0.1248
step 29480; step_loss: 0.1858
step 29490; step_loss: 0.1115
step 29500; step_loss: 0.0847
step 29510; step_loss: 0.1043
step 29520; step_loss: 0.0781
step 29530; step_loss: 0.2412
step 29540; step_loss: 0.0860
step 29550; step_loss: 0.1136
step 29560; step_loss: 0.1062
step 29570; step_loss: 0.1984
step 29580; step_loss: 0.1327
step 29590; step_loss: 0.1057
step 29600; step_loss: 0.1390
step 29610; step_loss: 0.1068
step 29620; step_loss: 0.1093
step 29630; step_loss: 0.0942
step 29640; step_loss: 0.1153
step 29650; step_loss: 0.0986
step 29660; step_loss: 0.0893
step 29670; step_loss: 0.1081
step 29680; step_loss: 0.1848
step 29690; step_loss: 0.1043
step 29700; step_loss: 0.0783
step 29710; step_loss: 0.0774
step 29720; step_loss: 0.0923
step 29730; step_loss: 0.2151
step 29740; step_loss: 0.0802
step 29750; step_loss: 0.1174
step 29760; step_loss: 0.0872
step 29770; step_loss: 0.0918
step 29780; step_loss: 0.1643
step 29790; step_loss: 0.1303
step 29800; step_loss: 0.1527
step 29810; step_loss: 0.0823
step 29820; step_loss: 0.0933
step 29830; step_loss: 0.0845
step 29840; step_loss: 0.1214
step 29850; step_loss: 0.1232
step 29860; step_loss: 0.1056
step 29870; step_loss: 0.1240
step 29880; step_loss: 0.0843
step 29890; step_loss: 0.0797
step 29900; step_loss: 0.1229
step 29910; step_loss: 0.0969
step 29920; step_loss: 0.0742
step 29930; step_loss: 0.1208
step 29940; step_loss: 0.1046
step 29950; step_loss: 0.0932
step 29960; step_loss: 0.0903
step 29970; step_loss: 0.0998
step 29980; step_loss: 0.0708
step 29990; step_loss: 0.0998

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 0.375 | 0.649 | 0.973 | 1.082 |   n/a |   n/a |

============================
Global step:         30000
Learning rate:       0.0043
Step-time (ms):     27.1414
Train loss avg:      0.1129
--------------------------
Val loss:            0.3606
srnn loss:           0.2962
============================

Saving the model...
done in 268.63 ms
step 30000; step_loss: 0.0797
step 30010; step_loss: 0.0877
step 30020; step_loss: 0.1172
step 30030; step_loss: 0.1886
step 30040; step_loss: 0.1321
step 30050; step_loss: 0.0796
step 30060; step_loss: 0.0845
step 30070; step_loss: 0.1199
step 30080; step_loss: 0.1676
step 30090; step_loss: 0.0958
step 30100; step_loss: 0.1998
step 30110; step_loss: 0.0753
step 30120; step_loss: 0.1011
step 30130; step_loss: 0.1307
step 30140; step_loss: 0.0969
step 30150; step_loss: 0.1093
step 30160; step_loss: 0.1766
step 30170; step_loss: 0.0900
step 30180; step_loss: 0.1002
step 30190; step_loss: 0.1590
step 30200; step_loss: 0.1616
step 30210; step_loss: 0.1109
step 30220; step_loss: 0.1742
step 30230; step_loss: 0.1081
step 30240; step_loss: 0.1098
step 30250; step_loss: 0.0955
step 30260; step_loss: 0.2331
step 30270; step_loss: 0.1166
step 30280; step_loss: 0.2108
step 30290; step_loss: 0.0852
step 30300; step_loss: 0.1060
step 30310; step_loss: 0.1015
step 30320; step_loss: 0.1103
step 30330; step_loss: 0.1104
step 30340; step_loss: 0.1062
step 30350; step_loss: 0.0857
step 30360; step_loss: 0.0797
step 30370; step_loss: 0.0713
step 30380; step_loss: 0.1019
step 30390; step_loss: 0.0902
step 30400; step_loss: 0.0862
step 30410; step_loss: 0.0917
step 30420; step_loss: 0.1185
step 30430; step_loss: 0.1442
step 30440; step_loss: 0.0816
step 30450; step_loss: 0.1260
step 30460; step_loss: 0.1980
step 30470; step_loss: 0.1162
step 30480; step_loss: 0.0734
step 30490; step_loss: 0.2461
step 30500; step_loss: 0.1040
step 30510; step_loss: 0.0818
step 30520; step_loss: 0.0797
step 30530; step_loss: 0.1223
step 30540; step_loss: 0.0954
step 30550; step_loss: 0.1022
step 30560; step_loss: 0.1211
step 30570; step_loss: 0.1179
step 30580; step_loss: 0.0638
step 30590; step_loss: 0.1799
step 30600; step_loss: 0.0998
step 30610; step_loss: 0.1200
step 30620; step_loss: 0.1052
step 30630; step_loss: 0.1043
step 30640; step_loss: 0.0834
step 30650; step_loss: 0.0967
step 30660; step_loss: 0.0831
step 30670; step_loss: 0.1249
step 30680; step_loss: 0.1368
step 30690; step_loss: 0.0794
step 30700; step_loss: 0.1208
step 30710; step_loss: 0.0747
step 30720; step_loss: 0.1013
step 30730; step_loss: 0.0930
step 30740; step_loss: 0.0974
step 30750; step_loss: 0.1200
step 30760; step_loss: 0.1200
step 30770; step_loss: 0.0559
step 30780; step_loss: 0.1000
step 30790; step_loss: 0.0980
step 30800; step_loss: 0.0973
step 30810; step_loss: 0.0914
step 30820; step_loss: 0.1008
step 30830; step_loss: 0.1343
step 30840; step_loss: 0.0986
step 30850; step_loss: 0.1764
step 30860; step_loss: 0.0902
step 30870; step_loss: 0.2117
step 30880; step_loss: 0.1686
step 30890; step_loss: 0.1260
step 30900; step_loss: 0.1145
step 30910; step_loss: 0.1149
step 30920; step_loss: 0.1543
step 30930; step_loss: 0.1297
step 30940; step_loss: 0.1796
step 30950; step_loss: 0.1013
step 30960; step_loss: 0.0783
step 30970; step_loss: 0.1094
step 30980; step_loss: 0.1332
step 30990; step_loss: 0.0849

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 0.374 | 0.646 | 0.961 | 1.064 |   n/a |   n/a |

============================
Global step:         31000
Learning rate:       0.0043
Step-time (ms):     27.1789
Train loss avg:      0.1140
--------------------------
Val loss:            0.3670
srnn loss:           0.2925
============================

Saving the model...
done in 262.70 ms
step 31000; step_loss: 0.1126
step 31010; step_loss: 0.1472
step 31020; step_loss: 0.0765
step 31030; step_loss: 0.1359
step 31040; step_loss: 0.1175
step 31050; step_loss: 0.1045
step 31060; step_loss: 0.0874
step 31070; step_loss: 0.1265
step 31080; step_loss: 0.0672
step 31090; step_loss: 0.1186
step 31100; step_loss: 0.1632
step 31110; step_loss: 0.1251
step 31120; step_loss: 0.0982
step 31130; step_loss: 0.1312
step 31140; step_loss: 0.0824
step 31150; step_loss: 0.1006
step 31160; step_loss: 0.1061
step 31170; step_loss: 0.0882
step 31180; step_loss: 0.0839
step 31190; step_loss: 0.0856
step 31200; step_loss: 0.1090
step 31210; step_loss: 0.1214
step 31220; step_loss: 0.1205
step 31230; step_loss: 0.1314
step 31240; step_loss: 0.1782
step 31250; step_loss: 0.1452
step 31260; step_loss: 0.1096
step 31270; step_loss: 0.0920
step 31280; step_loss: 0.0918
step 31290; step_loss: 0.1115
step 31300; step_loss: 0.0906
step 31310; step_loss: 0.0989
step 31320; step_loss: 0.0907
step 31330; step_loss: 0.1365
step 31340; step_loss: 0.0994
step 31350; step_loss: 0.0787
step 31360; step_loss: 0.1339
step 31370; step_loss: 0.0979
step 31380; step_loss: 0.1017
step 31390; step_loss: 0.0878
step 31400; step_loss: 0.1532
step 31410; step_loss: 0.1305
step 31420; step_loss: 0.0809
step 31430; step_loss: 0.1667
step 31440; step_loss: 0.0935
step 31450; step_loss: 0.1054
step 31460; step_loss: 0.0992
step 31470; step_loss: 0.1271
step 31480; step_loss: 0.0804
step 31490; step_loss: 0.0865
step 31500; step_loss: 0.1681
step 31510; step_loss: 0.0800
step 31520; step_loss: 0.0929
step 31530; step_loss: 0.1434
step 31540; step_loss: 0.1099
step 31550; step_loss: 0.1346
step 31560; step_loss: 0.1743
step 31570; step_loss: 0.1302
step 31580; step_loss: 0.1925
step 31590; step_loss: 0.0839
step 31600; step_loss: 0.1005
step 31610; step_loss: 0.1595
step 31620; step_loss: 0.0800
step 31630; step_loss: 0.1026
step 31640; step_loss: 0.0785
step 31650; step_loss: 0.1013
step 31660; step_loss: 0.1183
step 31670; step_loss: 0.1098
step 31680; step_loss: 0.0702
step 31690; step_loss: 0.1089
step 31700; step_loss: 0.0851
step 31710; step_loss: 0.0884
step 31720; step_loss: 0.1039
step 31730; step_loss: 0.1439
step 31740; step_loss: 0.1127
step 31750; step_loss: 0.1463
step 31760; step_loss: 0.2153
step 31770; step_loss: 0.1660
step 31780; step_loss: 0.1641
step 31790; step_loss: 0.1051
step 31800; step_loss: 0.0896
step 31810; step_loss: 0.0852
step 31820; step_loss: 0.1250
step 31830; step_loss: 0.0801
step 31840; step_loss: 0.1121
step 31850; step_loss: 0.0780
step 31860; step_loss: 0.1098
step 31870; step_loss: 0.0682
step 31880; step_loss: 0.0976
step 31890; step_loss: 0.0784
step 31900; step_loss: 0.1345
step 31910; step_loss: 0.1287
step 31920; step_loss: 0.1231
step 31930; step_loss: 0.1178
step 31940; step_loss: 0.1213
step 31950; step_loss: 0.0912
step 31960; step_loss: 0.0775
step 31970; step_loss: 0.0953
step 31980; step_loss: 0.1124
step 31990; step_loss: 0.0937

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 0.385 | 0.666 | 0.998 | 1.110 |   n/a |   n/a |

============================
Global step:         32000
Learning rate:       0.0043
Step-time (ms):     27.1078
Train loss avg:      0.1111
--------------------------
Val loss:            0.3328
srnn loss:           0.3058
============================

Saving the model...
done in 269.27 ms
step 32000; step_loss: 0.0921
step 32010; step_loss: 0.1154
step 32020; step_loss: 0.1022
step 32030; step_loss: 0.1134
step 32040; step_loss: 0.1685
step 32050; step_loss: 0.1825
step 32060; step_loss: 0.1808
step 32070; step_loss: 0.1413
step 32080; step_loss: 0.0856
step 32090; step_loss: 0.1568
step 32100; step_loss: 0.1326
step 32110; step_loss: 0.1203
step 32120; step_loss: 0.0850
step 32130; step_loss: 0.0952
step 32140; step_loss: 0.1406
step 32150; step_loss: 0.0952
step 32160; step_loss: 0.0762
step 32170; step_loss: 0.0873
step 32180; step_loss: 0.0772
step 32190; step_loss: 0.1019
step 32200; step_loss: 0.1155
step 32210; step_loss: 0.0814
step 32220; step_loss: 0.1024
step 32230; step_loss: 0.1097
step 32240; step_loss: 0.1190
step 32250; step_loss: 0.1441
step 32260; step_loss: 0.1647
step 32270; step_loss: 0.0737
step 32280; step_loss: 0.0834
step 32290; step_loss: 0.1225
step 32300; step_loss: 0.1170
step 32310; step_loss: 0.1084
step 32320; step_loss: 0.1039
step 32330; step_loss: 0.1286
step 32340; step_loss: 0.0898
step 32350; step_loss: 0.1205
step 32360; step_loss: 0.1038
step 32370; step_loss: 0.0739
step 32380; step_loss: 0.0847
step 32390; step_loss: 0.1107
step 32400; step_loss: 0.1163
step 32410; step_loss: 0.0962
step 32420; step_loss: 0.1225
step 32430; step_loss: 0.0943
step 32440; step_loss: 0.1135
step 32450; step_loss: 0.1168
step 32460; step_loss: 0.0969
step 32470; step_loss: 0.0885
step 32480; step_loss: 0.1907
step 32490; step_loss: 0.0897
step 32500; step_loss: 0.1042
step 32510; step_loss: 0.1440
step 32520; step_loss: 0.1259
step 32530; step_loss: 0.1102
step 32540; step_loss: 0.0921
step 32550; step_loss: 0.0714
step 32560; step_loss: 0.1247
step 32570; step_loss: 0.1359
step 32580; step_loss: 0.0901
step 32590; step_loss: 0.1128
step 32600; step_loss: 0.0981
step 32610; step_loss: 0.1700
step 32620; step_loss: 0.0875
step 32630; step_loss: 0.0797
step 32640; step_loss: 0.1372
step 32650; step_loss: 0.2066
step 32660; step_loss: 0.1227
step 32670; step_loss: 0.1211
step 32680; step_loss: 0.1204
step 32690; step_loss: 0.0834
step 32700; step_loss: 0.1611
step 32710; step_loss: 0.2129
step 32720; step_loss: 0.1342
step 32730; step_loss: 0.0889
step 32740; step_loss: 0.1344
step 32750; step_loss: 0.1788
step 32760; step_loss: 0.1171
step 32770; step_loss: 0.0994
step 32780; step_loss: 0.0964
step 32790; step_loss: 0.1413
step 32800; step_loss: 0.1251
step 32810; step_loss: 0.2012
step 32820; step_loss: 0.0996
step 32830; step_loss: 0.0922
step 32840; step_loss: 0.1294
step 32850; step_loss: 0.0880
step 32860; step_loss: 0.1825
step 32870; step_loss: 0.0850
step 32880; step_loss: 0.1242
step 32890; step_loss: 0.1190
step 32900; step_loss: 0.0978
step 32910; step_loss: 0.0919
step 32920; step_loss: 0.1075
step 32930; step_loss: 0.1054
step 32940; step_loss: 0.0869
step 32950; step_loss: 0.1167
step 32960; step_loss: 0.1016
step 32970; step_loss: 0.1177
step 32980; step_loss: 0.1911
step 32990; step_loss: 0.1533

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 0.378 | 0.655 | 0.974 | 1.083 |   n/a |   n/a |

============================
Global step:         33000
Learning rate:       0.0043
Step-time (ms):     27.1199
Train loss avg:      0.1122
--------------------------
Val loss:            0.4038
srnn loss:           0.3070
============================

Saving the model...
done in 265.56 ms
step 33000; step_loss: 0.0909
step 33010; step_loss: 0.0767
step 33020; step_loss: 0.1047
step 33030; step_loss: 0.0987
step 33040; step_loss: 0.1012
step 33050; step_loss: 0.0852
step 33060; step_loss: 0.1082
step 33070; step_loss: 0.1137
step 33080; step_loss: 0.0789
step 33090; step_loss: 0.0953
step 33100; step_loss: 0.1642
step 33110; step_loss: 0.0944
step 33120; step_loss: 0.0759
step 33130; step_loss: 0.0776
step 33140; step_loss: 0.0885
step 33150; step_loss: 0.0773
step 33160; step_loss: 0.0893
step 33170; step_loss: 0.0876
step 33180; step_loss: 0.1220
step 33190; step_loss: 0.0868
step 33200; step_loss: 0.2149
step 33210; step_loss: 0.0897
step 33220; step_loss: 0.0791
step 33230; step_loss: 0.0861
step 33240; step_loss: 0.1099
step 33250; step_loss: 0.0881
step 33260; step_loss: 0.1067
step 33270; step_loss: 0.1041
step 33280; step_loss: 0.0880
step 33290; step_loss: 0.1760
step 33300; step_loss: 0.0914
step 33310; step_loss: 0.1928
step 33320; step_loss: 0.0976
step 33330; step_loss: 0.1237
step 33340; step_loss: 0.1008
step 33350; step_loss: 0.1284
step 33360; step_loss: 0.0746
step 33370; step_loss: 0.0823
step 33380; step_loss: 0.0892
step 33390; step_loss: 0.1257
step 33400; step_loss: 0.1025
step 33410; step_loss: 0.1106
step 33420; step_loss: 0.1067
step 33430; step_loss: 0.1122
step 33440; step_loss: 0.1049
step 33450; step_loss: 0.0703
step 33460; step_loss: 0.1255
step 33470; step_loss: 0.1111
step 33480; step_loss: 0.1056
step 33490; step_loss: 0.1060
step 33500; step_loss: 0.0948
step 33510; step_loss: 0.1035
step 33520; step_loss: 0.1202
step 33530; step_loss: 0.0743
step 33540; step_loss: 0.1242
step 33550; step_loss: 0.0878
step 33560; step_loss: 0.0849
step 33570; step_loss: 0.0956
step 33580; step_loss: 0.0968
step 33590; step_loss: 0.0732
step 33600; step_loss: 0.0702
step 33610; step_loss: 0.1158
step 33620; step_loss: 0.0716
step 33630; step_loss: 0.0769
step 33640; step_loss: 0.0621
step 33650; step_loss: 0.0976
step 33660; step_loss: 0.2031
step 33670; step_loss: 0.0715
step 33680; step_loss: 0.0852
step 33690; step_loss: 0.2039
step 33700; step_loss: 0.0876
step 33710; step_loss: 0.0679
step 33720; step_loss: 0.0715
step 33730; step_loss: 0.0879
step 33740; step_loss: 0.1100
step 33750; step_loss: 0.1903
step 33760; step_loss: 0.0929
step 33770; step_loss: 0.1240
step 33780; step_loss: 0.0775
step 33790; step_loss: 0.0994
step 33800; step_loss: 0.0953
step 33810; step_loss: 0.1831
step 33820; step_loss: 0.1056
step 33830; step_loss: 0.1382
step 33840; step_loss: 0.0817
step 33850; step_loss: 0.1127
step 33860; step_loss: 0.1011
step 33870; step_loss: 0.1066
step 33880; step_loss: 0.0903
step 33890; step_loss: 0.0650
step 33900; step_loss: 0.1040
step 33910; step_loss: 0.0961
step 33920; step_loss: 0.1311
step 33930; step_loss: 0.0753
step 33940; step_loss: 0.1640
step 33950; step_loss: 0.1140
step 33960; step_loss: 0.0921
step 33970; step_loss: 0.0846
step 33980; step_loss: 0.0907
step 33990; step_loss: 0.1066

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 0.384 | 0.667 | 1.011 | 1.128 |   n/a |   n/a |

============================
Global step:         34000
Learning rate:       0.0043
Step-time (ms):     27.1568
Train loss avg:      0.1087
--------------------------
Val loss:            0.3597
srnn loss:           0.3117
============================

Saving the model...
done in 267.28 ms
step 34000; step_loss: 0.1562
step 34010; step_loss: 0.1748
step 34020; step_loss: 0.1274
step 34030; step_loss: 0.1128
step 34040; step_loss: 0.1126
step 34050; step_loss: 0.1022
step 34060; step_loss: 0.1297
step 34070; step_loss: 0.1556
step 34080; step_loss: 0.1034
step 34090; step_loss: 0.0737
step 34100; step_loss: 0.1412
step 34110; step_loss: 0.0824
step 34120; step_loss: 0.0905
step 34130; step_loss: 0.0599
step 34140; step_loss: 0.0805
step 34150; step_loss: 0.1330
step 34160; step_loss: 0.2142
step 34170; step_loss: 0.0953
step 34180; step_loss: 0.1657
step 34190; step_loss: 0.1153
step 34200; step_loss: 0.1016
step 34210; step_loss: 0.0839
step 34220; step_loss: 0.0783
step 34230; step_loss: 0.0660
step 34240; step_loss: 0.0852
step 34250; step_loss: 0.0730
step 34260; step_loss: 0.1597
step 34270; step_loss: 0.0702
step 34280; step_loss: 0.0865
step 34290; step_loss: 0.0901
step 34300; step_loss: 0.1366
step 34310; step_loss: 0.0816
step 34320; step_loss: 0.0676
step 34330; step_loss: 0.1030
step 34340; step_loss: 0.0655
step 34350; step_loss: 0.0989
step 34360; step_loss: 0.1362
step 34370; step_loss: 0.0941
step 34380; step_loss: 0.0963
step 34390; step_loss: 0.1326
step 34400; step_loss: 0.0904
step 34410; step_loss: 0.0764
step 34420; step_loss: 0.0876
step 34430; step_loss: 0.0909
step 34440; step_loss: 0.1811
step 34450; step_loss: 0.1144
step 34460; step_loss: 0.1211
step 34470; step_loss: 0.0946
step 34480; step_loss: 0.0872
step 34490; step_loss: 0.1069
step 34500; step_loss: 0.1186
step 34510; step_loss: 0.0689
step 34520; step_loss: 0.0864
step 34530; step_loss: 0.0883
step 34540; step_loss: 0.0971
step 34550; step_loss: 0.0819
step 34560; step_loss: 0.1284
step 34570; step_loss: 0.0802
step 34580; step_loss: 0.1738
step 34590; step_loss: 0.1293
step 34600; step_loss: 0.0874
step 34610; step_loss: 0.1050
step 34620; step_loss: 0.1790
step 34630; step_loss: 0.1135
step 34640; step_loss: 0.1564
step 34650; step_loss: 0.0816
step 34660; step_loss: 0.1179
step 34670; step_loss: 0.1347
step 34680; step_loss: 0.0740
step 34690; step_loss: 0.0898
step 34700; step_loss: 0.1254
step 34710; step_loss: 0.1131
step 34720; step_loss: 0.0812
step 34730; step_loss: 0.1062
step 34740; step_loss: 0.1335
step 34750; step_loss: 0.0936
step 34760; step_loss: 0.0810
step 34770; step_loss: 0.1065
step 34780; step_loss: 0.1218
step 34790; step_loss: 0.0981
step 34800; step_loss: 0.1161
step 34810; step_loss: 0.0970
step 34820; step_loss: 0.0838
step 34830; step_loss: 0.0851
step 34840; step_loss: 0.0877
step 34850; step_loss: 0.0686
step 34860; step_loss: 0.1335
step 34870; step_loss: 0.1394
step 34880; step_loss: 0.1325
step 34890; step_loss: 0.0925
step 34900; step_loss: 0.0941
step 34910; step_loss: 0.0893
step 34920; step_loss: 0.0974
step 34930; step_loss: 0.1038
step 34940; step_loss: 0.1313
step 34950; step_loss: 0.0843
step 34960; step_loss: 0.1003
step 34970; step_loss: 0.1801
step 34980; step_loss: 0.0853
step 34990; step_loss: 0.1738

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 0.382 | 0.663 | 0.998 | 1.113 |   n/a |   n/a |

============================
Global step:         35000
Learning rate:       0.0043
Step-time (ms):     27.1005
Train loss avg:      0.1086
--------------------------
Val loss:            0.3871
srnn loss:           0.3085
============================

Saving the model...
done in 263.71 ms
step 35000; step_loss: 0.0812
step 35010; step_loss: 0.0961
step 35020; step_loss: 0.0979
step 35030; step_loss: 0.1088
step 35040; step_loss: 0.1536
step 35050; step_loss: 0.0943
step 35060; step_loss: 0.1640
step 35070; step_loss: 0.1111
step 35080; step_loss: 0.1063
step 35090; step_loss: 0.1401
step 35100; step_loss: 0.1185
step 35110; step_loss: 0.0777
step 35120; step_loss: 0.2266
step 35130; step_loss: 0.1162
step 35140; step_loss: 0.0810
step 35150; step_loss: 0.0589
step 35160; step_loss: 0.1179
step 35170; step_loss: 0.0951
step 35180; step_loss: 0.0786
step 35190; step_loss: 0.1286
step 35200; step_loss: 0.1783
step 35210; step_loss: 0.1257
step 35220; step_loss: 0.0714
step 35230; step_loss: 0.1289
step 35240; step_loss: 0.0809
step 35250; step_loss: 0.1482
step 35260; step_loss: 0.1020
step 35270; step_loss: 0.0891
step 35280; step_loss: 0.0950
step 35290; step_loss: 0.0706
step 35300; step_loss: 0.0968
step 35310; step_loss: 0.0933
step 35320; step_loss: 0.1361
step 35330; step_loss: 0.1101
step 35340; step_loss: 0.0878
step 35350; step_loss: 0.1075
step 35360; step_loss: 0.0736
step 35370; step_loss: 0.0629
step 35380; step_loss: 0.1334
step 35390; step_loss: 0.1087
step 35400; step_loss: 0.0800
step 35410; step_loss: 0.1175
step 35420; step_loss: 0.1272
step 35430; step_loss: 0.1197
step 35440; step_loss: 0.1125
step 35450; step_loss: 0.1279
step 35460; step_loss: 0.1100
step 35470; step_loss: 0.1039
step 35480; step_loss: 0.0730
step 35490; step_loss: 0.1051
step 35500; step_loss: 0.0886
step 35510; step_loss: 0.0767
step 35520; step_loss: 0.0880
step 35530; step_loss: 0.1213
step 35540; step_loss: 0.1066
step 35550; step_loss: 0.1351
step 35560; step_loss: 0.1051
step 35570; step_loss: 0.1202
step 35580; step_loss: 0.0796
step 35590; step_loss: 0.1044
step 35600; step_loss: 0.0617
step 35610; step_loss: 0.1118
step 35620; step_loss: 0.0712
step 35630; step_loss: 0.1177
step 35640; step_loss: 0.1187
step 35650; step_loss: 0.0690
step 35660; step_loss: 0.1591
step 35670; step_loss: 0.1085
step 35680; step_loss: 0.1448
step 35690; step_loss: 0.0787
step 35700; step_loss: 0.0823
step 35710; step_loss: 0.0933
step 35720; step_loss: 0.0898
step 35730; step_loss: 0.1099
step 35740; step_loss: 0.0895
step 35750; step_loss: 0.0955
step 35760; step_loss: 0.1227
step 35770; step_loss: 0.2173
step 35780; step_loss: 0.1081
step 35790; step_loss: 0.2298
step 35800; step_loss: 0.0874
step 35810; step_loss: 0.0725
step 35820; step_loss: 0.0874
step 35830; step_loss: 0.1088
step 35840; step_loss: 0.0985
step 35850; step_loss: 0.1172
step 35860; step_loss: 0.0630
step 35870; step_loss: 0.0898
step 35880; step_loss: 0.0892
step 35890; step_loss: 0.0717
step 35900; step_loss: 0.0731
step 35910; step_loss: 0.1549
step 35920; step_loss: 0.1363
step 35930; step_loss: 0.1139
step 35940; step_loss: 0.0957
step 35950; step_loss: 0.1201
step 35960; step_loss: 0.1473
step 35970; step_loss: 0.0595
step 35980; step_loss: 0.1036
step 35990; step_loss: 0.0803

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 0.383 | 0.665 | 0.999 | 1.112 |   n/a |   n/a |

============================
Global step:         36000
Learning rate:       0.0043
Step-time (ms):     27.1223
Train loss avg:      0.1091
--------------------------
Val loss:            0.3029
srnn loss:           0.3116
============================

Saving the model...
done in 259.85 ms
step 36000; step_loss: 0.1118
step 36010; step_loss: 0.0753
step 36020; step_loss: 0.0702
step 36030; step_loss: 0.1037
step 36040; step_loss: 0.0838
step 36050; step_loss: 0.1168
step 36060; step_loss: 0.0864
step 36070; step_loss: 0.0616
step 36080; step_loss: 0.0737
step 36090; step_loss: 0.0940
step 36100; step_loss: 0.1419
step 36110; step_loss: 0.1257
step 36120; step_loss: 0.1072
step 36130; step_loss: 0.0863
step 36140; step_loss: 0.0895
step 36150; step_loss: 0.1018
step 36160; step_loss: 0.0747
step 36170; step_loss: 0.1117
step 36180; step_loss: 0.1064
step 36190; step_loss: 0.0988
step 36200; step_loss: 0.0932
step 36210; step_loss: 0.1246
step 36220; step_loss: 0.0871
step 36230; step_loss: 0.0537
step 36240; step_loss: 0.0909
step 36250; step_loss: 0.0976
step 36260; step_loss: 0.0830
step 36270; step_loss: 0.1078
step 36280; step_loss: 0.1542
step 36290; step_loss: 0.0860
step 36300; step_loss: 0.1187
step 36310; step_loss: 0.0971
step 36320; step_loss: 0.1189
step 36330; step_loss: 0.1056
step 36340; step_loss: 0.0941
step 36350; step_loss: 0.0782
step 36360; step_loss: 0.1100
step 36370; step_loss: 0.1000
step 36380; step_loss: 0.1048
step 36390; step_loss: 0.0760
step 36400; step_loss: 0.1877
step 36410; step_loss: 0.1133
step 36420; step_loss: 0.1423
step 36430; step_loss: 0.1287
step 36440; step_loss: 0.1302
step 36450; step_loss: 0.1096
step 36460; step_loss: 0.0948
step 36470; step_loss: 0.0900
step 36480; step_loss: 0.0921
step 36490; step_loss: 0.0959
step 36500; step_loss: 0.0968
step 36510; step_loss: 0.0913
step 36520; step_loss: 0.0809
step 36530; step_loss: 0.1040
step 36540; step_loss: 0.1453
step 36550; step_loss: 0.1525
step 36560; step_loss: 0.1313
step 36570; step_loss: 0.1348
step 36580; step_loss: 0.1424
step 36590; step_loss: 0.1010
step 36600; step_loss: 0.0940
step 36610; step_loss: 0.1048
step 36620; step_loss: 0.0800
step 36630; step_loss: 0.1042
step 36640; step_loss: 0.0671
step 36650; step_loss: 0.0860
step 36660; step_loss: 0.0947
step 36670; step_loss: 0.0892
step 36680; step_loss: 0.0915
step 36690; step_loss: 0.0596
step 36700; step_loss: 0.1050
step 36710; step_loss: 0.0956
step 36720; step_loss: 0.1008
step 36730; step_loss: 0.1670
step 36740; step_loss: 0.0703
step 36750; step_loss: 0.0867
step 36760; step_loss: 0.0825
step 36770; step_loss: 0.0958
step 36780; step_loss: 0.0586
step 36790; step_loss: 0.0717
step 36800; step_loss: 0.1477
step 36810; step_loss: 0.0782
step 36820; step_loss: 0.1631
step 36830; step_loss: 0.1589
step 36840; step_loss: 0.1246
step 36850; step_loss: 0.1899
step 36860; step_loss: 0.0724
step 36870; step_loss: 0.0609
step 36880; step_loss: 0.1064
step 36890; step_loss: 0.0917
step 36900; step_loss: 0.2267
step 36910; step_loss: 0.1129
step 36920; step_loss: 0.1482
step 36930; step_loss: 0.1051
step 36940; step_loss: 0.1008
step 36950; step_loss: 0.0878
step 36960; step_loss: 0.0925
step 36970; step_loss: 0.0827
step 36980; step_loss: 0.0980
step 36990; step_loss: 0.0848

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 0.379 | 0.661 | 0.994 | 1.106 |   n/a |   n/a |

============================
Global step:         37000
Learning rate:       0.0043
Step-time (ms):     27.1559
Train loss avg:      0.1090
--------------------------
Val loss:            0.3685
srnn loss:           0.3115
============================

Saving the model...
done in 278.57 ms
step 37000; step_loss: 0.1448
step 37010; step_loss: 0.1736
step 37020; step_loss: 0.0780
step 37030; step_loss: 0.1270
step 37040; step_loss: 0.0840
step 37050; step_loss: 0.0929
step 37060; step_loss: 0.0797
step 37070; step_loss: 0.1030
step 37080; step_loss: 0.1059
step 37090; step_loss: 0.1110
step 37100; step_loss: 0.1578
step 37110; step_loss: 0.0820
step 37120; step_loss: 0.1898
step 37130; step_loss: 0.1389
step 37140; step_loss: 0.1018
step 37150; step_loss: 0.1034
step 37160; step_loss: 0.0896
step 37170; step_loss: 0.1272
step 37180; step_loss: 0.0677
step 37190; step_loss: 0.3274
step 37200; step_loss: 0.0811
step 37210; step_loss: 0.0734
step 37220; step_loss: 0.0999
step 37230; step_loss: 0.1221
step 37240; step_loss: 0.0865
step 37250; step_loss: 0.1209
step 37260; step_loss: 0.0696
step 37270; step_loss: 0.1179
step 37280; step_loss: 0.0662
step 37290; step_loss: 0.1236
step 37300; step_loss: 0.0637
step 37310; step_loss: 0.0949
step 37320; step_loss: 0.0793
step 37330; step_loss: 0.0928
step 37340; step_loss: 0.1189
step 37350; step_loss: 0.1127
step 37360; step_loss: 0.1015
step 37370; step_loss: 0.1324
step 37380; step_loss: 0.0605
step 37390; step_loss: 0.0834
step 37400; step_loss: 0.1458
step 37410; step_loss: 0.0742
step 37420; step_loss: 0.0992
step 37430; step_loss: 0.1508
step 37440; step_loss: 0.1181
step 37450; step_loss: 0.1460
step 37460; step_loss: 0.1335
step 37470; step_loss: 0.1261
step 37480; step_loss: 0.1328
step 37490; step_loss: 0.1251
step 37500; step_loss: 0.1764
step 37510; step_loss: 0.1013
step 37520; step_loss: 0.1137
step 37530; step_loss: 0.1208
step 37540; step_loss: 0.0856
step 37550; step_loss: 0.0845
step 37560; step_loss: 0.0922
step 37570; step_loss: 0.1009
step 37580; step_loss: 0.0995
step 37590; step_loss: 0.1598
step 37600; step_loss: 0.0915
step 37610; step_loss: 0.0879
step 37620; step_loss: 0.1147
step 37630; step_loss: 0.0884
step 37640; step_loss: 0.1362
step 37650; step_loss: 0.1034
step 37660; step_loss: 0.0947
step 37670; step_loss: 0.1051
step 37680; step_loss: 0.0917
step 37690; step_loss: 0.1030
step 37700; step_loss: 0.1397
step 37710; step_loss: 0.1336
step 37720; step_loss: 0.1104
step 37730; step_loss: 0.1177
step 37740; step_loss: 0.0932
step 37750; step_loss: 0.0946
step 37760; step_loss: 0.0668
step 37770; step_loss: 0.1310
step 37780; step_loss: 0.0984
step 37790; step_loss: 0.0805
step 37800; step_loss: 0.0961
step 37810; step_loss: 0.0913
step 37820; step_loss: 0.1296
step 37830; step_loss: 0.1481
step 37840; step_loss: 0.0709
step 37850; step_loss: 0.0998
step 37860; step_loss: 0.0992
step 37870; step_loss: 0.0725
step 37880; step_loss: 0.0844
step 37890; step_loss: 0.1601
step 37900; step_loss: 0.1129
step 37910; step_loss: 0.0939
step 37920; step_loss: 0.2753
step 37930; step_loss: 0.0701
step 37940; step_loss: 0.0968
step 37950; step_loss: 0.0819
step 37960; step_loss: 0.0924
step 37970; step_loss: 0.0814
step 37980; step_loss: 0.0864
step 37990; step_loss: 0.0893

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 0.382 | 0.664 | 0.992 | 1.103 |   n/a |   n/a |

============================
Global step:         38000
Learning rate:       0.0043
Step-time (ms):     27.1125
Train loss avg:      0.1067
--------------------------
Val loss:            0.3573
srnn loss:           0.3107
============================

Saving the model...
done in 270.33 ms
step 38000; step_loss: 0.1307
step 38010; step_loss: 0.0790
step 38020; step_loss: 0.0779
step 38030; step_loss: 0.1175
step 38040; step_loss: 0.0914
step 38050; step_loss: 0.1530
step 38060; step_loss: 0.1141
step 38070; step_loss: 0.1102
step 38080; step_loss: 0.0780
step 38090; step_loss: 0.1198
step 38100; step_loss: 0.1349
step 38110; step_loss: 0.0889
step 38120; step_loss: 0.0939
step 38130; step_loss: 0.1292
step 38140; step_loss: 0.0917
step 38150; step_loss: 0.0907
step 38160; step_loss: 0.1415
step 38170; step_loss: 0.1048
step 38180; step_loss: 0.0901
step 38190; step_loss: 0.0697
step 38200; step_loss: 0.0890
step 38210; step_loss: 0.1028
step 38220; step_loss: 0.1217
step 38230; step_loss: 0.2529
step 38240; step_loss: 0.1282
step 38250; step_loss: 0.1631
step 38260; step_loss: 0.0968
step 38270; step_loss: 0.0810
step 38280; step_loss: 0.1195
step 38290; step_loss: 0.1334
step 38300; step_loss: 0.1536
step 38310; step_loss: 0.1374
step 38320; step_loss: 0.1069
step 38330; step_loss: 0.1122
step 38340; step_loss: 0.1004
step 38350; step_loss: 0.1015
step 38360; step_loss: 0.1404
step 38370; step_loss: 0.0833
step 38380; step_loss: 0.2089
step 38390; step_loss: 0.0698
step 38400; step_loss: 0.0970
step 38410; step_loss: 0.0804
step 38420; step_loss: 0.0631
step 38430; step_loss: 0.1589
step 38440; step_loss: 0.0937
step 38450; step_loss: 0.0899
step 38460; step_loss: 0.0942
step 38470; step_loss: 0.0739
step 38480; step_loss: 0.1062
step 38490; step_loss: 0.0830
step 38500; step_loss: 0.0825
step 38510; step_loss: 0.1525
step 38520; step_loss: 0.1635
step 38530; step_loss: 0.1489
step 38540; step_loss: 0.1038
step 38550; step_loss: 0.0763
step 38560; step_loss: 0.1109
step 38570; step_loss: 0.1216
step 38580; step_loss: 0.0898
step 38590; step_loss: 0.1048
step 38600; step_loss: 0.0662
step 38610; step_loss: 0.0836
step 38620; step_loss: 0.1030
step 38630; step_loss: 0.1593
step 38640; step_loss: 0.1033
step 38650; step_loss: 0.0835
step 38660; step_loss: 0.1215
step 38670; step_loss: 0.1145
step 38680; step_loss: 0.0732
step 38690; step_loss: 0.1111
step 38700; step_loss: 0.0693
step 38710; step_loss: 0.1110
step 38720; step_loss: 0.1591
step 38730; step_loss: 0.1197
step 38740; step_loss: 0.0910
step 38750; step_loss: 0.1053
step 38760; step_loss: 0.0803
step 38770; step_loss: 0.1242
step 38780; step_loss: 0.0942
step 38790; step_loss: 0.0756
step 38800; step_loss: 0.1594
step 38810; step_loss: 0.0711
step 38820; step_loss: 0.0952
step 38830; step_loss: 0.0776
step 38840; step_loss: 0.1357
step 38850; step_loss: 0.0977
step 38860; step_loss: 0.0832
step 38870; step_loss: 0.1319
step 38880; step_loss: 0.1152
step 38890; step_loss: 0.1237
step 38900; step_loss: 0.1390
step 38910; step_loss: 0.0734
step 38920; step_loss: 0.1483
step 38930; step_loss: 0.0860
step 38940; step_loss: 0.1105
step 38950; step_loss: 0.0804
step 38960; step_loss: 0.0843
step 38970; step_loss: 0.1121
step 38980; step_loss: 0.1067
step 38990; step_loss: 0.0744

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 0.391 | 0.680 | 1.027 | 1.145 |   n/a |   n/a |

============================
Global step:         39000
Learning rate:       0.0043
Step-time (ms):     27.2418
Train loss avg:      0.1068
--------------------------
Val loss:            0.3541
srnn loss:           0.3232
============================

Saving the model...
done in 292.52 ms
step 39000; step_loss: 0.0773
step 39010; step_loss: 0.0866
step 39020; step_loss: 0.0807
step 39030; step_loss: 0.1076
step 39040; step_loss: 0.0681
step 39050; step_loss: 0.0707
step 39060; step_loss: 0.0982
step 39070; step_loss: 0.0819
step 39080; step_loss: 0.1390
step 39090; step_loss: 0.0710
step 39100; step_loss: 0.0823
step 39110; step_loss: 0.0960
step 39120; step_loss: 0.1403
step 39130; step_loss: 0.1115
step 39140; step_loss: 0.1095
step 39150; step_loss: 0.0787
step 39160; step_loss: 0.1240
step 39170; step_loss: 0.0962
step 39180; step_loss: 0.0814
step 39190; step_loss: 0.1911
step 39200; step_loss: 0.1281
step 39210; step_loss: 0.0796
step 39220; step_loss: 0.0921
step 39230; step_loss: 0.0881
step 39240; step_loss: 0.1009
step 39250; step_loss: 0.1211
step 39260; step_loss: 0.1026
step 39270; step_loss: 0.0969
step 39280; step_loss: 0.0776
step 39290; step_loss: 0.0729
step 39300; step_loss: 0.1101
step 39310; step_loss: 0.0782
step 39320; step_loss: 0.0670
step 39330; step_loss: 0.0944
step 39340; step_loss: 0.1146
step 39350; step_loss: 0.0824
step 39360; step_loss: 0.1060
step 39370; step_loss: 0.1065
step 39380; step_loss: 0.1335
step 39390; step_loss: 0.0973
step 39400; step_loss: 0.1159
step 39410; step_loss: 0.1149
step 39420; step_loss: 0.0687
step 39430; step_loss: 0.0971
step 39440; step_loss: 0.1447
step 39450; step_loss: 0.1039
step 39460; step_loss: 0.0730
step 39470; step_loss: 0.0743
step 39480; step_loss: 0.1002
step 39490; step_loss: 0.0998
step 39500; step_loss: 0.0967
step 39510; step_loss: 0.0824
step 39520; step_loss: 0.0638
step 39530; step_loss: 0.1066
step 39540; step_loss: 0.1068
step 39550; step_loss: 0.0657
step 39560; step_loss: 0.0788
step 39570; step_loss: 0.1038
step 39580; step_loss: 0.0873
step 39590; step_loss: 0.0947
step 39600; step_loss: 0.1713
step 39610; step_loss: 0.1362
step 39620; step_loss: 0.2115
step 39630; step_loss: 0.1642
step 39640; step_loss: 0.1130
step 39650; step_loss: 0.1261
step 39660; step_loss: 0.0854
step 39670; step_loss: 0.1432
step 39680; step_loss: 0.0890
step 39690; step_loss: 0.0819
step 39700; step_loss: 0.0991
step 39710; step_loss: 0.0968
step 39720; step_loss: 0.0975
step 39730; step_loss: 0.1918
step 39740; step_loss: 0.0735
step 39750; step_loss: 0.0857
step 39760; step_loss: 0.0941
step 39770; step_loss: 0.1164
step 39780; step_loss: 0.1286
step 39790; step_loss: 0.1034
step 39800; step_loss: 0.1172
step 39810; step_loss: 0.0794
step 39820; step_loss: 0.0799
step 39830; step_loss: 0.1161
step 39840; step_loss: 0.1161
step 39850; step_loss: 0.0780
step 39860; step_loss: 0.0665
step 39870; step_loss: 0.0789
step 39880; step_loss: 0.0828
step 39890; step_loss: 0.1061
step 39900; step_loss: 0.1155
step 39910; step_loss: 0.0891
step 39920; step_loss: 0.1155
step 39930; step_loss: 0.0721
step 39940; step_loss: 0.1141
step 39950; step_loss: 0.1896
step 39960; step_loss: 0.1048
step 39970; step_loss: 0.0944
step 39980; step_loss: 0.0808
step 39990; step_loss: 0.1022

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 0.389 | 0.677 | 1.011 | 1.118 |   n/a |   n/a |

============================
Global step:         40000
Learning rate:       0.0041
Step-time (ms):     27.1333
Train loss avg:      0.1056
--------------------------
Val loss:            0.3710
srnn loss:           0.3221
============================

Saving the model...
done in 272.42 ms
step 40000; step_loss: 0.1395
step 40010; step_loss: 0.0970
step 40020; step_loss: 0.1046
step 40030; step_loss: 0.1014
step 40040; step_loss: 0.1474
step 40050; step_loss: 0.1069
step 40060; step_loss: 0.0665
step 40070; step_loss: 0.0846
step 40080; step_loss: 0.1443
step 40090; step_loss: 0.0810
step 40100; step_loss: 0.0900
step 40110; step_loss: 0.1577
step 40120; step_loss: 0.1168
step 40130; step_loss: 0.0992
step 40140; step_loss: 0.0756
step 40150; step_loss: 0.1265
step 40160; step_loss: 0.0749
step 40170; step_loss: 0.1116
step 40180; step_loss: 0.0888
step 40190; step_loss: 0.0988
step 40200; step_loss: 0.0720
step 40210; step_loss: 0.1317
step 40220; step_loss: 0.0742
step 40230; step_loss: 0.1108
step 40240; step_loss: 0.0929
step 40250; step_loss: 0.1771
step 40260; step_loss: 0.0915
step 40270; step_loss: 0.0928
step 40280; step_loss: 0.1241
step 40290; step_loss: 0.0727
step 40300; step_loss: 0.0953
step 40310; step_loss: 0.0870
step 40320; step_loss: 0.0958
step 40330; step_loss: 0.0922
step 40340; step_loss: 0.1141
step 40350; step_loss: 0.1026
step 40360; step_loss: 0.0977
step 40370; step_loss: 0.1022
step 40380; step_loss: 0.1078
step 40390; step_loss: 0.0757
step 40400; step_loss: 0.1095
step 40410; step_loss: 0.1363
step 40420; step_loss: 0.1422
step 40430; step_loss: 0.1104
step 40440; step_loss: 0.1278
step 40450; step_loss: 0.0908
step 40460; step_loss: 0.1095
step 40470; step_loss: 0.1216
step 40480; step_loss: 0.0908
step 40490; step_loss: 0.0814
step 40500; step_loss: 0.0775
step 40510; step_loss: 0.1778
step 40520; step_loss: 0.1203
step 40530; step_loss: 0.1167
step 40540; step_loss: 0.1270
step 40550; step_loss: 0.1100
step 40560; step_loss: 0.0926
step 40570; step_loss: 0.1112
step 40580; step_loss: 0.1063
step 40590; step_loss: 0.1050
step 40600; step_loss: 0.1021
step 40610; step_loss: 0.0856
step 40620; step_loss: 0.0874
step 40630; step_loss: 0.0713
step 40640; step_loss: 0.1058
step 40650; step_loss: 0.1075
step 40660; step_loss: 0.1186
step 40670; step_loss: 0.1063
step 40680; step_loss: 0.0980
step 40690; step_loss: 0.0871
step 40700; step_loss: 0.1513
step 40710; step_loss: 0.1131
step 40720; step_loss: 0.1073
step 40730; step_loss: 0.0699
step 40740; step_loss: 0.0880
step 40750; step_loss: 0.0860
step 40760; step_loss: 0.1117
step 40770; step_loss: 0.1155
step 40780; step_loss: 0.0745
step 40790; step_loss: 0.0789
step 40800; step_loss: 0.1119
step 40810; step_loss: 0.1189
step 40820; step_loss: 0.0973
step 40830; step_loss: 0.1024
step 40840; step_loss: 0.0923
step 40850; step_loss: 0.0876
step 40860; step_loss: 0.1084
step 40870; step_loss: 0.0859
step 40880; step_loss: 0.1160
step 40890; step_loss: 0.0857
step 40900; step_loss: 0.0820
step 40910; step_loss: 0.0831
step 40920; step_loss: 0.0942
step 40930; step_loss: 0.1077
step 40940; step_loss: 0.1385
step 40950; step_loss: 0.1009
step 40960; step_loss: 0.0867
step 40970; step_loss: 0.1020
step 40980; step_loss: 0.0866
step 40990; step_loss: 0.0679

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 0.394 | 0.686 | 1.038 | 1.155 |   n/a |   n/a |

============================
Global step:         41000
Learning rate:       0.0041
Step-time (ms):     27.1014
Train loss avg:      0.1045
--------------------------
Val loss:            0.3278
srnn loss:           0.3187
============================

Saving the model...
done in 270.17 ms
step 41000; step_loss: 0.1356
step 41010; step_loss: 0.1819
step 41020; step_loss: 0.0874
step 41030; step_loss: 0.1515
step 41040; step_loss: 0.1042
step 41050; step_loss: 0.1061
step 41060; step_loss: 0.1133
step 41070; step_loss: 0.0748
step 41080; step_loss: 0.1221
step 41090; step_loss: 0.0921
step 41100; step_loss: 0.0925
step 41110; step_loss: 0.0766
step 41120; step_loss: 0.0882
step 41130; step_loss: 0.0827
step 41140; step_loss: 0.1417
step 41150; step_loss: 0.0909
step 41160; step_loss: 0.1385
step 41170; step_loss: 0.0702
step 41180; step_loss: 0.0782
step 41190; step_loss: 0.1039
step 41200; step_loss: 0.0863
step 41210; step_loss: 0.0865
step 41220; step_loss: 0.1571
step 41230; step_loss: 0.1523
step 41240; step_loss: 0.0706
step 41250; step_loss: 0.1362
step 41260; step_loss: 0.1205
step 41270; step_loss: 0.1080
step 41280; step_loss: 0.0948
step 41290; step_loss: 0.1018
step 41300; step_loss: 0.0696
step 41310; step_loss: 0.0733
step 41320; step_loss: 0.0909
step 41330; step_loss: 0.1471
step 41340; step_loss: 0.1015
step 41350; step_loss: 0.1391
step 41360; step_loss: 0.1232
step 41370; step_loss: 0.0910
step 41380; step_loss: 0.0779
step 41390; step_loss: 0.1069
step 41400; step_loss: 0.0991
step 41410; step_loss: 0.0823
step 41420; step_loss: 0.0691
step 41430; step_loss: 0.1560
step 41440; step_loss: 0.1383
step 41450; step_loss: 0.0839
step 41460; step_loss: 0.1435
step 41470; step_loss: 0.1004
step 41480; step_loss: 0.0784
step 41490; step_loss: 0.1567
step 41500; step_loss: 0.1152
step 41510; step_loss: 0.1404
step 41520; step_loss: 0.0945
step 41530; step_loss: 0.0699
step 41540; step_loss: 0.1181
step 41550; step_loss: 0.2009
step 41560; step_loss: 0.0885
step 41570; step_loss: 0.1033
step 41580; step_loss: 0.0904
step 41590; step_loss: 0.1436
step 41600; step_loss: 0.0811
step 41610; step_loss: 0.0955
step 41620; step_loss: 0.0891
step 41630; step_loss: 0.1067
step 41640; step_loss: 0.0768
step 41650; step_loss: 0.1211
step 41660; step_loss: 0.1059
step 41670; step_loss: 0.1074
step 41680; step_loss: 0.1047
step 41690; step_loss: 0.1104
step 41700; step_loss: 0.0906
step 41710; step_loss: 0.1648
step 41720; step_loss: 0.0986
step 41730; step_loss: 0.0738
step 41740; step_loss: 0.0954
step 41750; step_loss: 0.0953
step 41760; step_loss: 0.1893
step 41770; step_loss: 0.0986
step 41780; step_loss: 0.0583
step 41790; step_loss: 0.0843
step 41800; step_loss: 0.2219
step 41810; step_loss: 0.0751
step 41820; step_loss: 0.1154
step 41830; step_loss: 0.1043
step 41840; step_loss: 0.1021
step 41850; step_loss: 0.0714
step 41860; step_loss: 0.0972
step 41870; step_loss: 0.1215
step 41880; step_loss: 0.1136
step 41890; step_loss: 0.1229
step 41900; step_loss: 0.1891
step 41910; step_loss: 0.1025
step 41920; step_loss: 0.1274
step 41930; step_loss: 0.0734
step 41940; step_loss: 0.1871
step 41950; step_loss: 0.0735
step 41960; step_loss: 0.1300
step 41970; step_loss: 0.1391
step 41980; step_loss: 0.1051
step 41990; step_loss: 0.2270

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 0.387 | 0.674 | 1.004 | 1.114 |   n/a |   n/a |

============================
Global step:         42000
Learning rate:       0.0041
Step-time (ms):     27.0939
Train loss avg:      0.1066
--------------------------
Val loss:            0.4149
srnn loss:           0.3187
============================

Saving the model...
done in 275.68 ms
step 42000; step_loss: 0.0815
step 42010; step_loss: 0.1287
step 42020; step_loss: 0.1155
step 42030; step_loss: 0.0792
step 42040; step_loss: 0.0792
step 42050; step_loss: 0.1125
step 42060; step_loss: 0.0745
step 42070; step_loss: 0.0908
step 42080; step_loss: 0.0910
step 42090; step_loss: 0.0873
step 42100; step_loss: 0.0956
step 42110; step_loss: 0.0727
step 42120; step_loss: 0.0921
step 42130; step_loss: 0.1084
step 42140; step_loss: 0.0754
step 42150; step_loss: 0.0961
step 42160; step_loss: 0.1190
step 42170; step_loss: 0.1795
step 42180; step_loss: 0.0991
step 42190; step_loss: 0.1086
step 42200; step_loss: 0.0689
step 42210; step_loss: 0.0698
step 42220; step_loss: 0.1069
step 42230; step_loss: 0.0701
step 42240; step_loss: 0.0770
step 42250; step_loss: 0.0681
step 42260; step_loss: 0.1147
step 42270; step_loss: 0.0962
step 42280; step_loss: 0.1321
step 42290; step_loss: 0.0863
step 42300; step_loss: 0.1250
step 42310; step_loss: 0.0959
step 42320; step_loss: 0.1144
step 42330; step_loss: 0.1188
step 42340; step_loss: 0.0933
step 42350; step_loss: 0.0988
step 42360; step_loss: 0.0707
step 42370; step_loss: 0.0797
step 42380; step_loss: 0.0897
step 42390; step_loss: 0.1107
step 42400; step_loss: 0.1179
step 42410; step_loss: 0.2711
step 42420; step_loss: 0.1022
step 42430; step_loss: 0.0794
step 42440; step_loss: 0.0940
step 42450; step_loss: 0.1079
step 42460; step_loss: 0.1032
step 42470; step_loss: 0.0833
step 42480; step_loss: 0.1402
step 42490; step_loss: 0.0930
step 42500; step_loss: 0.1784
step 42510; step_loss: 0.0903
step 42520; step_loss: 0.0963
step 42530; step_loss: 0.0782
step 42540; step_loss: 0.1637
step 42550; step_loss: 0.1152
step 42560; step_loss: 0.1487
step 42570; step_loss: 0.0843
step 42580; step_loss: 0.0892
step 42590; step_loss: 0.0909
step 42600; step_loss: 0.0943
step 42610; step_loss: 0.1346
step 42620; step_loss: 0.0918
step 42630; step_loss: 0.0683
step 42640; step_loss: 0.1392
step 42650; step_loss: 0.0921
step 42660; step_loss: 0.0958
step 42670; step_loss: 0.1090
step 42680; step_loss: 0.0744
step 42690; step_loss: 0.0764
step 42700; step_loss: 0.1011
step 42710; step_loss: 0.0885
step 42720; step_loss: 0.1274
step 42730; step_loss: 0.0988
step 42740; step_loss: 0.0778
step 42750; step_loss: 0.1016
step 42760; step_loss: 0.0977
step 42770; step_loss: 0.1063
step 42780; step_loss: 0.0899
step 42790; step_loss: 0.0865
step 42800; step_loss: 0.1069
step 42810; step_loss: 0.0772
step 42820; step_loss: 0.0667
step 42830; step_loss: 0.0921
step 42840; step_loss: 0.1202
step 42850; step_loss: 0.0751
step 42860; step_loss: 0.0875
step 42870; step_loss: 0.0882
step 42880; step_loss: 0.0799
step 42890; step_loss: 0.1388
step 42900; step_loss: 0.1510
step 42910; step_loss: 0.0738
step 42920; step_loss: 0.1105
step 42930; step_loss: 0.0847
step 42940; step_loss: 0.1181
step 42950; step_loss: 0.0661
step 42960; step_loss: 0.1146
step 42970; step_loss: 0.1097
step 42980; step_loss: 0.1084
step 42990; step_loss: 0.0882

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 0.395 | 0.688 | 1.042 | 1.162 |   n/a |   n/a |

============================
Global step:         43000
Learning rate:       0.0041
Step-time (ms):     27.1445
Train loss avg:      0.1024
--------------------------
Val loss:            0.4060
srnn loss:           0.3296
============================

Saving the model...
done in 274.95 ms
step 43000; step_loss: 0.0834
step 43010; step_loss: 0.0707
step 43020; step_loss: 0.1613
step 43030; step_loss: 0.1133
step 43040; step_loss: 0.0901
step 43050; step_loss: 0.1147
step 43060; step_loss: 0.2015
step 43070; step_loss: 0.1117
step 43080; step_loss: 0.0989
step 43090; step_loss: 0.0754
step 43100; step_loss: 0.0796
step 43110; step_loss: 0.0594
step 43120; step_loss: 0.1259
step 43130; step_loss: 0.1579
step 43140; step_loss: 0.0975
step 43150; step_loss: 0.1158
step 43160; step_loss: 0.0864
step 43170; step_loss: 0.0909
step 43180; step_loss: 0.0974
step 43190; step_loss: 0.0890
step 43200; step_loss: 0.0843
step 43210; step_loss: 0.1156
step 43220; step_loss: 0.1721
step 43230; step_loss: 0.1178
step 43240; step_loss: 0.0972
step 43250; step_loss: 0.0682
step 43260; step_loss: 0.1180
step 43270; step_loss: 0.1568
step 43280; step_loss: 0.1005
step 43290; step_loss: 0.0785
step 43300; step_loss: 0.0745
step 43310; step_loss: 0.0966
step 43320; step_loss: 0.1251
step 43330; step_loss: 0.0968
step 43340; step_loss: 0.1870
step 43350; step_loss: 0.1994
step 43360; step_loss: 0.1023
step 43370; step_loss: 0.1039
step 43380; step_loss: 0.0933
step 43390; step_loss: 0.0697
step 43400; step_loss: 0.1627
step 43410; step_loss: 0.1061
step 43420; step_loss: 0.1312
step 43430; step_loss: 0.1096
step 43440; step_loss: 0.0834
step 43450; step_loss: 0.1046
step 43460; step_loss: 0.2185
step 43470; step_loss: 0.0888
step 43480; step_loss: 0.0949
step 43490; step_loss: 0.1339
step 43500; step_loss: 0.0941
step 43510; step_loss: 0.0910
step 43520; step_loss: 0.1106
step 43530; step_loss: 0.0794
step 43540; step_loss: 0.1419
step 43550; step_loss: 0.1248
step 43560; step_loss: 0.1042
step 43570; step_loss: 0.0773
step 43580; step_loss: 0.0682
step 43590; step_loss: 0.0855
step 43600; step_loss: 0.0951
step 43610; step_loss: 0.0860
step 43620; step_loss: 0.1127
step 43630; step_loss: 0.1149
step 43640; step_loss: 0.1144
step 43650; step_loss: 0.0771
step 43660; step_loss: 0.1088
step 43670; step_loss: 0.0626
step 43680; step_loss: 0.0702
step 43690; step_loss: 0.1207
step 43700; step_loss: 0.0934
step 43710; step_loss: 0.0825
step 43720; step_loss: 0.0810
step 43730; step_loss: 0.0866
step 43740; step_loss: 0.1753
step 43750; step_loss: 0.0997
step 43760; step_loss: 0.1290
step 43770; step_loss: 0.1099
step 43780; step_loss: 0.0952
step 43790; step_loss: 0.1433
step 43800; step_loss: 0.0979
step 43810; step_loss: 0.0817
step 43820; step_loss: 0.1354
step 43830; step_loss: 0.0697
step 43840; step_loss: 0.1248
step 43850; step_loss: 0.0789
step 43860; step_loss: 0.0877
step 43870; step_loss: 0.1011
step 43880; step_loss: 0.0885
step 43890; step_loss: 0.0877
step 43900; step_loss: 0.1242
step 43910; step_loss: 0.1024
step 43920; step_loss: 0.1031
step 43930; step_loss: 0.0903
step 43940; step_loss: 0.1059
step 43950; step_loss: 0.1059
step 43960; step_loss: 0.0792
step 43970; step_loss: 0.0843
step 43980; step_loss: 0.0887
step 43990; step_loss: 0.0808

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 0.396 | 0.691 | 1.045 | 1.168 |   n/a |   n/a |

============================
Global step:         44000
Learning rate:       0.0041
Step-time (ms):     27.1182
Train loss avg:      0.1029
--------------------------
Val loss:            0.4171
srnn loss:           0.3279
============================

Saving the model...
done in 315.08 ms
step 44000; step_loss: 0.0960
step 44010; step_loss: 0.0888
step 44020; step_loss: 0.1352
step 44030; step_loss: 0.0990
step 44040; step_loss: 0.0889
step 44050; step_loss: 0.1955
step 44060; step_loss: 0.1055
step 44070; step_loss: 0.0738
step 44080; step_loss: 0.1128
step 44090; step_loss: 0.0779
step 44100; step_loss: 0.0791
step 44110; step_loss: 0.0863
step 44120; step_loss: 0.1294
step 44130; step_loss: 0.0841
step 44140; step_loss: 0.0944
step 44150; step_loss: 0.0840
step 44160; step_loss: 0.0938
step 44170; step_loss: 0.0759
step 44180; step_loss: 0.0985
step 44190; step_loss: 0.0998
step 44200; step_loss: 0.1255
step 44210; step_loss: 0.1009
step 44220; step_loss: 0.1055
step 44230; step_loss: 0.1033
step 44240; step_loss: 0.1413
step 44250; step_loss: 0.1392
step 44260; step_loss: 0.0921
step 44270; step_loss: 0.0862
step 44280; step_loss: 0.1019
step 44290; step_loss: 0.0575
step 44300; step_loss: 0.0559
step 44310; step_loss: 0.0676
step 44320; step_loss: 0.0997
step 44330; step_loss: 0.0967
step 44340; step_loss: 0.1032
step 44350; step_loss: 0.1105
step 44360; step_loss: 0.0876
step 44370; step_loss: 0.1783
step 44380; step_loss: 0.0988
step 44390; step_loss: 0.0921
step 44400; step_loss: 0.0995
step 44410; step_loss: 0.1202
step 44420; step_loss: 0.0788
step 44430; step_loss: 0.1351
step 44440; step_loss: 0.0656
step 44450; step_loss: 0.0924
step 44460; step_loss: 0.0629
step 44470; step_loss: 0.1002
step 44480; step_loss: 0.0832
step 44490; step_loss: 0.0758
step 44500; step_loss: 0.1140
step 44510; step_loss: 0.1389
step 44520; step_loss: 0.0843
step 44530; step_loss: 0.1357
step 44540; step_loss: 0.1060
step 44550; step_loss: 0.0875
step 44560; step_loss: 0.0810
step 44570; step_loss: 0.0936
step 44580; step_loss: 0.1072
step 44590; step_loss: 0.1484
step 44600; step_loss: 0.0995
step 44610; step_loss: 0.1991
step 44620; step_loss: 0.1140
step 44630; step_loss: 0.0653
step 44640; step_loss: 0.0728
step 44650; step_loss: 0.0964
step 44660; step_loss: 0.1266
step 44670; step_loss: 0.0985
step 44680; step_loss: 0.0734
step 44690; step_loss: 0.0656
step 44700; step_loss: 0.0950
step 44710; step_loss: 0.0757
step 44720; step_loss: 0.1490
step 44730; step_loss: 0.0811
step 44740; step_loss: 0.1233
step 44750; step_loss: 0.1058
step 44760; step_loss: 0.0865
step 44770; step_loss: 0.1853
step 44780; step_loss: 0.1041
step 44790; step_loss: 0.1755
step 44800; step_loss: 0.0784
step 44810; step_loss: 0.1468
step 44820; step_loss: 0.1235
step 44830; step_loss: 0.0659
step 44840; step_loss: 0.0595
step 44850; step_loss: 0.0817
step 44860; step_loss: 0.0977
step 44870; step_loss: 0.0903
step 44880; step_loss: 0.0802
step 44890; step_loss: 0.0955
step 44900; step_loss: 0.0840
step 44910; step_loss: 0.0949
step 44920; step_loss: 0.1108
step 44930; step_loss: 0.0785
step 44940; step_loss: 0.0765
step 44950; step_loss: 0.1432
step 44960; step_loss: 0.1000
step 44970; step_loss: 0.2296
step 44980; step_loss: 0.1216
step 44990; step_loss: 0.1105

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 0.391 | 0.680 | 1.024 | 1.141 |   n/a |   n/a |

============================
Global step:         45000
Learning rate:       0.0041
Step-time (ms):     27.2031
Train loss avg:      0.1021
--------------------------
Val loss:            0.4042
srnn loss:           0.3311
============================

Saving the model...
done in 273.19 ms
step 45000; step_loss: 0.0905
step 45010; step_loss: 0.1181
step 45020; step_loss: 0.0998
step 45030; step_loss: 0.0824
step 45040; step_loss: 0.0761
step 45050; step_loss: 0.0944
step 45060; step_loss: 0.1089
step 45070; step_loss: 0.0699
step 45080; step_loss: 0.0719
step 45090; step_loss: 0.0771
step 45100; step_loss: 0.0806
step 45110; step_loss: 0.1194
step 45120; step_loss: 0.1310
step 45130; step_loss: 0.1376
step 45140; step_loss: 0.0889
step 45150; step_loss: 0.1101
step 45160; step_loss: 0.1005
step 45170; step_loss: 0.1001
step 45180; step_loss: 0.0979
step 45190; step_loss: 0.1049
step 45200; step_loss: 0.0686
step 45210; step_loss: 0.0737
step 45220; step_loss: 0.0755
step 45230; step_loss: 0.0941
step 45240; step_loss: 0.0813
step 45250; step_loss: 0.0886
step 45260; step_loss: 0.0726
step 45270; step_loss: 0.0806
step 45280; step_loss: 0.0787
step 45290; step_loss: 0.0967
step 45300; step_loss: 0.0696
step 45310; step_loss: 0.1211
step 45320; step_loss: 0.0967
step 45330; step_loss: 0.0699
step 45340; step_loss: 0.0793
step 45350; step_loss: 0.1054
step 45360; step_loss: 0.1049
step 45370; step_loss: 0.1104
step 45380; step_loss: 0.0914
step 45390; step_loss: 0.0851
step 45400; step_loss: 0.1050
step 45410; step_loss: 0.0974
step 45420; step_loss: 0.0764
step 45430; step_loss: 0.0624
step 45440; step_loss: 0.0998
step 45450; step_loss: 0.1035
step 45460; step_loss: 0.1083
step 45470; step_loss: 0.0962
step 45480; step_loss: 0.1588
step 45490; step_loss: 0.0673
step 45500; step_loss: 0.0972
step 45510; step_loss: 0.1119
step 45520; step_loss: 0.1180
step 45530; step_loss: 0.0899
step 45540; step_loss: 0.0774
step 45550; step_loss: 0.1013
step 45560; step_loss: 0.1187
step 45570; step_loss: 0.0872
step 45580; step_loss: 0.1360
step 45590; step_loss: 0.1046
step 45600; step_loss: 0.0992
step 45610; step_loss: 0.1203
step 45620; step_loss: 0.0851
step 45630; step_loss: 0.0771
step 45640; step_loss: 0.0955
step 45650; step_loss: 0.0857
step 45660; step_loss: 0.0888
step 45670; step_loss: 0.0954
step 45680; step_loss: 0.0856
step 45690; step_loss: 0.0865
step 45700; step_loss: 0.0936
step 45710; step_loss: 0.0860
step 45720; step_loss: 0.1230
step 45730; step_loss: 0.1382
step 45740; step_loss: 0.1158
step 45750; step_loss: 0.1051
step 45760; step_loss: 0.1437
step 45770; step_loss: 0.0621
step 45780; step_loss: 0.0930
step 45790; step_loss: 0.0954
step 45800; step_loss: 0.0795
step 45810; step_loss: 0.0748
step 45820; step_loss: 0.0777
step 45830; step_loss: 0.1366
step 45840; step_loss: 0.1144
step 45850; step_loss: 0.1220
step 45860; step_loss: 0.1076
step 45870; step_loss: 0.0895
step 45880; step_loss: 0.0792
step 45890; step_loss: 0.0884
step 45900; step_loss: 0.1699
step 45910; step_loss: 0.1263
step 45920; step_loss: 0.1047
step 45930; step_loss: 0.0692
step 45940; step_loss: 0.0794
step 45950; step_loss: 0.0697
step 45960; step_loss: 0.0966
step 45970; step_loss: 0.0784
step 45980; step_loss: 0.1596
step 45990; step_loss: 0.1130

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 0.394 | 0.686 | 1.044 | 1.170 |   n/a |   n/a |

============================
Global step:         46000
Learning rate:       0.0041
Step-time (ms):     27.1306
Train loss avg:      0.1011
--------------------------
Val loss:            0.3803
srnn loss:           0.3262
============================

Saving the model...
done in 264.32 ms
step 46000; step_loss: 0.1508
step 46010; step_loss: 0.0972
step 46020; step_loss: 0.0998
step 46030; step_loss: 0.1062
step 46040; step_loss: 0.0969
step 46050; step_loss: 0.1230
step 46060; step_loss: 0.0825
step 46070; step_loss: 0.0701
step 46080; step_loss: 0.0837
step 46090; step_loss: 0.1107
step 46100; step_loss: 0.0793
step 46110; step_loss: 0.1216
step 46120; step_loss: 0.0890
step 46130; step_loss: 0.0969
step 46140; step_loss: 0.0934
step 46150; step_loss: 0.1178
step 46160; step_loss: 0.1051
step 46170; step_loss: 0.0834
step 46180; step_loss: 0.0730
step 46190; step_loss: 0.0786
step 46200; step_loss: 0.0952
step 46210; step_loss: 0.0802
step 46220; step_loss: 0.0981
step 46230; step_loss: 0.1156
step 46240; step_loss: 0.0766
step 46250; step_loss: 0.1479
step 46260; step_loss: 0.1047
step 46270; step_loss: 0.0877
step 46280; step_loss: 0.0908
step 46290; step_loss: 0.1073
step 46300; step_loss: 0.0773
step 46310; step_loss: 0.1167
step 46320; step_loss: 0.0727
step 46330; step_loss: 0.0969
step 46340; step_loss: 0.1770
step 46350; step_loss: 0.1246
step 46360; step_loss: 0.1080
step 46370; step_loss: 0.0839
step 46380; step_loss: 0.0654
step 46390; step_loss: 0.0853
step 46400; step_loss: 0.0803
step 46410; step_loss: 0.1104
step 46420; step_loss: 0.0923
step 46430; step_loss: 0.0719
step 46440; step_loss: 0.0923
step 46450; step_loss: 0.1167
step 46460; step_loss: 0.1201
step 46470; step_loss: 0.0666
step 46480; step_loss: 0.2212
step 46490; step_loss: 0.0917
step 46500; step_loss: 0.1133
step 46510; step_loss: 0.1619
step 46520; step_loss: 0.1081
step 46530; step_loss: 0.1028
step 46540; step_loss: 0.0777
step 46550; step_loss: 0.0890
step 46560; step_loss: 0.1416
step 46570; step_loss: 0.0937
step 46580; step_loss: 0.1099
step 46590; step_loss: 0.1081
step 46600; step_loss: 0.0579
step 46610; step_loss: 0.1088
step 46620; step_loss: 0.1090
step 46630; step_loss: 0.1378
step 46640; step_loss: 0.0968
step 46650; step_loss: 0.0963
step 46660; step_loss: 0.0911
step 46670; step_loss: 0.0714
step 46680; step_loss: 0.0855
step 46690; step_loss: 0.0913
step 46700; step_loss: 0.0875
step 46710; step_loss: 0.1199
step 46720; step_loss: 0.0685
step 46730; step_loss: 0.0802
step 46740; step_loss: 0.0918
step 46750; step_loss: 0.0691
step 46760; step_loss: 0.1020
step 46770; step_loss: 0.0950
step 46780; step_loss: 0.1125
step 46790; step_loss: 0.1151
step 46800; step_loss: 0.0821
step 46810; step_loss: 0.0925
step 46820; step_loss: 0.0886
step 46830; step_loss: 0.0950
step 46840; step_loss: 0.1150
step 46850; step_loss: 0.1113
step 46860; step_loss: 0.0827
step 46870; step_loss: 0.1153
step 46880; step_loss: 0.0838
step 46890; step_loss: 0.1067
step 46900; step_loss: 0.0987
step 46910; step_loss: 0.0743
step 46920; step_loss: 0.0865
step 46930; step_loss: 0.0866
step 46940; step_loss: 0.0934
step 46950; step_loss: 0.1284
step 46960; step_loss: 0.0886
step 46970; step_loss: 0.0877
step 46980; step_loss: 0.1328
step 46990; step_loss: 0.0632

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 0.395 | 0.688 | 1.038 | 1.155 |   n/a |   n/a |

============================
Global step:         47000
Learning rate:       0.0041
Step-time (ms):     27.1433
Train loss avg:      0.1011
--------------------------
Val loss:            0.3793
srnn loss:           0.3281
============================

Saving the model...
done in 269.89 ms
step 47000; step_loss: 0.1262
step 47010; step_loss: 0.0781
step 47020; step_loss: 0.1045
step 47030; step_loss: 0.0801
step 47040; step_loss: 0.0924
step 47050; step_loss: 0.0794
step 47060; step_loss: 0.1075
step 47070; step_loss: 0.1220
step 47080; step_loss: 0.1478
step 47090; step_loss: 0.0974
step 47100; step_loss: 0.0983
step 47110; step_loss: 0.0852
step 47120; step_loss: 0.1027
step 47130; step_loss: 0.1123
step 47140; step_loss: 0.0958
step 47150; step_loss: 0.0880
step 47160; step_loss: 0.1305
step 47170; step_loss: 0.1568
step 47180; step_loss: 0.0996
step 47190; step_loss: 0.0766
step 47200; step_loss: 0.1429
step 47210; step_loss: 0.0825
step 47220; step_loss: 0.0892
step 47230; step_loss: 0.1170
step 47240; step_loss: 0.0719
step 47250; step_loss: 0.1237
step 47260; step_loss: 0.0731
step 47270; step_loss: 0.1016
step 47280; step_loss: 0.1134
step 47290; step_loss: 0.1383
step 47300; step_loss: 0.1173
step 47310; step_loss: 0.1247
step 47320; step_loss: 0.1221
step 47330; step_loss: 0.1230
step 47340; step_loss: 0.1198
step 47350; step_loss: 0.0699
step 47360; step_loss: 0.1196
step 47370; step_loss: 0.1140
step 47380; step_loss: 0.0899
step 47390; step_loss: 0.0788
step 47400; step_loss: 0.1220
step 47410; step_loss: 0.1082
step 47420; step_loss: 0.1165
step 47430; step_loss: 0.1008
step 47440; step_loss: 0.0872
step 47450; step_loss: 0.0856
step 47460; step_loss: 0.0735
step 47470; step_loss: 0.0785
step 47480; step_loss: 0.0923
step 47490; step_loss: 0.0882
step 47500; step_loss: 0.0724
step 47510; step_loss: 0.1103
step 47520; step_loss: 0.0987
step 47530; step_loss: 0.1207
step 47540; step_loss: 0.0838
step 47550; step_loss: 0.1217
step 47560; step_loss: 0.1493
step 47570; step_loss: 0.1075
step 47580; step_loss: 0.1148
step 47590; step_loss: 0.0902
step 47600; step_loss: 0.0827
step 47610; step_loss: 0.0996
step 47620; step_loss: 0.1727
step 47630; step_loss: 0.0745
step 47640; step_loss: 0.0860
step 47650; step_loss: 0.0641
step 47660; step_loss: 0.0759
step 47670; step_loss: 0.1340
step 47680; step_loss: 0.1134
step 47690; step_loss: 0.1120
step 47700; step_loss: 0.1173
step 47710; step_loss: 0.0973
step 47720; step_loss: 0.1115
step 47730; step_loss: 0.1077
step 47740; step_loss: 0.0913
step 47750; step_loss: 0.0847
step 47760; step_loss: 0.1214
step 47770; step_loss: 0.1024
step 47780; step_loss: 0.0618
step 47790; step_loss: 0.0860
step 47800; step_loss: 0.0895
step 47810; step_loss: 0.0928
step 47820; step_loss: 0.0702
step 47830; step_loss: 0.0726
step 47840; step_loss: 0.0648
step 47850; step_loss: 0.0584
step 47860; step_loss: 0.2182
step 47870; step_loss: 0.0793
step 47880; step_loss: 0.0971
step 47890; step_loss: 0.0887
step 47900; step_loss: 0.0829
step 47910; step_loss: 0.0795
step 47920; step_loss: 0.0903
step 47930; step_loss: 0.1133
step 47940; step_loss: 0.0970
step 47950; step_loss: 0.0923
step 47960; step_loss: 0.0949
step 47970; step_loss: 0.1482
step 47980; step_loss: 0.0997
step 47990; step_loss: 0.0750

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 0.393 | 0.685 | 1.031 | 1.146 |   n/a |   n/a |

============================
Global step:         48000
Learning rate:       0.0041
Step-time (ms):     27.1214
Train loss avg:      0.1024
--------------------------
Val loss:            0.3809
srnn loss:           0.3351
============================

Saving the model...
done in 278.22 ms
step 48000; step_loss: 0.0985
step 48010; step_loss: 0.1643
step 48020; step_loss: 0.0861
step 48030; step_loss: 0.1294
step 48040; step_loss: 0.0836
step 48050; step_loss: 0.0547
step 48060; step_loss: 0.1424
step 48070; step_loss: 0.1235
step 48080; step_loss: 0.1356
step 48090; step_loss: 0.1287
step 48100; step_loss: 0.0790
step 48110; step_loss: 0.0843
step 48120; step_loss: 0.1573
step 48130; step_loss: 0.0687
step 48140; step_loss: 0.0864
step 48150; step_loss: 0.1256
step 48160; step_loss: 0.0730
step 48170; step_loss: 0.2841
step 48180; step_loss: 0.1480
step 48190; step_loss: 0.0742
step 48200; step_loss: 0.0738
step 48210; step_loss: 0.1883
step 48220; step_loss: 0.0609
step 48230; step_loss: 0.0877
step 48240; step_loss: 0.1110
step 48250; step_loss: 0.0743
step 48260; step_loss: 0.0628
step 48270; step_loss: 0.0729
step 48280; step_loss: 0.1432
step 48290; step_loss: 0.0986
step 48300; step_loss: 0.0783
step 48310; step_loss: 0.0860
step 48320; step_loss: 0.1194
step 48330; step_loss: 0.0529
step 48340; step_loss: 0.0938
step 48350; step_loss: 0.0711
step 48360; step_loss: 0.0996
step 48370; step_loss: 0.0708
step 48380; step_loss: 0.1178
step 48390; step_loss: 0.0901
step 48400; step_loss: 0.2375
step 48410; step_loss: 0.1420
step 48420; step_loss: 0.0698
step 48430; step_loss: 0.0715
step 48440; step_loss: 0.0889
step 48450; step_loss: 0.1829
step 48460; step_loss: 0.1636
step 48470; step_loss: 0.0719
step 48480; step_loss: 0.1288
step 48490; step_loss: 0.0656
step 48500; step_loss: 0.0979
step 48510; step_loss: 0.0711
step 48520; step_loss: 0.1465
step 48530; step_loss: 0.0870
step 48540; step_loss: 0.0817
step 48550; step_loss: 0.1019
step 48560; step_loss: 0.1077
step 48570; step_loss: 0.0770
step 48580; step_loss: 0.0842
step 48590; step_loss: 0.0888
step 48600; step_loss: 0.1405
step 48610; step_loss: 0.0668
step 48620; step_loss: 0.0973
step 48630; step_loss: 0.1323
step 48640; step_loss: 0.1318
step 48650; step_loss: 0.0725
step 48660; step_loss: 0.0761
step 48670; step_loss: 0.1238
step 48680; step_loss: 0.0815
step 48690; step_loss: 0.0951
step 48700; step_loss: 0.1022
step 48710; step_loss: 0.0873
step 48720; step_loss: 0.0822
step 48730; step_loss: 0.1172
step 48740; step_loss: 0.0764
step 48750; step_loss: 0.1016
step 48760; step_loss: 0.1335
step 48770; step_loss: 0.2070
step 48780; step_loss: 0.0861
step 48790; step_loss: 0.1408
step 48800; step_loss: 0.1041
step 48810; step_loss: 0.0794
step 48820; step_loss: 0.1144
step 48830; step_loss: 0.0996
step 48840; step_loss: 0.1519
step 48850; step_loss: 0.0950
step 48860; step_loss: 0.1281
step 48870; step_loss: 0.1154
step 48880; step_loss: 0.0972
step 48890; step_loss: 0.0720
step 48900; step_loss: 0.0741
step 48910; step_loss: 0.1481
step 48920; step_loss: 0.0730
step 48930; step_loss: 0.1080
step 48940; step_loss: 0.0920
step 48950; step_loss: 0.0790
step 48960; step_loss: 0.0794
step 48970; step_loss: 0.0667
step 48980; step_loss: 0.0938
step 48990; step_loss: 0.1362

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 0.396 | 0.690 | 1.047 | 1.167 |   n/a |   n/a |

============================
Global step:         49000
Learning rate:       0.0041
Step-time (ms):     27.0812
Train loss avg:      0.1006
--------------------------
Val loss:            0.3863
srnn loss:           0.3346
============================

Saving the model...
done in 266.45 ms
step 49000; step_loss: 0.0810
step 49010; step_loss: 0.0882
step 49020; step_loss: 0.0832
step 49030; step_loss: 0.0682
step 49040; step_loss: 0.0756
step 49050; step_loss: 0.0671
step 49060; step_loss: 0.0920
step 49070; step_loss: 0.1715
step 49080; step_loss: 0.0652
step 49090; step_loss: 0.0736
step 49100; step_loss: 0.1127
step 49110; step_loss: 0.1462
step 49120; step_loss: 0.0891
step 49130; step_loss: 0.0781
step 49140; step_loss: 0.1024
step 49150; step_loss: 0.0828
step 49160; step_loss: 0.0793
step 49170; step_loss: 0.0895
step 49180; step_loss: 0.0874
step 49190; step_loss: 0.0720
step 49200; step_loss: 0.0777
step 49210; step_loss: 0.1133
step 49220; step_loss: 0.1047
step 49230; step_loss: 0.0768
step 49240; step_loss: 0.0745
step 49250; step_loss: 0.1300
step 49260; step_loss: 0.1264
step 49270; step_loss: 0.0847
step 49280; step_loss: 0.0669
step 49290; step_loss: 0.1026
step 49300; step_loss: 0.0935
step 49310; step_loss: 0.0917
step 49320; step_loss: 0.1653
step 49330; step_loss: 0.0934
step 49340; step_loss: 0.0875
step 49350; step_loss: 0.0869
step 49360; step_loss: 0.0917
step 49370; step_loss: 0.1027
step 49380; step_loss: 0.0996
step 49390; step_loss: 0.1084
step 49400; step_loss: 0.1104
step 49410; step_loss: 0.0830
step 49420; step_loss: 0.0978
step 49430; step_loss: 0.1036
step 49440; step_loss: 0.0855
step 49450; step_loss: 0.0855
step 49460; step_loss: 0.0949
step 49470; step_loss: 0.0843
step 49480; step_loss: 0.0838
step 49490; step_loss: 0.1001
step 49500; step_loss: 0.0922
step 49510; step_loss: 0.0653
step 49520; step_loss: 0.0856
step 49530; step_loss: 0.0747
step 49540; step_loss: 0.1126
step 49550; step_loss: 0.1011
step 49560; step_loss: 0.0910
step 49570; step_loss: 0.0711
step 49580; step_loss: 0.0729
step 49590; step_loss: 0.0685
step 49600; step_loss: 0.0710
step 49610; step_loss: 0.0727
step 49620; step_loss: 0.0711
step 49630; step_loss: 0.0961
step 49640; step_loss: 0.0865
step 49650; step_loss: 0.0846
step 49660; step_loss: 0.0975
step 49670; step_loss: 0.1935
step 49680; step_loss: 0.0849
step 49690; step_loss: 0.1078
step 49700; step_loss: 0.1314
step 49710; step_loss: 0.1021
step 49720; step_loss: 0.1235
step 49730; step_loss: 0.0643
step 49740; step_loss: 0.1134
step 49750; step_loss: 0.1147
step 49760; step_loss: 0.0684
step 49770; step_loss: 0.1266
step 49780; step_loss: 0.0972
step 49790; step_loss: 0.0781
step 49800; step_loss: 0.0835
step 49810; step_loss: 0.0806
step 49820; step_loss: 0.1500
step 49830; step_loss: 0.1050
step 49840; step_loss: 0.0733
step 49850; step_loss: 0.1138
step 49860; step_loss: 0.0867
step 49870; step_loss: 0.1059
step 49880; step_loss: 0.0919
step 49890; step_loss: 0.1012
step 49900; step_loss: 0.0889
step 49910; step_loss: 0.1357
step 49920; step_loss: 0.0861
step 49930; step_loss: 0.1152
step 49940; step_loss: 0.0572
step 49950; step_loss: 0.0959
step 49960; step_loss: 0.0831
step 49970; step_loss: 0.0975
step 49980; step_loss: 0.0921
step 49990; step_loss: 0.1403

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 0.401 | 0.700 | 1.057 | 1.177 |   n/a |   n/a |

============================
Global step:         50000
Learning rate:       0.0039
Step-time (ms):     27.2172
Train loss avg:      0.0982
--------------------------
Val loss:            0.4584
srnn loss:           0.3362
============================

Saving the model...
done in 268.03 ms
step 50000; step_loss: 0.0796
step 50010; step_loss: 0.0881
step 50020; step_loss: 0.0617
step 50030; step_loss: 0.0874
step 50040; step_loss: 0.1233
step 50050; step_loss: 0.0992
step 50060; step_loss: 0.1121
step 50070; step_loss: 0.1087
step 50080; step_loss: 0.0714
step 50090; step_loss: 0.1223
step 50100; step_loss: 0.0743
step 50110; step_loss: 0.0796
step 50120; step_loss: 0.1108
step 50130; step_loss: 0.1234
step 50140; step_loss: 0.0935
step 50150; step_loss: 0.0824
step 50160; step_loss: 0.0923
step 50170; step_loss: 0.1098
step 50180; step_loss: 0.1244
step 50190; step_loss: 0.0895
step 50200; step_loss: 0.0655
step 50210; step_loss: 0.0758
step 50220; step_loss: 0.0788
step 50230; step_loss: 0.0710
step 50240; step_loss: 0.0752
step 50250; step_loss: 0.1002
step 50260; step_loss: 0.1032
step 50270; step_loss: 0.1332
step 50280; step_loss: 0.0817
step 50290; step_loss: 0.1084
step 50300; step_loss: 0.0659
step 50310; step_loss: 0.1256
step 50320; step_loss: 0.1042
step 50330; step_loss: 0.0740
step 50340; step_loss: 0.0980
step 50350; step_loss: 0.0684
step 50360; step_loss: 0.1295
step 50370; step_loss: 0.0977
step 50380; step_loss: 0.0711
step 50390; step_loss: 0.1134
step 50400; step_loss: 0.0960
step 50410; step_loss: 0.1331
step 50420; step_loss: 0.0729
step 50430; step_loss: 0.0811
step 50440; step_loss: 0.0784
step 50450; step_loss: 0.0882
step 50460; step_loss: 0.0939
step 50470; step_loss: 0.1169
step 50480; step_loss: 0.1147
step 50490; step_loss: 0.0882
step 50500; step_loss: 0.1096
step 50510; step_loss: 0.0719
step 50520; step_loss: 0.0632
step 50530; step_loss: 0.0875
step 50540; step_loss: 0.1069
step 50550; step_loss: 0.1009
step 50560; step_loss: 0.0661
step 50570; step_loss: 0.0973
step 50580; step_loss: 0.0812
step 50590; step_loss: 0.1354
step 50600; step_loss: 0.0774
step 50610; step_loss: 0.1521
step 50620; step_loss: 0.0716
step 50630; step_loss: 0.0915
step 50640; step_loss: 0.1003
step 50650; step_loss: 0.1073
step 50660; step_loss: 0.1287
step 50670; step_loss: 0.0782
step 50680; step_loss: 0.0839
step 50690; step_loss: 0.0977
step 50700; step_loss: 0.1216
step 50710; step_loss: 0.0778
step 50720; step_loss: 0.1321
step 50730; step_loss: 0.0850
step 50740; step_loss: 0.0790
step 50750; step_loss: 0.1844
step 50760; step_loss: 0.1186
step 50770; step_loss: 0.1277
step 50780; step_loss: 0.1005
step 50790; step_loss: 0.0951
step 50800; step_loss: 0.1510
step 50810; step_loss: 0.0907
step 50820; step_loss: 0.0733
step 50830; step_loss: 0.1241
step 50840; step_loss: 0.0683
step 50850; step_loss: 0.0944
step 50860; step_loss: 0.0864
step 50870; step_loss: 0.0816
step 50880; step_loss: 0.1506
step 50890; step_loss: 0.0914
step 50900; step_loss: 0.0757
step 50910; step_loss: 0.1204
step 50920; step_loss: 0.1236
step 50930; step_loss: 0.0743
step 50940; step_loss: 0.0653
step 50950; step_loss: 0.1198
step 50960; step_loss: 0.0943
step 50970; step_loss: 0.1079
step 50980; step_loss: 0.0742
step 50990; step_loss: 0.1093

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 0.399 | 0.695 | 1.059 | 1.185 |   n/a |   n/a |

============================
Global step:         51000
Learning rate:       0.0039
Step-time (ms):     27.1219
Train loss avg:      0.0997
--------------------------
Val loss:            0.3796
srnn loss:           0.3382
============================

Saving the model...
done in 264.37 ms
step 51000; step_loss: 0.1279
step 51010; step_loss: 0.0886
step 51020; step_loss: 0.1043
step 51030; step_loss: 0.0849
step 51040; step_loss: 0.0962
step 51050; step_loss: 0.0840
step 51060; step_loss: 0.0532
step 51070; step_loss: 0.0857
step 51080; step_loss: 0.0691
step 51090; step_loss: 0.0805
step 51100; step_loss: 0.0789
step 51110; step_loss: 0.0659
step 51120; step_loss: 0.0918
step 51130; step_loss: 0.0630
step 51140; step_loss: 0.0652
step 51150; step_loss: 0.0973
step 51160; step_loss: 0.0617
step 51170; step_loss: 0.0757
step 51180; step_loss: 0.1102
step 51190; step_loss: 0.1006
step 51200; step_loss: 0.1126
step 51210; step_loss: 0.0843
step 51220; step_loss: 0.1213
step 51230; step_loss: 0.0524
step 51240; step_loss: 0.0922
step 51250; step_loss: 0.0852
step 51260; step_loss: 0.0852
step 51270; step_loss: 0.0782
step 51280; step_loss: 0.1035
step 51290; step_loss: 0.1037
step 51300; step_loss: 0.0882
step 51310; step_loss: 0.0665
step 51320; step_loss: 0.0666
step 51330; step_loss: 0.0753
step 51340; step_loss: 0.0682
step 51350; step_loss: 0.0614
step 51360; step_loss: 0.0663
step 51370; step_loss: 0.0739
step 51380; step_loss: 0.0650
step 51390; step_loss: 0.0748
step 51400; step_loss: 0.0693
step 51410; step_loss: 0.0916
step 51420; step_loss: 0.1207
step 51430; step_loss: 0.0772
step 51440; step_loss: 0.0762
step 51450; step_loss: 0.0915
step 51460; step_loss: 0.0982
step 51470; step_loss: 0.0763
step 51480; step_loss: 0.1407
step 51490; step_loss: 0.1162
step 51500; step_loss: 0.0815
step 51510; step_loss: 0.0857
step 51520; step_loss: 0.1009
step 51530; step_loss: 0.1087
step 51540; step_loss: 0.1045
step 51550; step_loss: 0.0842
step 51560; step_loss: 0.0701
step 51570; step_loss: 0.0766
step 51580; step_loss: 0.0901
step 51590; step_loss: 0.0727
step 51600; step_loss: 0.1011
step 51610; step_loss: 0.1618
step 51620; step_loss: 0.0724
step 51630; step_loss: 0.0780
step 51640; step_loss: 0.1059
step 51650; step_loss: 0.0967
step 51660; step_loss: 0.1275
step 51670; step_loss: 0.1681
step 51680; step_loss: 0.0842
step 51690; step_loss: 0.1050
step 51700; step_loss: 0.1085
step 51710; step_loss: 0.0935
step 51720; step_loss: 0.0994
step 51730; step_loss: 0.1568
step 51740; step_loss: 0.1264
step 51750; step_loss: 0.2522
step 51760; step_loss: 0.1277
step 51770; step_loss: 0.0721
step 51780; step_loss: 0.0884
step 51790; step_loss: 0.1357
step 51800; step_loss: 0.0602
step 51810; step_loss: 0.0732
step 51820; step_loss: 0.0933
step 51830; step_loss: 0.0956
step 51840; step_loss: 0.0674
step 51850; step_loss: 0.0941
step 51860; step_loss: 0.0712
step 51870; step_loss: 0.0842
step 51880; step_loss: 0.0911
step 51890; step_loss: 0.0936
step 51900; step_loss: 0.1009
step 51910; step_loss: 0.1066
step 51920; step_loss: 0.0875
step 51930; step_loss: 0.0983
step 51940; step_loss: 0.0868
step 51950; step_loss: 0.0930
step 51960; step_loss: 0.1795
step 51970; step_loss: 0.1290
step 51980; step_loss: 0.1018
step 51990; step_loss: 0.1582

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 0.403 | 0.702 | 1.061 | 1.182 |   n/a |   n/a |

============================
Global step:         52000
Learning rate:       0.0039
Step-time (ms):     27.1436
Train loss avg:      0.0970
--------------------------
Val loss:            0.3690
srnn loss:           0.3406
============================

Saving the model...
done in 265.90 ms
step 52000; step_loss: 0.0986
step 52010; step_loss: 0.0809
step 52020; step_loss: 0.0904
step 52030; step_loss: 0.1017
step 52040; step_loss: 0.1297
step 52050; step_loss: 0.0641
step 52060; step_loss: 0.0965
step 52070; step_loss: 0.0744
step 52080; step_loss: 0.0873
step 52090; step_loss: 0.0788
step 52100; step_loss: 0.0932
step 52110; step_loss: 0.0880
step 52120; step_loss: 0.1125
step 52130; step_loss: 0.0821
step 52140; step_loss: 0.0609
step 52150; step_loss: 0.1412
step 52160; step_loss: 0.0842
step 52170; step_loss: 0.0752
step 52180; step_loss: 0.0965
step 52190; step_loss: 0.1316
step 52200; step_loss: 0.0893
step 52210; step_loss: 0.0952
step 52220; step_loss: 0.1609
step 52230; step_loss: 0.0593
step 52240; step_loss: 0.0770
step 52250; step_loss: 0.0920
step 52260; step_loss: 0.0800
step 52270; step_loss: 0.0660
step 52280; step_loss: 0.0844
step 52290; step_loss: 0.0689
step 52300; step_loss: 0.0955
step 52310; step_loss: 0.1136
step 52320; step_loss: 0.0860
step 52330; step_loss: 0.0716
step 52340; step_loss: 0.1079
step 52350; step_loss: 0.0846
step 52360; step_loss: 0.0811
step 52370; step_loss: 0.0978
step 52380; step_loss: 0.1520
step 52390; step_loss: 0.1336
step 52400; step_loss: 0.1084
step 52410; step_loss: 0.0843
step 52420; step_loss: 0.0714
step 52430; step_loss: 0.1262
step 52440; step_loss: 0.1299
step 52450; step_loss: 0.1620
step 52460; step_loss: 0.1076
step 52470; step_loss: 0.0671
step 52480; step_loss: 0.0915
step 52490; step_loss: 0.0654
step 52500; step_loss: 0.1079
step 52510; step_loss: 0.1270
step 52520; step_loss: 0.0884
step 52530; step_loss: 0.0971
step 52540; step_loss: 0.1108
step 52550; step_loss: 0.1235
step 52560; step_loss: 0.0875
step 52570; step_loss: 0.0839
step 52580; step_loss: 0.0631
step 52590; step_loss: 0.0877
step 52600; step_loss: 0.1705
step 52610; step_loss: 0.0818
step 52620; step_loss: 0.1225
step 52630; step_loss: 0.1065
step 52640; step_loss: 0.0804
step 52650; step_loss: 0.0768
step 52660; step_loss: 0.1702
step 52670; step_loss: 0.1754
step 52680; step_loss: 0.0714
step 52690; step_loss: 0.1348
step 52700; step_loss: 0.0634
step 52710; step_loss: 0.0935
step 52720; step_loss: 0.0688
step 52730; step_loss: 0.1215
step 52740; step_loss: 0.0892
step 52750; step_loss: 0.0672
step 52760; step_loss: 0.0722
step 52770; step_loss: 0.1188
step 52780; step_loss: 0.0661
step 52790; step_loss: 0.0672
step 52800; step_loss: 0.0783
step 52810; step_loss: 0.0687
step 52820; step_loss: 0.0768
step 52830; step_loss: 0.0905
step 52840; step_loss: 0.0702
step 52850; step_loss: 0.1105
step 52860; step_loss: 0.0618
step 52870; step_loss: 0.0802
step 52880; step_loss: 0.0885
step 52890; step_loss: 0.1278
step 52900; step_loss: 0.0888
step 52910; step_loss: 0.0805
step 52920; step_loss: 0.0670
step 52930; step_loss: 0.1002
step 52940; step_loss: 0.0513
step 52950; step_loss: 0.1104
step 52960; step_loss: 0.1538
step 52970; step_loss: 0.0899
step 52980; step_loss: 0.0904
step 52990; step_loss: 0.0556

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 0.400 | 0.695 | 1.058 | 1.181 |   n/a |   n/a |

============================
Global step:         53000
Learning rate:       0.0039
Step-time (ms):     27.0897
Train loss avg:      0.0978
--------------------------
Val loss:            0.3909
srnn loss:           0.3344
============================

Saving the model...
done in 266.37 ms
step 53000; step_loss: 0.1284
step 53010; step_loss: 0.1006
step 53020; step_loss: 0.0754
step 53030; step_loss: 0.0744
step 53040; step_loss: 0.1057
step 53050; step_loss: 0.0902
step 53060; step_loss: 0.0989
step 53070; step_loss: 0.1099
step 53080; step_loss: 0.0772
step 53090; step_loss: 0.1091
step 53100; step_loss: 0.1393
step 53110; step_loss: 0.0799
step 53120; step_loss: 0.0848
step 53130; step_loss: 0.0651
step 53140; step_loss: 0.0772
step 53150; step_loss: 0.0666
step 53160; step_loss: 0.0861
step 53170; step_loss: 0.1171
step 53180; step_loss: 0.1443
step 53190; step_loss: 0.0792
step 53200; step_loss: 0.0845
step 53210; step_loss: 0.0637
step 53220; step_loss: 0.1271
step 53230; step_loss: 0.0751
step 53240; step_loss: 0.0966
step 53250; step_loss: 0.1265
step 53260; step_loss: 0.0920
step 53270; step_loss: 0.0726
step 53280; step_loss: 0.0869
step 53290; step_loss: 0.0811
step 53300; step_loss: 0.0904
step 53310; step_loss: 0.1301
step 53320; step_loss: 0.0873
step 53330; step_loss: 0.0889
step 53340; step_loss: 0.1227
step 53350; step_loss: 0.0627
step 53360; step_loss: 0.1009
step 53370; step_loss: 0.0707
step 53380; step_loss: 0.1112
step 53390; step_loss: 0.1632
step 53400; step_loss: 0.1099
step 53410; step_loss: 0.1331
step 53420; step_loss: 0.0757
step 53430; step_loss: 0.1167
step 53440; step_loss: 0.0730
step 53450; step_loss: 0.1175
step 53460; step_loss: 0.0844
step 53470; step_loss: 0.0715
step 53480; step_loss: 0.1925
step 53490; step_loss: 0.0828
step 53500; step_loss: 0.1006
step 53510; step_loss: 0.0755
step 53520; step_loss: 0.0875
step 53530; step_loss: 0.0752
step 53540; step_loss: 0.0883
step 53550; step_loss: 0.0717
step 53560; step_loss: 0.0560
step 53570; step_loss: 0.0696
step 53580; step_loss: 0.1060
step 53590; step_loss: 0.1135
step 53600; step_loss: 0.0668
step 53610; step_loss: 0.1394
step 53620; step_loss: 0.0803
step 53630; step_loss: 0.0606
step 53640; step_loss: 0.0953
step 53650; step_loss: 0.0932
step 53660; step_loss: 0.1080
step 53670; step_loss: 0.0765
step 53680; step_loss: 0.1020
step 53690; step_loss: 0.0970
step 53700; step_loss: 0.0744
step 53710; step_loss: 0.0809
step 53720; step_loss: 0.0681
step 53730; step_loss: 0.0970
step 53740; step_loss: 0.0838
step 53750; step_loss: 0.0797
step 53760; step_loss: 0.1298
step 53770; step_loss: 0.1204
step 53780; step_loss: 0.0973
step 53790; step_loss: 0.0875
step 53800; step_loss: 0.0910
step 53810; step_loss: 0.0911
step 53820; step_loss: 0.0818
step 53830; step_loss: 0.0720
step 53840; step_loss: 0.0893
step 53850; step_loss: 0.0687
step 53860; step_loss: 0.1141
step 53870; step_loss: 0.0675
step 53880; step_loss: 0.0853
step 53890; step_loss: 0.0780
step 53900; step_loss: 0.1103
step 53910; step_loss: 0.1105
step 53920; step_loss: 0.1185
step 53930; step_loss: 0.0785
step 53940; step_loss: 0.0932
step 53950; step_loss: 0.0795
step 53960; step_loss: 0.0985
step 53970; step_loss: 0.0719
step 53980; step_loss: 0.0756
step 53990; step_loss: 0.0938

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 0.405 | 0.706 | 1.089 | 1.225 |   n/a |   n/a |

============================
Global step:         54000
Learning rate:       0.0039
Step-time (ms):     27.1115
Train loss avg:      0.0976
--------------------------
Val loss:            0.3586
srnn loss:           0.3475
============================

Saving the model...
done in 271.54 ms
step 54000; step_loss: 0.1401
step 54010; step_loss: 0.0785
step 54020; step_loss: 0.0904
step 54030; step_loss: 0.1045
step 54040; step_loss: 0.0731
step 54050; step_loss: 0.0854
step 54060; step_loss: 0.1070
step 54070; step_loss: 0.1454
step 54080; step_loss: 0.0747
step 54090; step_loss: 0.0850
step 54100; step_loss: 0.0797
step 54110; step_loss: 0.0942
step 54120; step_loss: 0.0752
step 54130; step_loss: 0.1160
step 54140; step_loss: 0.1646
step 54150; step_loss: 0.1008
step 54160; step_loss: 0.0911
step 54170; step_loss: 0.0824
step 54180; step_loss: 0.0808
step 54190; step_loss: 0.0776
step 54200; step_loss: 0.0665
step 54210; step_loss: 0.1151
step 54220; step_loss: 0.1429
step 54230; step_loss: 0.0606
step 54240; step_loss: 0.0801
step 54250; step_loss: 0.1975
step 54260; step_loss: 0.0887
step 54270; step_loss: 0.0747
step 54280; step_loss: 0.1127
step 54290; step_loss: 0.0865
step 54300; step_loss: 0.0918
step 54310; step_loss: 0.1360
step 54320; step_loss: 0.0982
step 54330; step_loss: 0.1502
step 54340; step_loss: 0.0666
step 54350; step_loss: 0.0701
step 54360; step_loss: 0.0766
step 54370; step_loss: 0.0723
step 54380; step_loss: 0.0773
step 54390; step_loss: 0.1693
step 54400; step_loss: 0.0670
step 54410; step_loss: 0.0661
step 54420; step_loss: 0.0799
step 54430; step_loss: 0.1115
step 54440; step_loss: 0.1182
step 54450; step_loss: 0.1752
step 54460; step_loss: 0.1265
step 54470; step_loss: 0.0791
step 54480; step_loss: 0.1299
step 54490; step_loss: 0.1119
step 54500; step_loss: 0.0834
step 54510; step_loss: 0.2122
step 54520; step_loss: 0.1240
step 54530; step_loss: 0.1488
step 54540; step_loss: 0.1013
step 54550; step_loss: 0.0869
step 54560; step_loss: 0.0913
step 54570; step_loss: 0.0743
step 54580; step_loss: 0.1258
step 54590; step_loss: 0.2360
step 54600; step_loss: 0.0981
step 54610; step_loss: 0.1778
step 54620; step_loss: 0.0724
step 54630; step_loss: 0.2198
step 54640; step_loss: 0.0566
step 54650; step_loss: 0.1404
step 54660; step_loss: 0.0959
step 54670; step_loss: 0.1196
step 54680; step_loss: 0.1471
step 54690; step_loss: 0.0663
step 54700; step_loss: 0.0851
step 54710; step_loss: 0.0827
step 54720; step_loss: 0.1296
step 54730; step_loss: 0.1005
step 54740; step_loss: 0.0775
step 54750; step_loss: 0.0835
step 54760; step_loss: 0.0771
step 54770; step_loss: 0.1219
step 54780; step_loss: 0.0842
step 54790; step_loss: 0.0885
step 54800; step_loss: 0.0747
step 54810; step_loss: 0.0765
step 54820; step_loss: 0.1940
step 54830; step_loss: 0.1141
step 54840; step_loss: 0.0978
step 54850; step_loss: 0.0801
step 54860; step_loss: 0.0866
step 54870; step_loss: 0.0890
step 54880; step_loss: 0.0807
step 54890; step_loss: 0.0769
step 54900; step_loss: 0.0931
step 54910; step_loss: 0.0814
step 54920; step_loss: 0.1044
step 54930; step_loss: 0.0880
step 54940; step_loss: 0.1106
step 54950; step_loss: 0.0916
step 54960; step_loss: 0.1214
step 54970; step_loss: 0.0732
step 54980; step_loss: 0.1050
step 54990; step_loss: 0.1073

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 0.402 | 0.700 | 1.063 | 1.188 |   n/a |   n/a |

============================
Global step:         55000
Learning rate:       0.0039
Step-time (ms):     27.0962
Train loss avg:      0.0975
--------------------------
Val loss:            0.4521
srnn loss:           0.3429
============================

Saving the model...
done in 272.31 ms
step 55000; step_loss: 0.0941
step 55010; step_loss: 0.0956
step 55020; step_loss: 0.1612
step 55030; step_loss: 0.0727
step 55040; step_loss: 0.1234
step 55050; step_loss: 0.0952
step 55060; step_loss: 0.0503
step 55070; step_loss: 0.0733
step 55080; step_loss: 0.1011
step 55090; step_loss: 0.0870
step 55100; step_loss: 0.0938
step 55110; step_loss: 0.0826
step 55120; step_loss: 0.0937
step 55130; step_loss: 0.0729
step 55140; step_loss: 0.0887
step 55150; step_loss: 0.1663
step 55160; step_loss: 0.0697
step 55170; step_loss: 0.1469
step 55180; step_loss: 0.0935
step 55190; step_loss: 0.0945
step 55200; step_loss: 0.0801
step 55210; step_loss: 0.1625
step 55220; step_loss: 0.0856
step 55230; step_loss: 0.0762
step 55240; step_loss: 0.0915
step 55250; step_loss: 0.0881
step 55260; step_loss: 0.0909
step 55270; step_loss: 0.1069
step 55280; step_loss: 0.1263
step 55290; step_loss: 0.0977
step 55300; step_loss: 0.0812
step 55310; step_loss: 0.1121
step 55320; step_loss: 0.0799
step 55330; step_loss: 0.0846
step 55340; step_loss: 0.0888
step 55350; step_loss: 0.2176
step 55360; step_loss: 0.0567
step 55370; step_loss: 0.1009
step 55380; step_loss: 0.1467
step 55390; step_loss: 0.0876
step 55400; step_loss: 0.0790
step 55410; step_loss: 0.1202
step 55420; step_loss: 0.0835
step 55430; step_loss: 0.1055
step 55440; step_loss: 0.0738
step 55450; step_loss: 0.0913
step 55460; step_loss: 0.0907
step 55470; step_loss: 0.0766
step 55480; step_loss: 0.0719
step 55490; step_loss: 0.1327
step 55500; step_loss: 0.0685
step 55510; step_loss: 0.0958
step 55520; step_loss: 0.0826
step 55530; step_loss: 0.0773
step 55540; step_loss: 0.0753
step 55550; step_loss: 0.0576
step 55560; step_loss: 0.0601
step 55570; step_loss: 0.0652
step 55580; step_loss: 0.0887
step 55590; step_loss: 0.0949
step 55600; step_loss: 0.0766
step 55610; step_loss: 0.0695
step 55620; step_loss: 0.0795
step 55630; step_loss: 0.0621
step 55640; step_loss: 0.0778
step 55650; step_loss: 0.0897
step 55660; step_loss: 0.1122
step 55670; step_loss: 0.1431
step 55680; step_loss: 0.0651
step 55690; step_loss: 0.0710
step 55700; step_loss: 0.1473
step 55710; step_loss: 0.0676
step 55720; step_loss: 0.0636
step 55730; step_loss: 0.0973
step 55740; step_loss: 0.0690
step 55750; step_loss: 0.2126
step 55760; step_loss: 0.0837
step 55770; step_loss: 0.0959
step 55780; step_loss: 0.1117
step 55790; step_loss: 0.0547
step 55800; step_loss: 0.0817
step 55810; step_loss: 0.0920
step 55820; step_loss: 0.0887
step 55830; step_loss: 0.0606
step 55840; step_loss: 0.1374
step 55850; step_loss: 0.0995
step 55860; step_loss: 0.0936
step 55870; step_loss: 0.0710
step 55880; step_loss: 0.0759
step 55890; step_loss: 0.0828
step 55900; step_loss: 0.0593
step 55910; step_loss: 0.1287
step 55920; step_loss: 0.0715
step 55930; step_loss: 0.1339
step 55940; step_loss: 0.1725
step 55950; step_loss: 0.0724
step 55960; step_loss: 0.0962
step 55970; step_loss: 0.0859
step 55980; step_loss: 0.1348
step 55990; step_loss: 0.1543

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 0.407 | 0.711 | 1.083 | 1.209 |   n/a |   n/a |

============================
Global step:         56000
Learning rate:       0.0039
Step-time (ms):     27.0741
Train loss avg:      0.0974
--------------------------
Val loss:            0.3768
srnn loss:           0.3486
============================

Saving the model...
done in 267.88 ms
step 56000; step_loss: 0.1298
step 56010; step_loss: 0.1045
step 56020; step_loss: 0.1193
step 56030; step_loss: 0.1043
step 56040; step_loss: 0.1213
step 56050; step_loss: 0.1394
step 56060; step_loss: 0.0701
step 56070; step_loss: 0.0843
step 56080; step_loss: 0.1324
step 56090; step_loss: 0.0771
step 56100; step_loss: 0.0716
step 56110; step_loss: 0.0855
step 56120; step_loss: 0.0876
step 56130; step_loss: 0.0957
step 56140; step_loss: 0.1080
step 56150; step_loss: 0.1208
step 56160; step_loss: 0.0828
step 56170; step_loss: 0.0828
step 56180; step_loss: 0.1280
step 56190; step_loss: 0.1189
step 56200; step_loss: 0.0756
step 56210; step_loss: 0.1057
step 56220; step_loss: 0.1198
step 56230; step_loss: 0.0927
step 56240; step_loss: 0.0729
step 56250; step_loss: 0.1129
step 56260; step_loss: 0.0806
step 56270; step_loss: 0.1085
step 56280; step_loss: 0.1178
step 56290; step_loss: 0.1107
step 56300; step_loss: 0.1439
step 56310; step_loss: 0.1156
step 56320; step_loss: 0.1022
step 56330; step_loss: 0.0725
step 56340; step_loss: 0.1127
step 56350; step_loss: 0.0889
step 56360; step_loss: 0.0827
step 56370; step_loss: 0.1089
step 56380; step_loss: 0.1061
step 56390; step_loss: 0.0875
step 56400; step_loss: 0.1128
step 56410; step_loss: 0.1179
step 56420; step_loss: 0.1185
step 56430; step_loss: 0.0821
step 56440; step_loss: 0.0561
step 56450; step_loss: 0.0929
step 56460; step_loss: 0.1114
step 56470; step_loss: 0.1074
step 56480; step_loss: 0.0923
step 56490; step_loss: 0.1280
step 56500; step_loss: 0.0724
step 56510; step_loss: 0.1336
step 56520; step_loss: 0.1034
step 56530; step_loss: 0.1048
step 56540; step_loss: 0.0652
step 56550; step_loss: 0.1750
step 56560; step_loss: 0.1215
step 56570; step_loss: 0.0831
step 56580; step_loss: 0.1610
step 56590; step_loss: 0.0779
step 56600; step_loss: 0.0739
step 56610; step_loss: 0.1104
step 56620; step_loss: 0.0839
step 56630; step_loss: 0.0614
step 56640; step_loss: 0.0672
step 56650; step_loss: 0.0990
step 56660; step_loss: 0.0886
step 56670; step_loss: 0.0757
step 56680; step_loss: 0.0678
step 56690; step_loss: 0.0934
step 56700; step_loss: 0.0936
step 56710; step_loss: 0.0741
step 56720; step_loss: 0.1070
step 56730; step_loss: 0.0643
step 56740; step_loss: 0.0640
step 56750; step_loss: 0.0972
step 56760; step_loss: 0.0634
step 56770; step_loss: 0.0995
step 56780; step_loss: 0.1209
step 56790; step_loss: 0.1175
step 56800; step_loss: 0.1242
step 56810; step_loss: 0.0740
step 56820; step_loss: 0.0726
step 56830; step_loss: 0.0896
step 56840; step_loss: 0.0872
step 56850; step_loss: 0.0634
step 56860; step_loss: 0.1192
step 56870; step_loss: 0.1222
step 56880; step_loss: 0.0965
step 56890; step_loss: 0.0896
step 56900; step_loss: 0.0905
step 56910; step_loss: 0.0746
step 56920; step_loss: 0.1043
step 56930; step_loss: 0.1066
step 56940; step_loss: 0.0714
step 56950; step_loss: 0.0808
step 56960; step_loss: 0.1106
step 56970; step_loss: 0.0971
step 56980; step_loss: 0.1063
step 56990; step_loss: 0.1097

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 0.408 | 0.710 | 1.089 | 1.223 |   n/a |   n/a |

============================
Global step:         57000
Learning rate:       0.0039
Step-time (ms):     27.1176
Train loss avg:      0.0964
--------------------------
Val loss:            0.3752
srnn loss:           0.3503
============================

Saving the model...
done in 278.31 ms
step 57000; step_loss: 0.1351
step 57010; step_loss: 0.0636
step 57020; step_loss: 0.0962
step 57030; step_loss: 0.0937
step 57040; step_loss: 0.1717
step 57050; step_loss: 0.1315
step 57060; step_loss: 0.0776
step 57070; step_loss: 0.0590
step 57080; step_loss: 0.0692
step 57090; step_loss: 0.0798
step 57100; step_loss: 0.1152
step 57110; step_loss: 0.0745
step 57120; step_loss: 0.0825
step 57130; step_loss: 0.0894
step 57140; step_loss: 0.1169
step 57150; step_loss: 0.0739
step 57160; step_loss: 0.0710
step 57170; step_loss: 0.1035
step 57180; step_loss: 0.0885
step 57190; step_loss: 0.1654
step 57200; step_loss: 0.1021
step 57210; step_loss: 0.0565
step 57220; step_loss: 0.0820
step 57230; step_loss: 0.1483
step 57240; step_loss: 0.1137
step 57250; step_loss: 0.0851
step 57260; step_loss: 0.1075
step 57270; step_loss: 0.1073
step 57280; step_loss: 0.0701
step 57290; step_loss: 0.0720
step 57300; step_loss: 0.0997
step 57310; step_loss: 0.0841
step 57320; step_loss: 0.0736
step 57330; step_loss: 0.1188
step 57340; step_loss: 0.0874
step 57350; step_loss: 0.0870
step 57360; step_loss: 0.1083
step 57370; step_loss: 0.0668
step 57380; step_loss: 0.0961
step 57390; step_loss: 0.1208
step 57400; step_loss: 0.0820
step 57410; step_loss: 0.0740
step 57420; step_loss: 0.0763
step 57430; step_loss: 0.0796
step 57440; step_loss: 0.0960
step 57450; step_loss: 0.1311
step 57460; step_loss: 0.0611
step 57470; step_loss: 0.1089
step 57480; step_loss: 0.1024
step 57490; step_loss: 0.0993
step 57500; step_loss: 0.0687
step 57510; step_loss: 0.1047
step 57520; step_loss: 0.0899
step 57530; step_loss: 0.0982
step 57540; step_loss: 0.1272
step 57550; step_loss: 0.0947
step 57560; step_loss: 0.0922
step 57570; step_loss: 0.0506
step 57580; step_loss: 0.1436
step 57590; step_loss: 0.0725
step 57600; step_loss: 0.0710
step 57610; step_loss: 0.0850
step 57620; step_loss: 0.0805
step 57630; step_loss: 0.0770
step 57640; step_loss: 0.0982
step 57650; step_loss: 0.0542
step 57660; step_loss: 0.0909
step 57670; step_loss: 0.0726
step 57680; step_loss: 0.0997
step 57690; step_loss: 0.1064
step 57700; step_loss: 0.0786
step 57710; step_loss: 0.1515
step 57720; step_loss: 0.0889
step 57730; step_loss: 0.0847
step 57740; step_loss: 0.1217
step 57750; step_loss: 0.0963
step 57760; step_loss: 0.1175
step 57770; step_loss: 0.0935
step 57780; step_loss: 0.1080
step 57790; step_loss: 0.0784
step 57800; step_loss: 0.0607
step 57810; step_loss: 0.0654
step 57820; step_loss: 0.1344
step 57830; step_loss: 0.0953
step 57840; step_loss: 0.1181
step 57850; step_loss: 0.0859
step 57860; step_loss: 0.0857
step 57870; step_loss: 0.1779
step 57880; step_loss: 0.1991
step 57890; step_loss: 0.1315
step 57900; step_loss: 0.0783
step 57910; step_loss: 0.0733
step 57920; step_loss: 0.1168
step 57930; step_loss: 0.1011
step 57940; step_loss: 0.0855
step 57950; step_loss: 0.0566
step 57960; step_loss: 0.0561
step 57970; step_loss: 0.0966
step 57980; step_loss: 0.0878
step 57990; step_loss: 0.1093

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 0.410 | 0.715 | 1.097 | 1.229 |   n/a |   n/a |

============================
Global step:         58000
Learning rate:       0.0039
Step-time (ms):     27.0890
Train loss avg:      0.0959
--------------------------
Val loss:            0.3817
srnn loss:           0.3467
============================

Saving the model...
done in 291.00 ms
step 58000; step_loss: 0.1553
step 58010; step_loss: 0.0605
step 58020; step_loss: 0.0652
step 58030; step_loss: 0.1063
step 58040; step_loss: 0.0924
step 58050; step_loss: 0.0787
step 58060; step_loss: 0.1478
step 58070; step_loss: 0.0731
step 58080; step_loss: 0.1344
step 58090; step_loss: 0.0869
step 58100; step_loss: 0.0970
step 58110; step_loss: 0.0964
step 58120; step_loss: 0.0906
step 58130; step_loss: 0.1101
step 58140; step_loss: 0.0775
step 58150; step_loss: 0.1131
step 58160; step_loss: 0.1120
step 58170; step_loss: 0.1098
step 58180; step_loss: 0.0697
step 58190; step_loss: 0.1019
step 58200; step_loss: 0.0630
step 58210; step_loss: 0.0928
step 58220; step_loss: 0.0981
step 58230; step_loss: 0.0745
step 58240; step_loss: 0.0853
step 58250; step_loss: 0.0855
step 58260; step_loss: 0.0995
step 58270; step_loss: 0.0749
step 58280; step_loss: 0.1371
step 58290; step_loss: 0.0910
step 58300; step_loss: 0.0663
step 58310; step_loss: 0.0782
step 58320; step_loss: 0.0746
step 58330; step_loss: 0.0875
step 58340; step_loss: 0.0858
step 58350; step_loss: 0.0678
step 58360; step_loss: 0.0995
step 58370; step_loss: 0.0859
step 58380; step_loss: 0.0745
step 58390; step_loss: 0.0836
step 58400; step_loss: 0.0693
step 58410; step_loss: 0.0572
step 58420; step_loss: 0.1020
step 58430; step_loss: 0.0829
step 58440; step_loss: 0.0716
step 58450; step_loss: 0.0828
step 58460; step_loss: 0.0895
step 58470; step_loss: 0.0867
step 58480; step_loss: 0.0629
step 58490; step_loss: 0.0933
step 58500; step_loss: 0.0817
step 58510; step_loss: 0.0999
step 58520; step_loss: 0.0659
step 58530; step_loss: 0.0707
step 58540; step_loss: 0.1171
step 58550; step_loss: 0.0689
step 58560; step_loss: 0.0844
step 58570; step_loss: 0.0905
step 58580; step_loss: 0.0781
step 58590; step_loss: 0.1439
step 58600; step_loss: 0.0823
step 58610; step_loss: 0.1097
step 58620; step_loss: 0.0704
step 58630; step_loss: 0.0789
step 58640; step_loss: 0.0583
step 58650; step_loss: 0.1392
step 58660; step_loss: 0.0688
step 58670; step_loss: 0.0531
step 58680; step_loss: 0.0813
step 58690; step_loss: 0.0648
step 58700; step_loss: 0.0842
step 58710; step_loss: 0.1015
step 58720; step_loss: 0.0912
step 58730; step_loss: 0.0769
step 58740; step_loss: 0.0793
step 58750; step_loss: 0.0849
step 58760; step_loss: 0.0976
step 58770; step_loss: 0.0943
step 58780; step_loss: 0.0663
step 58790; step_loss: 0.0591
step 58800; step_loss: 0.0952
step 58810; step_loss: 0.0934
step 58820; step_loss: 0.0756
step 58830; step_loss: 0.0691
step 58840; step_loss: 0.0966
step 58850; step_loss: 0.0948
step 58860; step_loss: 0.0955
step 58870; step_loss: 0.1006
step 58880; step_loss: 0.0615
step 58890; step_loss: 0.1559
step 58900; step_loss: 0.0726
step 58910; step_loss: 0.0704
step 58920; step_loss: 0.0827
step 58930; step_loss: 0.1081
step 58940; step_loss: 0.0855
step 58950; step_loss: 0.1079
step 58960; step_loss: 0.0895
step 58970; step_loss: 0.0657
step 58980; step_loss: 0.0849
step 58990; step_loss: 0.0924

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 0.404 | 0.704 | 1.078 | 1.208 |   n/a |   n/a |

============================
Global step:         59000
Learning rate:       0.0039
Step-time (ms):     27.1214
Train loss avg:      0.0942
--------------------------
Val loss:            0.3950
srnn loss:           0.3531
============================

Saving the model...
done in 271.02 ms
step 59000; step_loss: 0.2345
step 59010; step_loss: 0.1051
step 59020; step_loss: 0.0908
step 59030; step_loss: 0.1156
step 59040; step_loss: 0.0626
step 59050; step_loss: 0.1640
step 59060; step_loss: 0.0811
step 59070; step_loss: 0.1073
step 59080; step_loss: 0.1229
step 59090; step_loss: 0.0679
step 59100; step_loss: 0.0823
step 59110; step_loss: 0.0767
step 59120; step_loss: 0.0771
step 59130; step_loss: 0.0798
step 59140; step_loss: 0.0929
step 59150; step_loss: 0.0729
step 59160; step_loss: 0.1917
step 59170; step_loss: 0.0757
step 59180; step_loss: 0.0857
step 59190; step_loss: 0.1145
step 59200; step_loss: 0.0665
step 59210; step_loss: 0.0627
step 59220; step_loss: 0.0954
step 59230; step_loss: 0.1114
step 59240; step_loss: 0.0722
step 59250; step_loss: 0.0760
step 59260; step_loss: 0.0992
step 59270; step_loss: 0.0572
step 59280; step_loss: 0.0917
step 59290; step_loss: 0.0660
step 59300; step_loss: 0.0542
step 59310; step_loss: 0.1094
step 59320; step_loss: 0.0733
step 59330; step_loss: 0.0892
step 59340; step_loss: 0.0761
step 59350; step_loss: 0.0794
step 59360; step_loss: 0.0855
step 59370; step_loss: 0.0766
step 59380; step_loss: 0.0946
step 59390; step_loss: 0.1348
step 59400; step_loss: 0.0833
step 59410; step_loss: 0.1405
step 59420; step_loss: 0.0771
step 59430; step_loss: 0.1216
step 59440; step_loss: 0.1194
step 59450; step_loss: 0.1021
step 59460; step_loss: 0.0697
step 59470; step_loss: 0.1636
step 59480; step_loss: 0.0635
step 59490; step_loss: 0.1121
step 59500; step_loss: 0.0921
step 59510; step_loss: 0.1232
step 59520; step_loss: 0.0935
step 59530; step_loss: 0.0729
step 59540; step_loss: 0.0685
step 59550; step_loss: 0.0779
step 59560; step_loss: 0.0882
step 59570; step_loss: 0.1213
step 59580; step_loss: 0.0694
step 59590; step_loss: 0.1824
step 59600; step_loss: 0.1033
step 59610; step_loss: 0.0951
step 59620; step_loss: 0.1043
step 59630; step_loss: 0.0919
step 59640; step_loss: 0.1302
step 59650; step_loss: 0.2623
step 59660; step_loss: 0.1033
step 59670; step_loss: 0.2347
step 59680; step_loss: 0.0915
step 59690; step_loss: 0.0677
step 59700; step_loss: 0.1408
step 59710; step_loss: 0.1690
step 59720; step_loss: 0.0750
step 59730; step_loss: 0.0660
step 59740; step_loss: 0.0616
step 59750; step_loss: 0.0890
step 59760; step_loss: 0.0929
step 59770; step_loss: 0.1038
step 59780; step_loss: 0.0613
step 59790; step_loss: 0.0939
step 59800; step_loss: 0.1252
step 59810; step_loss: 0.0683
step 59820; step_loss: 0.1315
step 59830; step_loss: 0.0805
step 59840; step_loss: 0.0879
step 59850; step_loss: 0.0804
step 59860; step_loss: 0.0926
step 59870; step_loss: 0.1678
step 59880; step_loss: 0.1325
step 59890; step_loss: 0.0700
step 59900; step_loss: 0.0995
step 59910; step_loss: 0.0772
step 59920; step_loss: 0.0546
step 59930; step_loss: 0.0849
step 59940; step_loss: 0.0691
step 59950; step_loss: 0.0690
step 59960; step_loss: 0.0954
step 59970; step_loss: 0.0803
step 59980; step_loss: 0.0644
step 59990; step_loss: 0.0834

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 0.411 | 0.717 | 1.112 | 1.258 |   n/a |   n/a |

============================
Global step:         60000
Learning rate:       0.0037
Step-time (ms):     27.1217
Train loss avg:      0.0947
--------------------------
Val loss:            0.4114
srnn loss:           0.3572
============================

Saving the model...
done in 262.77 ms
step 60000; step_loss: 0.0835
step 60010; step_loss: 0.0661
step 60020; step_loss: 0.0706
step 60030; step_loss: 0.1110
step 60040; step_loss: 0.1855
step 60050; step_loss: 0.0605
step 60060; step_loss: 0.1202
step 60070; step_loss: 0.0606
step 60080; step_loss: 0.0911
step 60090; step_loss: 0.1044
step 60100; step_loss: 0.0827
step 60110; step_loss: 0.1079
step 60120; step_loss: 0.0746
step 60130; step_loss: 0.1430
step 60140; step_loss: 0.0731
step 60150; step_loss: 0.1208
step 60160; step_loss: 0.1449
step 60170; step_loss: 0.1389
step 60180; step_loss: 0.0944
step 60190; step_loss: 0.0683
step 60200; step_loss: 0.0651
step 60210; step_loss: 0.0691
step 60220; step_loss: 0.0663
step 60230; step_loss: 0.1335
step 60240; step_loss: 0.0637
step 60250; step_loss: 0.1062
step 60260; step_loss: 0.1031
step 60270; step_loss: 0.0951
step 60280; step_loss: 0.0805
step 60290; step_loss: 0.0825
step 60300; step_loss: 0.1043
step 60310; step_loss: 0.0583
step 60320; step_loss: 0.0733
step 60330; step_loss: 0.0726
step 60340; step_loss: 0.0766
step 60350; step_loss: 0.2104
step 60360; step_loss: 0.0901
step 60370; step_loss: 0.0757
step 60380; step_loss: 0.0867
step 60390; step_loss: 0.1268
step 60400; step_loss: 0.0723
step 60410; step_loss: 0.0933
step 60420; step_loss: 0.1125
step 60430; step_loss: 0.0682
step 60440; step_loss: 0.0982
step 60450; step_loss: 0.0875
step 60460; step_loss: 0.1057
step 60470; step_loss: 0.1460
step 60480; step_loss: 0.1129
step 60490; step_loss: 0.0633
step 60500; step_loss: 0.0608
step 60510; step_loss: 0.0767
step 60520; step_loss: 0.1291
step 60530; step_loss: 0.0683
step 60540; step_loss: 0.0596
step 60550; step_loss: 0.0668
step 60560; step_loss: 0.0882
step 60570; step_loss: 0.1114
step 60580; step_loss: 0.0737
step 60590; step_loss: 0.0789
step 60600; step_loss: 0.0676
step 60610; step_loss: 0.0573
step 60620; step_loss: 0.1234
step 60630; step_loss: 0.0708
step 60640; step_loss: 0.1336
step 60650; step_loss: 0.0860
step 60660; step_loss: 0.1209
step 60670; step_loss: 0.0706
step 60680; step_loss: 0.0946
step 60690; step_loss: 0.0761
step 60700; step_loss: 0.1041
step 60710; step_loss: 0.0600
step 60720; step_loss: 0.0715
step 60730; step_loss: 0.0679
step 60740; step_loss: 0.1011
step 60750; step_loss: 0.0797
step 60760; step_loss: 0.0612
step 60770; step_loss: 0.1121
step 60780; step_loss: 0.0713
step 60790; step_loss: 0.1028
step 60800; step_loss: 0.0635
step 60810; step_loss: 0.0763
step 60820; step_loss: 0.1028
step 60830; step_loss: 0.0973
step 60840; step_loss: 0.0531
step 60850; step_loss: 0.0700
step 60860; step_loss: 0.0839
step 60870; step_loss: 0.0812
step 60880; step_loss: 0.0999
step 60890; step_loss: 0.1247
step 60900; step_loss: 0.0678
step 60910; step_loss: 0.0913
step 60920; step_loss: 0.0926
step 60930; step_loss: 0.0915
step 60940; step_loss: 0.0873
step 60950; step_loss: 0.0769
step 60960; step_loss: 0.1178
step 60970; step_loss: 0.0972
step 60980; step_loss: 0.1245
step 60990; step_loss: 0.0624

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 0.414 | 0.721 | 1.108 | 1.244 |   n/a |   n/a |

============================
Global step:         61000
Learning rate:       0.0037
Step-time (ms):     27.1829
Train loss avg:      0.0929
--------------------------
Val loss:            0.4085
srnn loss:           0.3533
============================

Saving the model...
done in 270.23 ms
step 61000; step_loss: 0.0773
step 61010; step_loss: 0.0891
step 61020; step_loss: 0.0697
step 61030; step_loss: 0.0858
step 61040; step_loss: 0.1895
step 61050; step_loss: 0.0920
step 61060; step_loss: 0.1048
step 61070; step_loss: 0.0868
step 61080; step_loss: 0.1060
step 61090; step_loss: 0.1323
step 61100; step_loss: 0.0617
step 61110; step_loss: 0.0957
step 61120; step_loss: 0.0814
step 61130; step_loss: 0.0677
step 61140; step_loss: 0.1849
step 61150; step_loss: 0.1176
step 61160; step_loss: 0.1920
step 61170; step_loss: 0.0717
step 61180; step_loss: 0.1015
step 61190; step_loss: 0.0845
step 61200; step_loss: 0.1012
step 61210; step_loss: 0.0773
step 61220; step_loss: 0.0916
step 61230; step_loss: 0.1306
step 61240; step_loss: 0.2649
step 61250; step_loss: 0.0981
step 61260; step_loss: 0.0744
step 61270; step_loss: 0.1103
step 61280; step_loss: 0.0811
step 61290; step_loss: 0.0811
step 61300; step_loss: 0.1831
step 61310; step_loss: 0.0912
step 61320; step_loss: 0.0818
step 61330; step_loss: 0.1650
step 61340; step_loss: 0.0985
step 61350; step_loss: 0.1016
step 61360; step_loss: 0.0619
step 61370; step_loss: 0.0613
step 61380; step_loss: 0.1053
step 61390; step_loss: 0.0812
step 61400; step_loss: 0.1054
step 61410; step_loss: 0.1424
step 61420; step_loss: 0.1788
step 61430; step_loss: 0.0772
step 61440; step_loss: 0.0952
step 61450; step_loss: 0.1026
step 61460; step_loss: 0.1106
step 61470; step_loss: 0.1362
step 61480; step_loss: 0.0917
step 61490; step_loss: 0.0784
step 61500; step_loss: 0.0992
step 61510; step_loss: 0.0742
step 61520; step_loss: 0.0911
step 61530; step_loss: 0.1525
step 61540; step_loss: 0.0870
step 61550; step_loss: 0.1069
step 61560; step_loss: 0.0771
step 61570; step_loss: 0.1068
step 61580; step_loss: 0.0846
step 61590; step_loss: 0.0845
step 61600; step_loss: 0.0969
step 61610; step_loss: 0.0713
step 61620; step_loss: 0.0884
step 61630; step_loss: 0.1091
step 61640; step_loss: 0.1649
step 61650; step_loss: 0.0864
step 61660; step_loss: 0.0723
step 61670; step_loss: 0.0784
step 61680; step_loss: 0.0934
step 61690; step_loss: 0.1219
step 61700; step_loss: 0.0768
step 61710; step_loss: 0.0867
step 61720; step_loss: 0.0905
step 61730; step_loss: 0.0934
step 61740; step_loss: 0.0728
step 61750; step_loss: 0.1074
step 61760; step_loss: 0.0760
step 61770; step_loss: 0.0949
step 61780; step_loss: 0.0778
step 61790; step_loss: 0.0815
step 61800; step_loss: 0.0695
step 61810; step_loss: 0.1372
step 61820; step_loss: 0.0714
step 61830; step_loss: 0.0625
step 61840; step_loss: 0.0803
step 61850; step_loss: 0.0902
step 61860; step_loss: 0.0866
step 61870; step_loss: 0.0803
step 61880; step_loss: 0.1005
step 61890; step_loss: 0.0855
step 61900; step_loss: 0.0742
step 61910; step_loss: 0.1046
step 61920; step_loss: 0.0674
step 61930; step_loss: 0.1271
step 61940; step_loss: 0.0744
step 61950; step_loss: 0.0898
step 61960; step_loss: 0.0684
step 61970; step_loss: 0.1029
step 61980; step_loss: 0.1362
step 61990; step_loss: 0.0810

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 0.409 | 0.711 | 1.076 | 1.201 |   n/a |   n/a |

============================
Global step:         62000
Learning rate:       0.0037
Step-time (ms):     27.0995
Train loss avg:      0.0932
--------------------------
Val loss:            0.4261
srnn loss:           0.3527
============================

Saving the model...
done in 268.33 ms
step 62000; step_loss: 0.0885
step 62010; step_loss: 0.0755
step 62020; step_loss: 0.0821
step 62030; step_loss: 0.1118
step 62040; step_loss: 0.1163
step 62050; step_loss: 0.0901
step 62060; step_loss: 0.0926
step 62070; step_loss: 0.0741
step 62080; step_loss: 0.0806
step 62090; step_loss: 0.0895
step 62100; step_loss: 0.0556
step 62110; step_loss: 0.0897
step 62120; step_loss: 0.1026
step 62130; step_loss: 0.0890
step 62140; step_loss: 0.0545
step 62150; step_loss: 0.1347
step 62160; step_loss: 0.0753
step 62170; step_loss: 0.1230
step 62180; step_loss: 0.1030
step 62190; step_loss: 0.0645
step 62200; step_loss: 0.1323
step 62210; step_loss: 0.1139
step 62220; step_loss: 0.1032
step 62230; step_loss: 0.0621
step 62240; step_loss: 0.1078
step 62250; step_loss: 0.1319
step 62260; step_loss: 0.0929
step 62270; step_loss: 0.1141
step 62280; step_loss: 0.0617
step 62290; step_loss: 0.1115
step 62300; step_loss: 0.1496
step 62310; step_loss: 0.1191
step 62320; step_loss: 0.1075
step 62330; step_loss: 0.1471
step 62340; step_loss: 0.0637
step 62350; step_loss: 0.1087
step 62360; step_loss: 0.0920
step 62370; step_loss: 0.1026
step 62380; step_loss: 0.0816
step 62390; step_loss: 0.1186
step 62400; step_loss: 0.0741
step 62410; step_loss: 0.0682
step 62420; step_loss: 0.1363
step 62430; step_loss: 0.0692
step 62440; step_loss: 0.1371
step 62450; step_loss: 0.0973
step 62460; step_loss: 0.0775
step 62470; step_loss: 0.1135
step 62480; step_loss: 0.1069
step 62490; step_loss: 0.0882
step 62500; step_loss: 0.0822
step 62510; step_loss: 0.0974
step 62520; step_loss: 0.0791
step 62530; step_loss: 0.0886
step 62540; step_loss: 0.1075
step 62550; step_loss: 0.0927
step 62560; step_loss: 0.0871
step 62570; step_loss: 0.0880
step 62580; step_loss: 0.0918
step 62590; step_loss: 0.0904
step 62600; step_loss: 0.0641
step 62610; step_loss: 0.1077
step 62620; step_loss: 0.0640
step 62630; step_loss: 0.0957
step 62640; step_loss: 0.1443
step 62650; step_loss: 0.1270
step 62660; step_loss: 0.0835
step 62670; step_loss: 0.0758
step 62680; step_loss: 0.1112
step 62690; step_loss: 0.0742
step 62700; step_loss: 0.0812
step 62710; step_loss: 0.0736
step 62720; step_loss: 0.0777
step 62730; step_loss: 0.0758
step 62740; step_loss: 0.0830
step 62750; step_loss: 0.0781
step 62760; step_loss: 0.0826
step 62770; step_loss: 0.0702
step 62780; step_loss: 0.1278
step 62790; step_loss: 0.1110
step 62800; step_loss: 0.0620
step 62810; step_loss: 0.1014
step 62820; step_loss: 0.0673
step 62830; step_loss: 0.0972
step 62840; step_loss: 0.0674
step 62850; step_loss: 0.0719
step 62860; step_loss: 0.0825
step 62870; step_loss: 0.0911
step 62880; step_loss: 0.1042
step 62890; step_loss: 0.0763
step 62900; step_loss: 0.1093
step 62910; step_loss: 0.0907
step 62920; step_loss: 0.0693
step 62930; step_loss: 0.1092
step 62940; step_loss: 0.0857
step 62950; step_loss: 0.0923
step 62960; step_loss: 0.0671
step 62970; step_loss: 0.0989
step 62980; step_loss: 0.0866
step 62990; step_loss: 0.0949

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 0.406 | 0.705 | 1.064 | 1.187 |   n/a |   n/a |

============================
Global step:         63000
Learning rate:       0.0037
Step-time (ms):     27.1118
Train loss avg:      0.0930
--------------------------
Val loss:            0.3852
srnn loss:           0.3512
============================

Saving the model...
done in 266.36 ms
step 63000; step_loss: 0.0936
step 63010; step_loss: 0.0627
step 63020; step_loss: 0.2242
step 63030; step_loss: 0.0845
step 63040; step_loss: 0.1270
step 63050; step_loss: 0.0675
step 63060; step_loss: 0.0761
step 63070; step_loss: 0.1005
step 63080; step_loss: 0.0717
step 63090; step_loss: 0.1185
step 63100; step_loss: 0.0903
step 63110; step_loss: 0.0715
step 63120; step_loss: 0.0704
step 63130; step_loss: 0.1320
step 63140; step_loss: 0.0809
step 63150; step_loss: 0.0615
step 63160; step_loss: 0.0835
step 63170; step_loss: 0.1659
step 63180; step_loss: 0.1180
step 63190; step_loss: 0.0945
step 63200; step_loss: 0.1062
step 63210; step_loss: 0.1172
step 63220; step_loss: 0.1121
step 63230; step_loss: 0.1046
step 63240; step_loss: 0.0727
step 63250; step_loss: 0.0945
step 63260; step_loss: 0.0967
step 63270; step_loss: 0.1169
step 63280; step_loss: 0.0708
step 63290; step_loss: 0.0890
step 63300; step_loss: 0.1444
step 63310; step_loss: 0.0801
step 63320; step_loss: 0.0954
step 63330; step_loss: 0.0811
step 63340; step_loss: 0.0825
step 63350; step_loss: 0.0667
step 63360; step_loss: 0.0727
step 63370; step_loss: 0.0684
step 63380; step_loss: 0.0623
step 63390; step_loss: 0.2013
step 63400; step_loss: 0.1022
step 63410; step_loss: 0.0672
step 63420; step_loss: 0.1211
step 63430; step_loss: 0.0789
step 63440; step_loss: 0.0733
step 63450; step_loss: 0.0806
step 63460; step_loss: 0.1326
step 63470; step_loss: 0.1945
step 63480; step_loss: 0.0844
step 63490; step_loss: 0.1185
step 63500; step_loss: 0.0905
step 63510; step_loss: 0.0726
step 63520; step_loss: 0.0723
step 63530; step_loss: 0.1066
step 63540; step_loss: 0.1168
step 63550; step_loss: 0.0757
step 63560; step_loss: 0.1184
step 63570; step_loss: 0.1060
step 63580; step_loss: 0.0848
step 63590; step_loss: 0.0750
step 63600; step_loss: 0.0613
step 63610; step_loss: 0.1252
step 63620; step_loss: 0.0735
step 63630; step_loss: 0.1121
step 63640; step_loss: 0.0611
step 63650; step_loss: 0.0994
step 63660; step_loss: 0.0735
step 63670; step_loss: 0.1065
step 63680; step_loss: 0.0758
step 63690; step_loss: 0.0787
step 63700; step_loss: 0.0852
step 63710; step_loss: 0.0951
step 63720; step_loss: 0.0665
step 63730; step_loss: 0.0907
step 63740; step_loss: 0.1169
step 63750; step_loss: 0.1144
step 63760; step_loss: 0.0890
step 63770; step_loss: 0.0715
step 63780; step_loss: 0.0696
step 63790; step_loss: 0.0896
step 63800; step_loss: 0.1178
step 63810; step_loss: 0.0831
step 63820; step_loss: 0.0812
step 63830; step_loss: 0.0809
step 63840; step_loss: 0.0729
step 63850; step_loss: 0.0932
step 63860; step_loss: 0.0927
step 63870; step_loss: 0.1333
step 63880; step_loss: 0.1223
step 63890; step_loss: 0.0819
step 63900; step_loss: 0.0655
step 63910; step_loss: 0.1150
step 63920; step_loss: 0.0798
step 63930; step_loss: 0.0570
step 63940; step_loss: 0.1083
step 63950; step_loss: 0.1032
step 63960; step_loss: 0.1007
step 63970; step_loss: 0.1087
step 63980; step_loss: 0.0673
step 63990; step_loss: 0.0866

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 0.413 | 0.718 | 1.100 | 1.237 |   n/a |   n/a |

============================
Global step:         64000
Learning rate:       0.0037
Step-time (ms):     27.0862
Train loss avg:      0.0943
--------------------------
Val loss:            0.4218
srnn loss:           0.3560
============================

Saving the model...
done in 267.86 ms
step 64000; step_loss: 0.0720
step 64010; step_loss: 0.1204
step 64020; step_loss: 0.0759
step 64030; step_loss: 0.0710
step 64040; step_loss: 0.0652
step 64050; step_loss: 0.1129
step 64060; step_loss: 0.0907
step 64070; step_loss: 0.0821
step 64080; step_loss: 0.0772
step 64090; step_loss: 0.0734
step 64100; step_loss: 0.0784
step 64110; step_loss: 0.0622
step 64120; step_loss: 0.1018
step 64130; step_loss: 0.1756
step 64140; step_loss: 0.1027
step 64150; step_loss: 0.1669
step 64160; step_loss: 0.0753
step 64170; step_loss: 0.1432
step 64180; step_loss: 0.1003
step 64190; step_loss: 0.0748
step 64200; step_loss: 0.0626
step 64210; step_loss: 0.0820
step 64220; step_loss: 0.0745
step 64230; step_loss: 0.0700
step 64240; step_loss: 0.1385
step 64250; step_loss: 0.0588
step 64260; step_loss: 0.0787
step 64270; step_loss: 0.0683
step 64280; step_loss: 0.0849
step 64290; step_loss: 0.0926
step 64300; step_loss: 0.0710
step 64310; step_loss: 0.0742
step 64320; step_loss: 0.1307
step 64330; step_loss: 0.0930
step 64340; step_loss: 0.0491
step 64350; step_loss: 0.1039
step 64360; step_loss: 0.0910
step 64370; step_loss: 0.0689
step 64380; step_loss: 0.1204
step 64390; step_loss: 0.0921
step 64400; step_loss: 0.1586
step 64410; step_loss: 0.0928
step 64420; step_loss: 0.0980
step 64430; step_loss: 0.1003
step 64440; step_loss: 0.1143
step 64450; step_loss: 0.0655
step 64460; step_loss: 0.0817
step 64470; step_loss: 0.0977
step 64480; step_loss: 0.1336
step 64490; step_loss: 0.0848
step 64500; step_loss: 0.0605
step 64510; step_loss: 0.0848
step 64520; step_loss: 0.1164
step 64530; step_loss: 0.1001
step 64540; step_loss: 0.0633
step 64550; step_loss: 0.0822
step 64560; step_loss: 0.1029
step 64570; step_loss: 0.0907
step 64580; step_loss: 0.0975
step 64590; step_loss: 0.0618
step 64600; step_loss: 0.1061
step 64610; step_loss: 0.0792
step 64620; step_loss: 0.0884
step 64630; step_loss: 0.1398
step 64640; step_loss: 0.0612
step 64650; step_loss: 0.0957
step 64660; step_loss: 0.1111
step 64670; step_loss: 0.0937
step 64680; step_loss: 0.1021
step 64690; step_loss: 0.0879
step 64700; step_loss: 0.0952
step 64710; step_loss: 0.0529
step 64720; step_loss: 0.0781
step 64730; step_loss: 0.1198
step 64740; step_loss: 0.1035
step 64750; step_loss: 0.0837
step 64760; step_loss: 0.0975
step 64770; step_loss: 0.0723
step 64780; step_loss: 0.0979
step 64790; step_loss: 0.0981
step 64800; step_loss: 0.0878
step 64810; step_loss: 0.1147
step 64820; step_loss: 0.0932
step 64830; step_loss: 0.0741
step 64840; step_loss: 0.0714
step 64850; step_loss: 0.1049
step 64860; step_loss: 0.0696
step 64870; step_loss: 0.0978
step 64880; step_loss: 0.0899
step 64890; step_loss: 0.0991
step 64900; step_loss: 0.0799
step 64910; step_loss: 0.0780
step 64920; step_loss: 0.0687
step 64930; step_loss: 0.1158
step 64940; step_loss: 0.0943
step 64950; step_loss: 0.0608
step 64960; step_loss: 0.0715
step 64970; step_loss: 0.0736
step 64980; step_loss: 0.0628
step 64990; step_loss: 0.0893

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 0.412 | 0.718 | 1.096 | 1.228 |   n/a |   n/a |

============================
Global step:         65000
Learning rate:       0.0037
Step-time (ms):     27.0968
Train loss avg:      0.0924
--------------------------
Val loss:            0.4168
srnn loss:           0.3565
============================

Saving the model...
done in 263.31 ms
step 65000; step_loss: 0.1174
step 65010; step_loss: 0.1439
step 65020; step_loss: 0.0737
step 65030; step_loss: 0.0753
step 65040; step_loss: 0.0935
step 65050; step_loss: 0.0588
step 65060; step_loss: 0.0689
step 65070; step_loss: 0.0651
step 65080; step_loss: 0.0743
step 65090; step_loss: 0.0872
step 65100; step_loss: 0.0892
step 65110; step_loss: 0.0778
step 65120; step_loss: 0.0941
step 65130; step_loss: 0.0840
step 65140; step_loss: 0.0655
step 65150; step_loss: 0.0722
step 65160; step_loss: 0.1340
step 65170; step_loss: 0.1397
step 65180; step_loss: 0.1119
step 65190; step_loss: 0.0948
step 65200; step_loss: 0.0596
step 65210; step_loss: 0.0775
step 65220; step_loss: 0.1006
step 65230; step_loss: 0.1675
step 65240; step_loss: 0.0900
step 65250; step_loss: 0.0552
step 65260; step_loss: 0.1009
step 65270; step_loss: 0.0787
step 65280; step_loss: 0.0797
step 65290; step_loss: 0.0746
step 65300; step_loss: 0.0695
step 65310; step_loss: 0.1040
step 65320; step_loss: 0.0764
step 65330; step_loss: 0.0850
step 65340; step_loss: 0.0928
step 65350; step_loss: 0.1383
step 65360; step_loss: 0.0901
step 65370; step_loss: 0.0708
step 65380; step_loss: 0.0677
step 65390; step_loss: 0.0908
step 65400; step_loss: 0.0810
step 65410; step_loss: 0.0882
step 65420; step_loss: 0.0915
step 65430; step_loss: 0.0602
step 65440; step_loss: 0.1030
step 65450; step_loss: 0.1440
step 65460; step_loss: 0.0720
step 65470; step_loss: 0.0853
step 65480; step_loss: 0.1198
step 65490; step_loss: 0.1025
step 65500; step_loss: 0.0880
step 65510; step_loss: 0.0891
step 65520; step_loss: 0.0695
step 65530; step_loss: 0.0866
step 65540; step_loss: 0.0731
step 65550; step_loss: 0.0731
step 65560; step_loss: 0.1083
step 65570; step_loss: 0.1948
step 65580; step_loss: 0.0828
step 65590; step_loss: 0.0850
step 65600; step_loss: 0.0812
step 65610; step_loss: 0.1125
step 65620; step_loss: 0.1254
step 65630; step_loss: 0.1126
step 65640; step_loss: 0.1439
step 65650; step_loss: 0.0831
step 65660; step_loss: 0.1392
step 65670; step_loss: 0.0666
step 65680; step_loss: 0.0758
step 65690; step_loss: 0.0887
step 65700; step_loss: 0.1032
step 65710; step_loss: 0.1132
step 65720; step_loss: 0.1036
step 65730; step_loss: 0.1350
step 65740; step_loss: 0.0685
step 65750; step_loss: 0.0590
step 65760; step_loss: 0.0691
step 65770; step_loss: 0.0760
step 65780; step_loss: 0.0856
step 65790; step_loss: 0.0793
step 65800; step_loss: 0.0684
step 65810; step_loss: 0.0881
step 65820; step_loss: 0.0523
step 65830; step_loss: 0.1165
step 65840; step_loss: 0.0894
step 65850; step_loss: 0.0577
step 65860; step_loss: 0.0806
step 65870; step_loss: 0.0718
step 65880; step_loss: 0.0909
step 65890; step_loss: 0.1176
step 65900; step_loss: 0.0803
step 65910; step_loss: 0.0796
step 65920; step_loss: 0.0668
step 65930; step_loss: 0.0600
step 65940; step_loss: 0.0813
step 65950; step_loss: 0.1219
step 65960; step_loss: 0.1012
step 65970; step_loss: 0.0799
step 65980; step_loss: 0.1240
step 65990; step_loss: 0.0922

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 0.415 | 0.723 | 1.107 | 1.244 |   n/a |   n/a |

============================
Global step:         66000
Learning rate:       0.0037
Step-time (ms):     27.1323
Train loss avg:      0.0931
--------------------------
Val loss:            0.4235
srnn loss:           0.3654
============================

Saving the model...
done in 272.96 ms
step 66000; step_loss: 0.1386
step 66010; step_loss: 0.0747
step 66020; step_loss: 0.0744
step 66030; step_loss: 0.0727
step 66040; step_loss: 0.0916
step 66050; step_loss: 0.1713
step 66060; step_loss: 0.0938
step 66070; step_loss: 0.0955
step 66080; step_loss: 0.0877
step 66090; step_loss: 0.1070
step 66100; step_loss: 0.0752
step 66110; step_loss: 0.0761
step 66120; step_loss: 0.0972
step 66130; step_loss: 0.0913
step 66140; step_loss: 0.0600
step 66150; step_loss: 0.0758
step 66160; step_loss: 0.0695
step 66170; step_loss: 0.0510
step 66180; step_loss: 0.0783
step 66190; step_loss: 0.0507
step 66200; step_loss: 0.0836
step 66210; step_loss: 0.0564
step 66220; step_loss: 0.1148
step 66230; step_loss: 0.0853
step 66240; step_loss: 0.0812
step 66250; step_loss: 0.1115
step 66260; step_loss: 0.0772
step 66270; step_loss: 0.0844
step 66280; step_loss: 0.0665
step 66290; step_loss: 0.0977
step 66300; step_loss: 0.0751
step 66310; step_loss: 0.0678
step 66320; step_loss: 0.0524
step 66330; step_loss: 0.1627
step 66340; step_loss: 0.1069
step 66350; step_loss: 0.0837
step 66360; step_loss: 0.0688
step 66370; step_loss: 0.0818
step 66380; step_loss: 0.0948
step 66390; step_loss: 0.0596
step 66400; step_loss: 0.0884
step 66410; step_loss: 0.1399
step 66420; step_loss: 0.1148
step 66430; step_loss: 0.1198
step 66440; step_loss: 0.0766
step 66450; step_loss: 0.0589
step 66460; step_loss: 0.0972
step 66470; step_loss: 0.0746
step 66480; step_loss: 0.1039
step 66490; step_loss: 0.0736
step 66500; step_loss: 0.1266
step 66510; step_loss: 0.0726
step 66520; step_loss: 0.0603
step 66530; step_loss: 0.0663
step 66540; step_loss: 0.0919
step 66550; step_loss: 0.0959
step 66560; step_loss: 0.0702
step 66570; step_loss: 0.0766
step 66580; step_loss: 0.0950
step 66590; step_loss: 0.1170
step 66600; step_loss: 0.1384
step 66610; step_loss: 0.0810
step 66620; step_loss: 0.0758
step 66630; step_loss: 0.0802
step 66640; step_loss: 0.0777
step 66650; step_loss: 0.0700
step 66660; step_loss: 0.1048
step 66670; step_loss: 0.0767
step 66680; step_loss: 0.0597
step 66690; step_loss: 0.0870
step 66700; step_loss: 0.1137
step 66710; step_loss: 0.0886
step 66720; step_loss: 0.0926
step 66730; step_loss: 0.1085
step 66740; step_loss: 0.0830
step 66750; step_loss: 0.0870
step 66760; step_loss: 0.0847
step 66770; step_loss: 0.1098
step 66780; step_loss: 0.1233
step 66790; step_loss: 0.0757
step 66800; step_loss: 0.0990
step 66810; step_loss: 0.1057
step 66820; step_loss: 0.1559
step 66830; step_loss: 0.0845
step 66840; step_loss: 0.0819
step 66850; step_loss: 0.0986
step 66860; step_loss: 0.1286
step 66870; step_loss: 0.0920
step 66880; step_loss: 0.0939
step 66890; step_loss: 0.0680
step 66900; step_loss: 0.0685
step 66910; step_loss: 0.0739
step 66920; step_loss: 0.0607
step 66930; step_loss: 0.1181
step 66940; step_loss: 0.0954
step 66950; step_loss: 0.0775
step 66960; step_loss: 0.0883
step 66970; step_loss: 0.1067
step 66980; step_loss: 0.0714
step 66990; step_loss: 0.0863

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 0.414 | 0.721 | 1.104 | 1.240 |   n/a |   n/a |

============================
Global step:         67000
Learning rate:       0.0037
Step-time (ms):     27.0681
Train loss avg:      0.0932
--------------------------
Val loss:            0.4621
srnn loss:           0.3648
============================

Saving the model...
done in 280.51 ms
step 67000; step_loss: 0.1459
step 67010; step_loss: 0.0628
step 67020; step_loss: 0.1011
step 67030; step_loss: 0.0730
step 67040; step_loss: 0.1513
step 67050; step_loss: 0.0811
step 67060; step_loss: 0.0802
step 67070; step_loss: 0.1658
step 67080; step_loss: 0.0683
step 67090; step_loss: 0.1254
step 67100; step_loss: 0.1185
step 67110; step_loss: 0.0929
step 67120; step_loss: 0.0901
step 67130; step_loss: 0.0798
step 67140; step_loss: 0.0856
step 67150; step_loss: 0.0882
step 67160; step_loss: 0.0849
step 67170; step_loss: 0.0819
step 67180; step_loss: 0.0860
step 67190; step_loss: 0.1603
step 67200; step_loss: 0.1002
step 67210; step_loss: 0.1048
step 67220; step_loss: 0.1275
step 67230; step_loss: 0.0750
step 67240; step_loss: 0.0871
step 67250; step_loss: 0.1116
step 67260; step_loss: 0.0995
step 67270; step_loss: 0.1098
step 67280; step_loss: 0.0801
step 67290; step_loss: 0.1376
step 67300; step_loss: 0.0801
step 67310; step_loss: 0.1304
step 67320; step_loss: 0.0798
step 67330; step_loss: 0.0655
step 67340; step_loss: 0.0891
step 67350; step_loss: 0.0965
step 67360; step_loss: 0.0832
step 67370; step_loss: 0.0727
step 67380; step_loss: 0.1081
step 67390; step_loss: 0.1054
step 67400; step_loss: 0.1017
step 67410; step_loss: 0.1016
step 67420; step_loss: 0.0777
step 67430; step_loss: 0.1059
step 67440; step_loss: 0.0698
step 67450; step_loss: 0.1663
step 67460; step_loss: 0.0686
step 67470; step_loss: 0.0802
step 67480; step_loss: 0.0830
step 67490; step_loss: 0.0746
step 67500; step_loss: 0.1002
step 67510; step_loss: 0.0758
step 67520; step_loss: 0.0687
step 67530; step_loss: 0.0915
step 67540; step_loss: 0.1396
step 67550; step_loss: 0.0662
step 67560; step_loss: 0.0935
step 67570; step_loss: 0.1125
step 67580; step_loss: 0.1761
step 67590; step_loss: 0.1561
step 67600; step_loss: 0.0774
step 67610; step_loss: 0.0707
step 67620; step_loss: 0.0657
step 67630; step_loss: 0.1220
step 67640; step_loss: 0.0817
step 67650; step_loss: 0.0651
step 67660; step_loss: 0.0888
step 67670; step_loss: 0.1133
step 67680; step_loss: 0.0975
step 67690; step_loss: 0.0967
step 67700; step_loss: 0.0661
step 67710; step_loss: 0.1261
step 67720; step_loss: 0.1439
step 67730; step_loss: 0.0638
step 67740; step_loss: 0.0673
step 67750; step_loss: 0.0947
step 67760; step_loss: 0.1089
step 67770; step_loss: 0.0882
step 67780; step_loss: 0.1172
step 67790; step_loss: 0.0824
step 67800; step_loss: 0.1181
step 67810; step_loss: 0.0843
step 67820; step_loss: 0.0983
step 67830; step_loss: 0.0709
step 67840; step_loss: 0.0753
step 67850; step_loss: 0.0940
step 67860; step_loss: 0.1539
step 67870; step_loss: 0.0871
step 67880; step_loss: 0.0972
step 67890; step_loss: 0.0805
step 67900; step_loss: 0.0751
step 67910; step_loss: 0.0669
step 67920; step_loss: 0.0840
step 67930; step_loss: 0.1575
step 67940; step_loss: 0.0799
step 67950; step_loss: 0.1370
step 67960; step_loss: 0.0768
step 67970; step_loss: 0.0812
step 67980; step_loss: 0.1289
step 67990; step_loss: 0.1007

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 0.420 | 0.728 | 1.105 | 1.234 |   n/a |   n/a |

============================
Global step:         68000
Learning rate:       0.0037
Step-time (ms):     27.1014
Train loss avg:      0.0921
--------------------------
Val loss:            0.3623
srnn loss:           0.3693
============================

Saving the model...
done in 266.62 ms
step 68000; step_loss: 0.0557
step 68010; step_loss: 0.0850
step 68020; step_loss: 0.0654
step 68030; step_loss: 0.0762
step 68040; step_loss: 0.0919
step 68050; step_loss: 0.1189
step 68060; step_loss: 0.1441
step 68070; step_loss: 0.1005
step 68080; step_loss: 0.0977
step 68090; step_loss: 0.1139
step 68100; step_loss: 0.1978
step 68110; step_loss: 0.1210
step 68120; step_loss: 0.1010
step 68130; step_loss: 0.0990
step 68140; step_loss: 0.1047
step 68150; step_loss: 0.0788
step 68160; step_loss: 0.0854
step 68170; step_loss: 0.0650
step 68180; step_loss: 0.1217
step 68190; step_loss: 0.0987
step 68200; step_loss: 0.1950
step 68210; step_loss: 0.0769
step 68220; step_loss: 0.0685
step 68230; step_loss: 0.0890
step 68240; step_loss: 0.0637
step 68250; step_loss: 0.0955
step 68260; step_loss: 0.0961
step 68270; step_loss: 0.0968
step 68280; step_loss: 0.1107
step 68290; step_loss: 0.0808
step 68300; step_loss: 0.0627
step 68310; step_loss: 0.0853
step 68320; step_loss: 0.0857
step 68330; step_loss: 0.1045
step 68340; step_loss: 0.0722
step 68350; step_loss: 0.0535
step 68360; step_loss: 0.0914
step 68370; step_loss: 0.0635
step 68380; step_loss: 0.0725
step 68390; step_loss: 0.0593
step 68400; step_loss: 0.1516
step 68410; step_loss: 0.0757
step 68420; step_loss: 0.0990
step 68430; step_loss: 0.1191
step 68440; step_loss: 0.0748
step 68450; step_loss: 0.1063
step 68460; step_loss: 0.0634
step 68470; step_loss: 0.0713
step 68480; step_loss: 0.1577
step 68490; step_loss: 0.0863
step 68500; step_loss: 0.0954
step 68510; step_loss: 0.0928
step 68520; step_loss: 0.1297
step 68530; step_loss: 0.1055
step 68540; step_loss: 0.0746
step 68550; step_loss: 0.0736
step 68560; step_loss: 0.0689
step 68570; step_loss: 0.0754
step 68580; step_loss: 0.0924
step 68590; step_loss: 0.1115
step 68600; step_loss: 0.0707
step 68610; step_loss: 0.0905
step 68620; step_loss: 0.0513
step 68630; step_loss: 0.0953
step 68640; step_loss: 0.0566
step 68650; step_loss: 0.0977
step 68660; step_loss: 0.0927
step 68670; step_loss: 0.0838
step 68680; step_loss: 0.0964
step 68690; step_loss: 0.0669
step 68700; step_loss: 0.0781
step 68710; step_loss: 0.1176
step 68720; step_loss: 0.0799
step 68730; step_loss: 0.0864
step 68740; step_loss: 0.0860
step 68750; step_loss: 0.1152
step 68760; step_loss: 0.0919
step 68770; step_loss: 0.0606
step 68780; step_loss: 0.0794
step 68790; step_loss: 0.0766
step 68800; step_loss: 0.0869
step 68810; step_loss: 0.0738
step 68820; step_loss: 0.1129
step 68830; step_loss: 0.1031
step 68840; step_loss: 0.1653
step 68850; step_loss: 0.0972
step 68860; step_loss: 0.0679
step 68870; step_loss: 0.1303
step 68880; step_loss: 0.0974
step 68890; step_loss: 0.0675
step 68900; step_loss: 0.1190
step 68910; step_loss: 0.0902
step 68920; step_loss: 0.0821
step 68930; step_loss: 0.0771
step 68940; step_loss: 0.1110
step 68950; step_loss: 0.0827
step 68960; step_loss: 0.1063
step 68970; step_loss: 0.1105
step 68980; step_loss: 0.0866
step 68990; step_loss: 0.0650

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 0.420 | 0.730 | 1.111 | 1.242 |   n/a |   n/a |

============================
Global step:         69000
Learning rate:       0.0037
Step-time (ms):     27.1760
Train loss avg:      0.0923
--------------------------
Val loss:            0.3891
srnn loss:           0.3657
============================

Saving the model...
done in 288.63 ms
step 69000; step_loss: 0.0979
step 69010; step_loss: 0.1917
step 69020; step_loss: 0.0919
step 69030; step_loss: 0.1248
step 69040; step_loss: 0.1216
step 69050; step_loss: 0.0906
step 69060; step_loss: 0.1270
step 69070; step_loss: 0.0980
step 69080; step_loss: 0.0749
step 69090; step_loss: 0.0693
step 69100; step_loss: 0.0655
step 69110; step_loss: 0.0954
step 69120; step_loss: 0.1149
step 69130; step_loss: 0.0968
step 69140; step_loss: 0.0667
step 69150; step_loss: 0.0754
step 69160; step_loss: 0.0592
step 69170; step_loss: 0.0872
step 69180; step_loss: 0.0766
step 69190; step_loss: 0.0819
step 69200; step_loss: 0.0683
step 69210; step_loss: 0.0753
step 69220; step_loss: 0.0741
step 69230; step_loss: 0.1012
step 69240; step_loss: 0.0898
step 69250; step_loss: 0.0842
step 69260; step_loss: 0.0861
step 69270; step_loss: 0.0915
step 69280; step_loss: 0.0668
step 69290; step_loss: 0.0678
step 69300; step_loss: 0.0527
step 69310; step_loss: 0.0652
step 69320; step_loss: 0.1325
step 69330; step_loss: 0.0666
step 69340; step_loss: 0.0716
step 69350; step_loss: 0.0743
step 69360; step_loss: 0.0944
step 69370; step_loss: 0.0732
step 69380; step_loss: 0.1020
step 69390; step_loss: 0.0751
step 69400; step_loss: 0.0528
step 69410; step_loss: 0.0749
step 69420; step_loss: 0.0832
step 69430; step_loss: 0.0916
step 69440; step_loss: 0.0672
step 69450; step_loss: 0.0785
step 69460; step_loss: 0.0842
step 69470; step_loss: 0.1077
step 69480; step_loss: 0.0870
step 69490; step_loss: 0.1177
step 69500; step_loss: 0.0811
step 69510; step_loss: 0.0665
step 69520; step_loss: 0.0778
step 69530; step_loss: 0.0660
step 69540; step_loss: 0.0697
step 69550; step_loss: 0.0679
step 69560; step_loss: 0.0876
step 69570; step_loss: 0.0714
step 69580; step_loss: 0.0608
step 69590; step_loss: 0.1092
step 69600; step_loss: 0.0792
step 69610; step_loss: 0.0753
step 69620; step_loss: 0.1032
step 69630; step_loss: 0.1490
step 69640; step_loss: 0.1167
step 69650; step_loss: 0.0858
step 69660; step_loss: 0.0900
step 69670; step_loss: 0.0771
step 69680; step_loss: 0.1108
step 69690; step_loss: 0.0872
step 69700; step_loss: 0.1030
step 69710; step_loss: 0.0788
step 69720; step_loss: 0.0925
step 69730; step_loss: 0.0729
step 69740; step_loss: 0.1176
step 69750; step_loss: 0.1607
step 69760; step_loss: 0.0999
step 69770; step_loss: 0.0876
step 69780; step_loss: 0.1096
step 69790; step_loss: 0.0739
step 69800; step_loss: 0.0689
step 69810; step_loss: 0.1730
step 69820; step_loss: 0.1041
step 69830; step_loss: 0.1011
step 69840; step_loss: 0.0629
step 69850; step_loss: 0.0816
step 69860; step_loss: 0.1019
step 69870; step_loss: 0.0625
step 69880; step_loss: 0.1049
step 69890; step_loss: 0.0935
step 69900; step_loss: 0.1292
step 69910; step_loss: 0.0624
step 69920; step_loss: 0.0836
step 69930; step_loss: 0.1392
step 69940; step_loss: 0.0694
step 69950; step_loss: 0.0968
step 69960; step_loss: 0.0732
step 69970; step_loss: 0.0994
step 69980; step_loss: 0.1343
step 69990; step_loss: 0.0773

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 0.422 | 0.734 | 1.124 | 1.262 |   n/a |   n/a |

============================
Global step:         70000
Learning rate:       0.0035
Step-time (ms):     27.1008
Train loss avg:      0.0911
--------------------------
Val loss:            0.4299
srnn loss:           0.3669
============================

Saving the model...
done in 269.80 ms
step 70000; step_loss: 0.1255
step 70010; step_loss: 0.0727
step 70020; step_loss: 0.1015
step 70030; step_loss: 0.1017
step 70040; step_loss: 0.1462
step 70050; step_loss: 0.0770
step 70060; step_loss: 0.0691
step 70070; step_loss: 0.0773
step 70080; step_loss: 0.0895
step 70090; step_loss: 0.0821
step 70100; step_loss: 0.0561
step 70110; step_loss: 0.0986
step 70120; step_loss: 0.0852
step 70130; step_loss: 0.1059
step 70140; step_loss: 0.0889
step 70150; step_loss: 0.0762
step 70160; step_loss: 0.0975
step 70170; step_loss: 0.0700
step 70180; step_loss: 0.0794
step 70190; step_loss: 0.0824
step 70200; step_loss: 0.0924
step 70210; step_loss: 0.0703
step 70220; step_loss: 0.0741
step 70230; step_loss: 0.0625
step 70240; step_loss: 0.0866
step 70250; step_loss: 0.0712
step 70260; step_loss: 0.1393
step 70270; step_loss: 0.1529
step 70280; step_loss: 0.0689
step 70290; step_loss: 0.1181
step 70300; step_loss: 0.0714
step 70310; step_loss: 0.0954
step 70320; step_loss: 0.0651
step 70330; step_loss: 0.0844
step 70340; step_loss: 0.0613
step 70350; step_loss: 0.0851
step 70360; step_loss: 0.0953
step 70370; step_loss: 0.0921
step 70380; step_loss: 0.0758
step 70390; step_loss: 0.1033
step 70400; step_loss: 0.1871
step 70410; step_loss: 0.0770
step 70420; step_loss: 0.0679
step 70430; step_loss: 0.0641
step 70440; step_loss: 0.1041
step 70450; step_loss: 0.0918
step 70460; step_loss: 0.1221
step 70470; step_loss: 0.0621
step 70480; step_loss: 0.0848
step 70490; step_loss: 0.0721
step 70500; step_loss: 0.0731
step 70510; step_loss: 0.0945
step 70520; step_loss: 0.0985
step 70530; step_loss: 0.0581
step 70540; step_loss: 0.0847
step 70550; step_loss: 0.0926
step 70560; step_loss: 0.0913
step 70570; step_loss: 0.0889
step 70580; step_loss: 0.1230
step 70590; step_loss: 0.0819
step 70600; step_loss: 0.0909
step 70610; step_loss: 0.0998
step 70620; step_loss: 0.0621
step 70630; step_loss: 0.0935
step 70640; step_loss: 0.0864
step 70650; step_loss: 0.0815
step 70660; step_loss: 0.0618
step 70670; step_loss: 0.0786
step 70680; step_loss: 0.1165
step 70690; step_loss: 0.1359
step 70700; step_loss: 0.1171
step 70710; step_loss: 0.0733
step 70720; step_loss: 0.1029
step 70730; step_loss: 0.0712
step 70740; step_loss: 0.1829
step 70750; step_loss: 0.0854
step 70760; step_loss: 0.0730
step 70770; step_loss: 0.1566
step 70780; step_loss: 0.0568
step 70790; step_loss: 0.0774
step 70800; step_loss: 0.0710
step 70810; step_loss: 0.0851
step 70820; step_loss: 0.0612
step 70830; step_loss: 0.0695
step 70840; step_loss: 0.0729
step 70850; step_loss: 0.0592
step 70860; step_loss: 0.1304
step 70870; step_loss: 0.0787
step 70880; step_loss: 0.0651
step 70890; step_loss: 0.0730
step 70900; step_loss: 0.1005
step 70910; step_loss: 0.1067
step 70920; step_loss: 0.0542
step 70930; step_loss: 0.0806
step 70940; step_loss: 0.0784
step 70950; step_loss: 0.0683
step 70960; step_loss: 0.0778
step 70970; step_loss: 0.1618
step 70980; step_loss: 0.0580
step 70990; step_loss: 0.1144

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 0.418 | 0.731 | 1.119 | 1.255 |   n/a |   n/a |

============================
Global step:         71000
Learning rate:       0.0035
Step-time (ms):     27.0822
Train loss avg:      0.0894
--------------------------
Val loss:            0.4227
srnn loss:           0.3720
============================

Saving the model...
done in 294.18 ms
step 71000; step_loss: 0.0834
step 71010; step_loss: 0.1316
step 71020; step_loss: 0.1236
step 71030; step_loss: 0.0675
step 71040; step_loss: 0.0890
step 71050; step_loss: 0.0721
step 71060; step_loss: 0.0824
step 71070; step_loss: 0.0745
step 71080; step_loss: 0.0538
step 71090; step_loss: 0.0974
step 71100; step_loss: 0.0569
step 71110; step_loss: 0.0571
step 71120; step_loss: 0.0658
step 71130; step_loss: 0.1657
step 71140; step_loss: 0.0747
step 71150; step_loss: 0.0903
step 71160; step_loss: 0.0583
step 71170; step_loss: 0.1155
step 71180; step_loss: 0.1084
step 71190; step_loss: 0.0968
step 71200; step_loss: 0.0965
step 71210; step_loss: 0.1470
step 71220; step_loss: 0.0829
step 71230; step_loss: 0.1344
step 71240; step_loss: 0.0659
step 71250; step_loss: 0.0743
step 71260; step_loss: 0.0901
step 71270; step_loss: 0.0708
step 71280; step_loss: 0.1457
step 71290; step_loss: 0.0689
step 71300; step_loss: 0.0734
step 71310; step_loss: 0.0842
step 71320; step_loss: 0.0672
step 71330; step_loss: 0.0915
step 71340; step_loss: 0.0699
step 71350; step_loss: 0.2055
step 71360; step_loss: 0.0784
step 71370; step_loss: 0.0748
step 71380; step_loss: 0.0667
step 71390; step_loss: 0.0938
step 71400; step_loss: 0.0785
step 71410; step_loss: 0.0870
step 71420; step_loss: 0.0876
step 71430; step_loss: 0.1218
step 71440; step_loss: 0.0737
step 71450; step_loss: 0.1133
step 71460; step_loss: 0.0654
step 71470; step_loss: 0.0577
step 71480; step_loss: 0.0875
step 71490; step_loss: 0.0735
step 71500; step_loss: 0.0813
step 71510; step_loss: 0.0962
step 71520; step_loss: 0.0836
step 71530; step_loss: 0.1098
step 71540; step_loss: 0.0817
step 71550; step_loss: 0.0640
step 71560; step_loss: 0.0987
step 71570; step_loss: 0.0563
step 71580; step_loss: 0.0625
step 71590; step_loss: 0.0586
step 71600; step_loss: 0.0884
step 71610; step_loss: 0.0605
step 71620; step_loss: 0.1685
step 71630; step_loss: 0.0627
step 71640; step_loss: 0.0876
step 71650; step_loss: 0.0728
step 71660; step_loss: 0.0719
step 71670; step_loss: 0.0609
step 71680; step_loss: 0.1307
step 71690; step_loss: 0.0922
step 71700; step_loss: 0.1017
step 71710; step_loss: 0.1144
step 71720; step_loss: 0.0771
step 71730; step_loss: 0.0833
step 71740; step_loss: 0.0971
step 71750; step_loss: 0.0734
step 71760; step_loss: 0.0695
step 71770; step_loss: 0.0637
step 71780; step_loss: 0.0708
step 71790; step_loss: 0.0564
step 71800; step_loss: 0.0918
step 71810; step_loss: 0.1136
step 71820; step_loss: 0.0679
step 71830; step_loss: 0.1693
step 71840; step_loss: 0.0981
step 71850; step_loss: 0.0963
step 71860; step_loss: 0.1383
step 71870; step_loss: 0.1133
step 71880; step_loss: 0.0843
step 71890; step_loss: 0.1298
step 71900; step_loss: 0.0656
step 71910; step_loss: 0.0802
step 71920; step_loss: 0.0712
step 71930; step_loss: 0.0578
step 71940; step_loss: 0.0707
step 71950; step_loss: 0.0713
step 71960; step_loss: 0.0714
step 71970; step_loss: 0.1368
step 71980; step_loss: 0.1204
step 71990; step_loss: 0.0795

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 0.422 | 0.738 | 1.135 | 1.273 |   n/a |   n/a |

============================
Global step:         72000
Learning rate:       0.0035
Step-time (ms):     27.1334
Train loss avg:      0.0890
--------------------------
Val loss:            0.4144
srnn loss:           0.3713
============================

Saving the model...
done in 312.39 ms
step 72000; step_loss: 0.0865
step 72010; step_loss: 0.0649
step 72020; step_loss: 0.1080
step 72030; step_loss: 0.0764
step 72040; step_loss: 0.1134
step 72050; step_loss: 0.0667
step 72060; step_loss: 0.0773
step 72070; step_loss: 0.1298
step 72080; step_loss: 0.0945
step 72090; step_loss: 0.0811
step 72100; step_loss: 0.0709
step 72110; step_loss: 0.0935
step 72120; step_loss: 0.0731
step 72130; step_loss: 0.0633
step 72140; step_loss: 0.0754
step 72150; step_loss: 0.0840
step 72160; step_loss: 0.1158
step 72170; step_loss: 0.0930
step 72180; step_loss: 0.0589
step 72190; step_loss: 0.0953
step 72200; step_loss: 0.0528
step 72210; step_loss: 0.1183
step 72220; step_loss: 0.1022
step 72230; step_loss: 0.1107
step 72240; step_loss: 0.1298
step 72250; step_loss: 0.0885
step 72260; step_loss: 0.0736
step 72270; step_loss: 0.0769
step 72280; step_loss: 0.0831
step 72290; step_loss: 0.0699
step 72300; step_loss: 0.0930
step 72310; step_loss: 0.0653
step 72320; step_loss: 0.0614
step 72330; step_loss: 0.0807
step 72340; step_loss: 0.0863
step 72350; step_loss: 0.0768
step 72360; step_loss: 0.1130
step 72370; step_loss: 0.0702
step 72380; step_loss: 0.1544
step 72390; step_loss: 0.0733
step 72400; step_loss: 0.0850
step 72410; step_loss: 0.0472
step 72420; step_loss: 0.0992
step 72430; step_loss: 0.0913
step 72440; step_loss: 0.0858
step 72450; step_loss: 0.0980
step 72460; step_loss: 0.1419
step 72470; step_loss: 0.1110
step 72480; step_loss: 0.0958
step 72490; step_loss: 0.0860
step 72500; step_loss: 0.0776
step 72510; step_loss: 0.0941
step 72520; step_loss: 0.0869
step 72530; step_loss: 0.0802
step 72540; step_loss: 0.0656
step 72550; step_loss: 0.0730
step 72560; step_loss: 0.1125
step 72570; step_loss: 0.1000
step 72580; step_loss: 0.0727
step 72590; step_loss: 0.0583
step 72600; step_loss: 0.0564
step 72610; step_loss: 0.0890
step 72620; step_loss: 0.0707
step 72630; step_loss: 0.0770
step 72640; step_loss: 0.0932
step 72650; step_loss: 0.1105
step 72660; step_loss: 0.0787
step 72670; step_loss: 0.1674
step 72680; step_loss: 0.0875
step 72690; step_loss: 0.0893
step 72700; step_loss: 0.0541
step 72710; step_loss: 0.1122
step 72720; step_loss: 0.0926
step 72730; step_loss: 0.0686
step 72740; step_loss: 0.0704
step 72750; step_loss: 0.1044
step 72760; step_loss: 0.1219
step 72770; step_loss: 0.0880
step 72780; step_loss: 0.1350
step 72790; step_loss: 0.1652
step 72800; step_loss: 0.0954
step 72810; step_loss: 0.0864
step 72820; step_loss: 0.0744
step 72830; step_loss: 0.1331
step 72840; step_loss: 0.0990
step 72850; step_loss: 0.0978
step 72860; step_loss: 0.0512
step 72870; step_loss: 0.0582
step 72880; step_loss: 0.1360
step 72890; step_loss: 0.0712
step 72900; step_loss: 0.0870
step 72910; step_loss: 0.0725
step 72920; step_loss: 0.0719
step 72930; step_loss: 0.1095
step 72940; step_loss: 0.0685
step 72950; step_loss: 0.0803
step 72960; step_loss: 0.1678
step 72970; step_loss: 0.0556
step 72980; step_loss: 0.1002
step 72990; step_loss: 0.0930

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 0.430 | 0.749 | 1.149 | 1.285 |   n/a |   n/a |

============================
Global step:         73000
Learning rate:       0.0035
Step-time (ms):     27.1755
Train loss avg:      0.0912
--------------------------
Val loss:            0.4444
srnn loss:           0.3894
============================

Saving the model...
done in 274.92 ms
step 73000; step_loss: 0.1030
step 73010; step_loss: 0.0819
step 73020; step_loss: 0.0892
step 73030; step_loss: 0.0739
step 73040; step_loss: 0.0884
step 73050; step_loss: 0.0963
step 73060; step_loss: 0.0850
step 73070; step_loss: 0.1066
step 73080; step_loss: 0.0763
step 73090; step_loss: 0.0754
step 73100; step_loss: 0.0932
step 73110; step_loss: 0.1016
step 73120; step_loss: 0.0805
step 73130; step_loss: 0.0668
step 73140; step_loss: 0.0998
step 73150; step_loss: 0.0537
step 73160; step_loss: 0.0690
step 73170; step_loss: 0.0864
step 73180; step_loss: 0.0941
step 73190; step_loss: 0.0609
step 73200; step_loss: 0.0923
step 73210; step_loss: 0.0883
step 73220; step_loss: 0.0682
step 73230; step_loss: 0.0649
step 73240; step_loss: 0.0708
step 73250; step_loss: 0.0738
step 73260; step_loss: 0.0914
step 73270; step_loss: 0.1133
step 73280; step_loss: 0.0704
step 73290; step_loss: 0.0664
step 73300; step_loss: 0.1352
step 73310; step_loss: 0.0748
step 73320; step_loss: 0.0755
step 73330; step_loss: 0.0727
step 73340; step_loss: 0.0908
step 73350; step_loss: 0.0739
step 73360; step_loss: 0.0918
step 73370; step_loss: 0.1035
step 73380; step_loss: 0.0748
step 73390; step_loss: 0.0833
step 73400; step_loss: 0.1093
step 73410; step_loss: 0.0879
step 73420; step_loss: 0.0716
step 73430; step_loss: 0.0809
step 73440; step_loss: 0.0853
step 73450; step_loss: 0.1068
step 73460; step_loss: 0.1413
step 73470; step_loss: 0.0588
step 73480; step_loss: 0.1077
step 73490; step_loss: 0.0672
step 73500; step_loss: 0.0942
step 73510; step_loss: 0.1642
step 73520; step_loss: 0.0843
step 73530; step_loss: 0.0867
step 73540; step_loss: 0.0762
step 73550; step_loss: 0.0844
step 73560; step_loss: 0.0782
step 73570; step_loss: 0.0867
step 73580; step_loss: 0.0740
step 73590; step_loss: 0.1599
step 73600; step_loss: 0.1294
step 73610; step_loss: 0.0761
step 73620; step_loss: 0.0660
step 73630; step_loss: 0.0510
step 73640; step_loss: 0.0684
step 73650; step_loss: 0.1184
step 73660; step_loss: 0.0808
step 73670; step_loss: 0.0847
step 73680; step_loss: 0.0784
step 73690; step_loss: 0.0758
step 73700; step_loss: 0.1259
step 73710; step_loss: 0.1133
step 73720; step_loss: 0.1008
step 73730; step_loss: 0.0741
step 73740; step_loss: 0.0990
step 73750; step_loss: 0.1506
step 73760; step_loss: 0.0980
step 73770; step_loss: 0.1031
step 73780; step_loss: 0.0640
step 73790; step_loss: 0.0798
step 73800; step_loss: 0.0976
step 73810; step_loss: 0.1029
step 73820; step_loss: 0.0760
step 73830; step_loss: 0.1268
step 73840; step_loss: 0.0743
step 73850; step_loss: 0.0679
step 73860; step_loss: 0.0588
step 73870; step_loss: 0.1171
step 73880; step_loss: 0.1118
step 73890; step_loss: 0.0712
step 73900; step_loss: 0.0606
step 73910; step_loss: 0.0784
step 73920; step_loss: 0.0745
step 73930; step_loss: 0.1082
step 73940; step_loss: 0.1278
step 73950; step_loss: 0.0623
step 73960; step_loss: 0.0900
step 73970; step_loss: 0.1086
step 73980; step_loss: 0.1235
step 73990; step_loss: 0.0755

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 0.418 | 0.725 | 1.093 | 1.220 |   n/a |   n/a |

============================
Global step:         74000
Learning rate:       0.0035
Step-time (ms):     27.1447
Train loss avg:      0.0899
--------------------------
Val loss:            0.4406
srnn loss:           0.3791
============================

Saving the model...
done in 284.57 ms
step 74000; step_loss: 0.0784
step 74010; step_loss: 0.0660
step 74020; step_loss: 0.0927
step 74030; step_loss: 0.1122
step 74040; step_loss: 0.0838
step 74050; step_loss: 0.0896
step 74060; step_loss: 0.1307
step 74070; step_loss: 0.0771
step 74080; step_loss: 0.0776
step 74090; step_loss: 0.0998
step 74100; step_loss: 0.0729
step 74110; step_loss: 0.0845
step 74120; step_loss: 0.0886
step 74130; step_loss: 0.1058
step 74140; step_loss: 0.0712
step 74150; step_loss: 0.1671
step 74160; step_loss: 0.0586
step 74170; step_loss: 0.1029
step 74180; step_loss: 0.0954
step 74190; step_loss: 0.0909
step 74200; step_loss: 0.0876
step 74210; step_loss: 0.0636
step 74220; step_loss: 0.0859
step 74230; step_loss: 0.0708
step 74240; step_loss: 0.0837
step 74250; step_loss: 0.0924
step 74260; step_loss: 0.1031
step 74270; step_loss: 0.0878
step 74280; step_loss: 0.1086
step 74290; step_loss: 0.0748
step 74300; step_loss: 0.0889
step 74310; step_loss: 0.0880
step 74320; step_loss: 0.1107
step 74330; step_loss: 0.0562
step 74340; step_loss: 0.1303
step 74350; step_loss: 0.0705
step 74360; step_loss: 0.0975
step 74370; step_loss: 0.0695
step 74380; step_loss: 0.0739
step 74390; step_loss: 0.0864
step 74400; step_loss: 0.0868
step 74410; step_loss: 0.0787
step 74420; step_loss: 0.0744
step 74430; step_loss: 0.0885
step 74440; step_loss: 0.0796
step 74450; step_loss: 0.1114
step 74460; step_loss: 0.0857
step 74470; step_loss: 0.1141
step 74480; step_loss: 0.0787
step 74490; step_loss: 0.0877
step 74500; step_loss: 0.0681
step 74510; step_loss: 0.0829
step 74520; step_loss: 0.0884
step 74530; step_loss: 0.0731
step 74540; step_loss: 0.0838
step 74550; step_loss: 0.0937
step 74560; step_loss: 0.0899
step 74570; step_loss: 0.1034
step 74580; step_loss: 0.1210
step 74590; step_loss: 0.0688
step 74600; step_loss: 0.0899
step 74610; step_loss: 0.1104
step 74620; step_loss: 0.0705
step 74630; step_loss: 0.0885
step 74640; step_loss: 0.0777
step 74650; step_loss: 0.0675
step 74660; step_loss: 0.0893
step 74670; step_loss: 0.1183
step 74680; step_loss: 0.0876
step 74690; step_loss: 0.0815
step 74700; step_loss: 0.0863
step 74710; step_loss: 0.0930
step 74720; step_loss: 0.0789
step 74730; step_loss: 0.1042
step 74740; step_loss: 0.0741
step 74750; step_loss: 0.0654
step 74760; step_loss: 0.0647
step 74770; step_loss: 0.0463
step 74780; step_loss: 0.0630
step 74790; step_loss: 0.0892
step 74800; step_loss: 0.1887
step 74810; step_loss: 0.0779
step 74820; step_loss: 0.1026
step 74830; step_loss: 0.0676
step 74840; step_loss: 0.0762
step 74850; step_loss: 0.0936
step 74860; step_loss: 0.1029
step 74870; step_loss: 0.0776
step 74880; step_loss: 0.0674
step 74890; step_loss: 0.0779
step 74900; step_loss: 0.0643
step 74910; step_loss: 0.0809
step 74920; step_loss: 0.0792
step 74930; step_loss: 0.0556
step 74940; step_loss: 0.0694
step 74950; step_loss: 0.2216
step 74960; step_loss: 0.1034
step 74970; step_loss: 0.0918
step 74980; step_loss: 0.0712
step 74990; step_loss: 0.0707

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 0.421 | 0.734 | 1.123 | 1.256 |   n/a |   n/a |

============================
Global step:         75000
Learning rate:       0.0035
Step-time (ms):     27.1077
Train loss avg:      0.0890
--------------------------
Val loss:            0.3939
srnn loss:           0.3755
============================

Saving the model...
done in 278.24 ms
step 75000; step_loss: 0.0759
step 75010; step_loss: 0.0796
step 75020; step_loss: 0.0999
step 75030; step_loss: 0.0780
step 75040; step_loss: 0.1391
step 75050; step_loss: 0.0777
step 75060; step_loss: 0.0616
step 75070; step_loss: 0.0811
step 75080; step_loss: 0.0980
step 75090; step_loss: 0.1405
step 75100; step_loss: 0.0869
step 75110; step_loss: 0.0739
step 75120; step_loss: 0.0832
step 75130; step_loss: 0.1309
step 75140; step_loss: 0.0836
step 75150; step_loss: 0.0843
step 75160; step_loss: 0.0976
step 75170; step_loss: 0.0670
step 75180; step_loss: 0.0802
step 75190; step_loss: 0.0811
step 75200; step_loss: 0.0796
step 75210; step_loss: 0.0855
step 75220; step_loss: 0.0829
step 75230; step_loss: 0.0746
step 75240; step_loss: 0.1126
step 75250; step_loss: 0.0807
step 75260; step_loss: 0.1022
step 75270; step_loss: 0.0979
step 75280; step_loss: 0.0615
step 75290; step_loss: 0.0618
step 75300; step_loss: 0.0751
step 75310; step_loss: 0.1018
step 75320; step_loss: 0.1427
step 75330; step_loss: 0.0705
step 75340; step_loss: 0.0786
step 75350; step_loss: 0.0809
step 75360; step_loss: 0.0547
step 75370; step_loss: 0.1042
step 75380; step_loss: 0.1653
step 75390; step_loss: 0.1157
step 75400; step_loss: 0.0842
step 75410; step_loss: 0.0841
step 75420; step_loss: 0.0653
step 75430; step_loss: 0.0881
step 75440; step_loss: 0.0762
step 75450; step_loss: 0.0782
step 75460; step_loss: 0.0782
step 75470; step_loss: 0.0664
step 75480; step_loss: 0.0881
step 75490; step_loss: 0.1018
step 75500; step_loss: 0.0624
step 75510; step_loss: 0.0672
step 75520; step_loss: 0.0660
step 75530; step_loss: 0.0837
step 75540; step_loss: 0.0760
step 75550; step_loss: 0.0976
step 75560; step_loss: 0.0851
step 75570; step_loss: 0.0860
step 75580; step_loss: 0.0840
step 75590; step_loss: 0.0907
step 75600; step_loss: 0.0785
step 75610; step_loss: 0.0692
step 75620; step_loss: 0.0609
step 75630; step_loss: 0.0718
step 75640; step_loss: 0.0881
step 75650; step_loss: 0.0711
step 75660; step_loss: 0.0826
step 75670; step_loss: 0.0777
step 75680; step_loss: 0.1015
step 75690; step_loss: 0.0821
step 75700; step_loss: 0.0764
step 75710; step_loss: 0.0663
step 75720; step_loss: 0.0664
step 75730; step_loss: 0.0942
step 75740; step_loss: 0.0874
step 75750; step_loss: 0.0724
step 75760; step_loss: 0.1265
step 75770; step_loss: 0.1863
step 75780; step_loss: 0.0828
step 75790; step_loss: 0.1137
step 75800; step_loss: 0.0567
step 75810; step_loss: 0.1129
step 75820; step_loss: 0.0623
step 75830; step_loss: 0.0671
step 75840; step_loss: 0.0886
step 75850; step_loss: 0.0801
step 75860; step_loss: 0.0611
step 75870; step_loss: 0.0876
step 75880; step_loss: 0.0868
step 75890; step_loss: 0.0894
step 75900; step_loss: 0.0599
step 75910; step_loss: 0.0571
step 75920; step_loss: 0.1308
step 75930; step_loss: 0.1440
step 75940; step_loss: 0.0789
step 75950; step_loss: 0.0750
step 75960; step_loss: 0.0995
step 75970; step_loss: 0.1119
step 75980; step_loss: 0.0788
step 75990; step_loss: 0.0743

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 0.421 | 0.735 | 1.134 | 1.272 |   n/a |   n/a |

============================
Global step:         76000
Learning rate:       0.0035
Step-time (ms):     27.0738
Train loss avg:      0.0886
--------------------------
Val loss:            0.3975
srnn loss:           0.3819
============================

Saving the model...
done in 276.26 ms
step 76000; step_loss: 0.0725
step 76010; step_loss: 0.0665
step 76020; step_loss: 0.0620
step 76030; step_loss: 0.0638
step 76040; step_loss: 0.1171
step 76050; step_loss: 0.0963
step 76060; step_loss: 0.0752
step 76070; step_loss: 0.0579
step 76080; step_loss: 0.0637
step 76090; step_loss: 0.0743
step 76100; step_loss: 0.0859
step 76110; step_loss: 0.1055
step 76120; step_loss: 0.0578
step 76130; step_loss: 0.0817
step 76140; step_loss: 0.0697
step 76150; step_loss: 0.0656
step 76160; step_loss: 0.0979
step 76170; step_loss: 0.0816
step 76180; step_loss: 0.0680
step 76190; step_loss: 0.1385
step 76200; step_loss: 0.0625
step 76210; step_loss: 0.0730
step 76220; step_loss: 0.0988
step 76230; step_loss: 0.0785
step 76240; step_loss: 0.1134
step 76250; step_loss: 0.0810
step 76260; step_loss: 0.1163
step 76270; step_loss: 0.0622
step 76280; step_loss: 0.0761
step 76290; step_loss: 0.0661
step 76300; step_loss: 0.0782
step 76310; step_loss: 0.0834
step 76320; step_loss: 0.0834
step 76330; step_loss: 0.1472
step 76340; step_loss: 0.0723
step 76350; step_loss: 0.0715
step 76360; step_loss: 0.0563
step 76370; step_loss: 0.0922
step 76380; step_loss: 0.0948
step 76390; step_loss: 0.0944
step 76400; step_loss: 0.0635
step 76410; step_loss: 0.0615
step 76420; step_loss: 0.0817
step 76430; step_loss: 0.0687
step 76440; step_loss: 0.0653
step 76450; step_loss: 0.1234
step 76460; step_loss: 0.0816
step 76470; step_loss: 0.0985
step 76480; step_loss: 0.0766
step 76490; step_loss: 0.0646
step 76500; step_loss: 0.0679
step 76510; step_loss: 0.0725
step 76520; step_loss: 0.0749
step 76530; step_loss: 0.0960
step 76540; step_loss: 0.0880
step 76550; step_loss: 0.0974
step 76560; step_loss: 0.0839
step 76570; step_loss: 0.0872
step 76580; step_loss: 0.0797
step 76590; step_loss: 0.0815
step 76600; step_loss: 0.0954
step 76610; step_loss: 0.1723
step 76620; step_loss: 0.0890
step 76630; step_loss: 0.0878
step 76640; step_loss: 0.0726
step 76650; step_loss: 0.0553
step 76660; step_loss: 0.1222
step 76670; step_loss: 0.1238
step 76680; step_loss: 0.0629
step 76690; step_loss: 0.0604
step 76700; step_loss: 0.1177
step 76710; step_loss: 0.0940
step 76720; step_loss: 0.0787
step 76730; step_loss: 0.0723
step 76740; step_loss: 0.0695
step 76750; step_loss: 0.0892
step 76760; step_loss: 0.0885
step 76770; step_loss: 0.0999
step 76780; step_loss: 0.0804
step 76790; step_loss: 0.0801
step 76800; step_loss: 0.1189
step 76810; step_loss: 0.0853
step 76820; step_loss: 0.0914
step 76830; step_loss: 0.0699
step 76840; step_loss: 0.0920
step 76850; step_loss: 0.0629
step 76860; step_loss: 0.0782
step 76870; step_loss: 0.1390
step 76880; step_loss: 0.0755
step 76890; step_loss: 0.0891
step 76900; step_loss: 0.0885
step 76910; step_loss: 0.0825
step 76920; step_loss: 0.0850
step 76930; step_loss: 0.0851
step 76940; step_loss: 0.1024
step 76950; step_loss: 0.0792
step 76960; step_loss: 0.0823
step 76970; step_loss: 0.0822
step 76980; step_loss: 0.0635
step 76990; step_loss: 0.0821

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 0.427 | 0.743 | 1.146 | 1.290 |   n/a |   n/a |

============================
Global step:         77000
Learning rate:       0.0035
Step-time (ms):     27.0944
Train loss avg:      0.0893
--------------------------
Val loss:            0.4433
srnn loss:           0.3911
============================

Saving the model...
done in 279.09 ms
step 77000; step_loss: 0.0921
step 77010; step_loss: 0.0885
step 77020; step_loss: 0.0974
step 77030; step_loss: 0.0838
step 77040; step_loss: 0.0704
step 77050; step_loss: 0.0793
step 77060; step_loss: 0.0932
step 77070; step_loss: 0.0967
step 77080; step_loss: 0.0835
step 77090; step_loss: 0.1975
step 77100; step_loss: 0.0855
step 77110; step_loss: 0.0796
step 77120; step_loss: 0.0946
step 77130; step_loss: 0.1216
step 77140; step_loss: 0.1515
step 77150; step_loss: 0.0719
step 77160; step_loss: 0.0685
step 77170; step_loss: 0.0916
step 77180; step_loss: 0.0676
step 77190; step_loss: 0.1254
step 77200; step_loss: 0.0808
step 77210; step_loss: 0.0617
step 77220; step_loss: 0.0987
step 77230; step_loss: 0.1007
step 77240; step_loss: 0.0775
step 77250; step_loss: 0.0732
step 77260; step_loss: 0.0834
step 77270; step_loss: 0.0591
step 77280; step_loss: 0.0799
step 77290; step_loss: 0.1235
step 77300; step_loss: 0.0606
step 77310; step_loss: 0.1124
step 77320; step_loss: 0.0730
step 77330; step_loss: 0.0851
step 77340; step_loss: 0.0614
step 77350; step_loss: 0.0847
step 77360; step_loss: 0.1075
step 77370; step_loss: 0.0979
step 77380; step_loss: 0.1134
step 77390; step_loss: 0.0734
step 77400; step_loss: 0.1152
step 77410; step_loss: 0.0869
step 77420; step_loss: 0.0552
step 77430; step_loss: 0.0913
step 77440; step_loss: 0.0878
step 77450; step_loss: 0.0901
step 77460; step_loss: 0.0600
step 77470; step_loss: 0.0998
step 77480; step_loss: 0.2178
step 77490; step_loss: 0.0727
step 77500; step_loss: 0.0761
step 77510; step_loss: 0.1204
step 77520; step_loss: 0.0760
step 77530; step_loss: 0.0937
step 77540; step_loss: 0.0633
step 77550; step_loss: 0.0775
step 77560; step_loss: 0.0980
step 77570; step_loss: 0.1015
step 77580; step_loss: 0.0778
step 77590; step_loss: 0.1568
step 77600; step_loss: 0.0967
step 77610; step_loss: 0.0742
step 77620; step_loss: 0.0511
step 77630; step_loss: 0.0632
step 77640; step_loss: 0.0853
step 77650; step_loss: 0.0708
step 77660; step_loss: 0.0892
step 77670; step_loss: 0.0709
step 77680; step_loss: 0.0710
step 77690; step_loss: 0.0839
step 77700; step_loss: 0.1262
step 77710; step_loss: 0.0695
step 77720; step_loss: 0.0702
step 77730; step_loss: 0.1458
step 77740; step_loss: 0.0626
step 77750; step_loss: 0.0605
step 77760; step_loss: 0.0726
step 77770; step_loss: 0.0855
step 77780; step_loss: 0.0707
step 77790; step_loss: 0.0707
step 77800; step_loss: 0.1602
step 77810; step_loss: 0.1064
step 77820; step_loss: 0.0644
step 77830; step_loss: 0.0707
step 77840; step_loss: 0.0998
step 77850; step_loss: 0.0980
step 77860; step_loss: 0.0934
step 77870; step_loss: 0.0976
step 77880; step_loss: 0.0721
step 77890; step_loss: 0.0829
step 77900; step_loss: 0.1719
step 77910; step_loss: 0.0798
step 77920; step_loss: 0.0605
step 77930; step_loss: 0.1323
step 77940; step_loss: 0.0761
step 77950; step_loss: 0.0844
step 77960; step_loss: 0.0871
step 77970; step_loss: 0.0475
step 77980; step_loss: 0.0827
step 77990; step_loss: 0.1280

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 0.428 | 0.747 | 1.154 | 1.299 |   n/a |   n/a |

============================
Global step:         78000
Learning rate:       0.0035
Step-time (ms):     27.2251
Train loss avg:      0.0877
--------------------------
Val loss:            0.4199
srnn loss:           0.3799
============================

Saving the model...
done in 281.16 ms
step 78000; step_loss: 0.1187
step 78010; step_loss: 0.0718
step 78020; step_loss: 0.0745
step 78030; step_loss: 0.1001
step 78040; step_loss: 0.0936
step 78050; step_loss: 0.1270
step 78060; step_loss: 0.0912
step 78070; step_loss: 0.0944
step 78080; step_loss: 0.0964
step 78090; step_loss: 0.0915
step 78100; step_loss: 0.0872
step 78110; step_loss: 0.0849
step 78120; step_loss: 0.1500
step 78130; step_loss: 0.1181
step 78140; step_loss: 0.1146
step 78150; step_loss: 0.0883
step 78160; step_loss: 0.0718
step 78170; step_loss: 0.1311
step 78180; step_loss: 0.0961
step 78190; step_loss: 0.0712
step 78200; step_loss: 0.1438
step 78210; step_loss: 0.1208
step 78220; step_loss: 0.0845
step 78230; step_loss: 0.0764
step 78240; step_loss: 0.1170
step 78250; step_loss: 0.0754
step 78260; step_loss: 0.0634
step 78270; step_loss: 0.1116
step 78280; step_loss: 0.1053
step 78290; step_loss: 0.0535
step 78300; step_loss: 0.0894
step 78310; step_loss: 0.0742
step 78320; step_loss: 0.0624
step 78330; step_loss: 0.0439
step 78340; step_loss: 0.1269
step 78350; step_loss: 0.0654
step 78360; step_loss: 0.0708
step 78370; step_loss: 0.0695
step 78380; step_loss: 0.0797
step 78390; step_loss: 0.1002
step 78400; step_loss: 0.0756
step 78410; step_loss: 0.0781
step 78420; step_loss: 0.0702
step 78430; step_loss: 0.0789
step 78440; step_loss: 0.0841
step 78450; step_loss: 0.0917
step 78460; step_loss: 0.0912
step 78470; step_loss: 0.0855
step 78480; step_loss: 0.1066
step 78490; step_loss: 0.0903
step 78500; step_loss: 0.0828
step 78510; step_loss: 0.0764
step 78520; step_loss: 0.0746
step 78530; step_loss: 0.1010
step 78540; step_loss: 0.1119
step 78550; step_loss: 0.1264
step 78560; step_loss: 0.0871
step 78570; step_loss: 0.1122
step 78580; step_loss: 0.1091
step 78590; step_loss: 0.1147
step 78600; step_loss: 0.0899
step 78610; step_loss: 0.1130
step 78620; step_loss: 0.1019
step 78630; step_loss: 0.0976
step 78640; step_loss: 0.0705
step 78650; step_loss: 0.0678
step 78660; step_loss: 0.1603
step 78670; step_loss: 0.0812
step 78680; step_loss: 0.0641
step 78690; step_loss: 0.0819
step 78700; step_loss: 0.0790
step 78710; step_loss: 0.1910
step 78720; step_loss: 0.0956
step 78730; step_loss: 0.0862
step 78740; step_loss: 0.0588
step 78750; step_loss: 0.1169
step 78760; step_loss: 0.0918
step 78770; step_loss: 0.0454
step 78780; step_loss: 0.1728
step 78790; step_loss: 0.0780
step 78800; step_loss: 0.1456
step 78810; step_loss: 0.1375
step 78820; step_loss: 0.1066
step 78830; step_loss: 0.1004
step 78840; step_loss: 0.0894
step 78850; step_loss: 0.0924
step 78860; step_loss: 0.0782
step 78870; step_loss: 0.1047
step 78880; step_loss: 0.0834
step 78890; step_loss: 0.0597
step 78900; step_loss: 0.1221
step 78910; step_loss: 0.1079
step 78920; step_loss: 0.0837
step 78930; step_loss: 0.0608
step 78940; step_loss: 0.1067
step 78950; step_loss: 0.1109
step 78960; step_loss: 0.0870
step 78970; step_loss: 0.1162
step 78980; step_loss: 0.1861
step 78990; step_loss: 0.1003

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 0.430 | 0.751 | 1.153 | 1.295 |   n/a |   n/a |

============================
Global step:         79000
Learning rate:       0.0035
Step-time (ms):     27.1252
Train loss avg:      0.0883
--------------------------
Val loss:            0.4635
srnn loss:           0.3941
============================

Saving the model...
done in 272.83 ms
step 79000; step_loss: 0.0772
step 79010; step_loss: 0.0820
step 79020; step_loss: 0.0798
step 79030; step_loss: 0.0804
step 79040; step_loss: 0.1062
step 79050; step_loss: 0.0635
step 79060; step_loss: 0.1037
step 79070; step_loss: 0.1003
step 79080; step_loss: 0.0771
step 79090; step_loss: 0.0598
step 79100; step_loss: 0.0841
step 79110; step_loss: 0.0681
step 79120; step_loss: 0.1352
step 79130; step_loss: 0.0719
step 79140; step_loss: 0.0653
step 79150; step_loss: 0.1065
step 79160; step_loss: 0.0735
step 79170; step_loss: 0.1891
step 79180; step_loss: 0.0716
step 79190; step_loss: 0.0752
step 79200; step_loss: 0.0585
step 79210; step_loss: 0.0821
step 79220; step_loss: 0.0813
step 79230; step_loss: 0.0931
step 79240; step_loss: 0.0963
step 79250; step_loss: 0.1558
step 79260; step_loss: 0.0758
step 79270; step_loss: 0.0631
step 79280; step_loss: 0.0821
step 79290; step_loss: 0.0481
step 79300; step_loss: 0.0621
step 79310; step_loss: 0.0536
step 79320; step_loss: 0.0909
step 79330; step_loss: 0.0690
step 79340; step_loss: 0.0749
step 79350; step_loss: 0.0623
step 79360; step_loss: 0.0875
step 79370; step_loss: 0.0805
step 79380; step_loss: 0.2123
step 79390; step_loss: 0.0747
step 79400; step_loss: 0.0793
step 79410; step_loss: 0.0804
step 79420; step_loss: 0.0573
step 79430; step_loss: 0.0712
step 79440; step_loss: 0.1120
step 79450; step_loss: 0.0695
step 79460; step_loss: 0.0976
step 79470; step_loss: 0.0932
step 79480; step_loss: 0.0928
step 79490; step_loss: 0.0572
step 79500; step_loss: 0.1331
step 79510; step_loss: 0.0680
step 79520; step_loss: 0.1212
step 79530; step_loss: 0.1759
step 79540; step_loss: 0.0929
step 79550; step_loss: 0.0886
step 79560; step_loss: 0.1171
step 79570; step_loss: 0.0749
step 79580; step_loss: 0.0818
step 79590; step_loss: 0.0934
step 79600; step_loss: 0.0455
step 79610; step_loss: 0.0799
step 79620; step_loss: 0.0821
step 79630; step_loss: 0.0874
step 79640; step_loss: 0.0701
step 79650; step_loss: 0.0721
step 79660; step_loss: 0.0692
step 79670; step_loss: 0.0730
step 79680; step_loss: 0.0925
step 79690; step_loss: 0.0934
step 79700; step_loss: 0.0917
step 79710; step_loss: 0.0715
step 79720; step_loss: 0.0643
step 79730; step_loss: 0.0850
step 79740; step_loss: 0.1004
step 79750; step_loss: 0.0806
step 79760; step_loss: 0.1436
step 79770; step_loss: 0.1196
step 79780; step_loss: 0.1134
step 79790; step_loss: 0.0664
step 79800; step_loss: 0.0766
step 79810; step_loss: 0.1147
step 79820; step_loss: 0.0866
step 79830; step_loss: 0.1450
step 79840; step_loss: 0.0816
step 79850; step_loss: 0.0642
step 79860; step_loss: 0.0801
step 79870; step_loss: 0.0868
step 79880; step_loss: 0.0683
step 79890; step_loss: 0.0733
step 79900; step_loss: 0.1156
step 79910; step_loss: 0.0709
step 79920; step_loss: 0.0727
step 79930; step_loss: 0.1118
step 79940; step_loss: 0.0708
step 79950; step_loss: 0.0980
step 79960; step_loss: 0.0951
step 79970; step_loss: 0.0754
step 79980; step_loss: 0.0639
step 79990; step_loss: 0.0765

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 0.426 | 0.742 | 1.138 | 1.275 |   n/a |   n/a |

============================
Global step:         80000
Learning rate:       0.0033
Step-time (ms):     27.0785
Train loss avg:      0.0869
--------------------------
Val loss:            0.4420
srnn loss:           0.3875
============================

Saving the model...
done in 272.76 ms
step 80000; step_loss: 0.0821
step 80010; step_loss: 0.0559
step 80020; step_loss: 0.1226
step 80030; step_loss: 0.0690
step 80040; step_loss: 0.0748
step 80050; step_loss: 0.0677
step 80060; step_loss: 0.0808
step 80070; step_loss: 0.1028
step 80080; step_loss: 0.0948
step 80090; step_loss: 0.0715
step 80100; step_loss: 0.0670
step 80110; step_loss: 0.0627
step 80120; step_loss: 0.0954
step 80130; step_loss: 0.0895
step 80140; step_loss: 0.0592
step 80150; step_loss: 0.0665
step 80160; step_loss: 0.0658
step 80170; step_loss: 0.0885
step 80180; step_loss: 0.0827
step 80190; step_loss: 0.0959
step 80200; step_loss: 0.0814
step 80210; step_loss: 0.0838
step 80220; step_loss: 0.0905
step 80230; step_loss: 0.0866
step 80240; step_loss: 0.0906
step 80250; step_loss: 0.1106
step 80260; step_loss: 0.0634
step 80270; step_loss: 0.0798
step 80280; step_loss: 0.0672
step 80290; step_loss: 0.1040
step 80300; step_loss: 0.0729
step 80310; step_loss: 0.0877
step 80320; step_loss: 0.0737
step 80330; step_loss: 0.1007
step 80340; step_loss: 0.0652
step 80350; step_loss: 0.0887
step 80360; step_loss: 0.1159
step 80370; step_loss: 0.1820
step 80380; step_loss: 0.0716
step 80390; step_loss: 0.0740
step 80400; step_loss: 0.0687
step 80410; step_loss: 0.0766
step 80420; step_loss: 0.1106
step 80430; step_loss: 0.0613
step 80440; step_loss: 0.1005
step 80450; step_loss: 0.0921
step 80460; step_loss: 0.0770
step 80470; step_loss: 0.0710
step 80480; step_loss: 0.0715
step 80490; step_loss: 0.0604
step 80500; step_loss: 0.0705
step 80510; step_loss: 0.0618
step 80520; step_loss: 0.0570
step 80530; step_loss: 0.0641
step 80540; step_loss: 0.0641
step 80550; step_loss: 0.0955
step 80560; step_loss: 0.1088
step 80570; step_loss: 0.0681
step 80580; step_loss: 0.0653
step 80590; step_loss: 0.1059
step 80600; step_loss: 0.0714
step 80610; step_loss: 0.0538
step 80620; step_loss: 0.1311
step 80630; step_loss: 0.0701
step 80640; step_loss: 0.0824
step 80650; step_loss: 0.0660
step 80660; step_loss: 0.0651
step 80670; step_loss: 0.0785
step 80680; step_loss: 0.0875
step 80690; step_loss: 0.0832
step 80700; step_loss: 0.0782
step 80710; step_loss: 0.0778
step 80720; step_loss: 0.0911
step 80730; step_loss: 0.0777
step 80740; step_loss: 0.0903
step 80750; step_loss: 0.0798
step 80760; step_loss: 0.1171
step 80770; step_loss: 0.0780
step 80780; step_loss: 0.0560
step 80790; step_loss: 0.0513
step 80800; step_loss: 0.0693
step 80810; step_loss: 0.0871
step 80820; step_loss: 0.1161
step 80830; step_loss: 0.1104
step 80840; step_loss: 0.1080
step 80850; step_loss: 0.0830
step 80860; step_loss: 0.0721
step 80870; step_loss: 0.0834
step 80880; step_loss: 0.0759
step 80890; step_loss: 0.0795
step 80900; step_loss: 0.0738
step 80910; step_loss: 0.0708
step 80920; step_loss: 0.0594
step 80930; step_loss: 0.0941
step 80940; step_loss: 0.0949
step 80950; step_loss: 0.0933
step 80960; step_loss: 0.0811
step 80970; step_loss: 0.0683
step 80980; step_loss: 0.0726
step 80990; step_loss: 0.0906

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 0.431 | 0.753 | 1.163 | 1.308 |   n/a |   n/a |

============================
Global step:         81000
Learning rate:       0.0033
Step-time (ms):     27.1271
Train loss avg:      0.0870
--------------------------
Val loss:            0.4917
srnn loss:           0.3857
============================

Saving the model...
done in 276.51 ms
step 81000; step_loss: 0.0832
step 81010; step_loss: 0.1151
step 81020; step_loss: 0.0479
step 81030; step_loss: 0.1191
step 81040; step_loss: 0.0641
step 81050; step_loss: 0.0760
step 81060; step_loss: 0.0629
step 81070; step_loss: 0.0586
step 81080; step_loss: 0.0639
step 81090; step_loss: 0.1051
step 81100; step_loss: 0.0755
step 81110; step_loss: 0.0870
step 81120; step_loss: 0.0565
step 81130; step_loss: 0.1026
step 81140; step_loss: 0.0672
step 81150; step_loss: 0.1001
step 81160; step_loss: 0.0789
step 81170; step_loss: 0.0965
step 81180; step_loss: 0.0997
step 81190; step_loss: 0.0828
step 81200; step_loss: 0.0864
step 81210; step_loss: 0.0856
step 81220; step_loss: 0.1040
step 81230; step_loss: 0.0748
step 81240; step_loss: 0.1019
step 81250; step_loss: 0.1004
step 81260; step_loss: 0.1296
step 81270; step_loss: 0.1311
step 81280; step_loss: 0.0746
step 81290; step_loss: 0.0750
step 81300; step_loss: 0.0868
step 81310; step_loss: 0.0788
step 81320; step_loss: 0.0762
step 81330; step_loss: 0.0847
step 81340; step_loss: 0.0601
step 81350; step_loss: 0.0580
step 81360; step_loss: 0.1854
step 81370; step_loss: 0.0874
step 81380; step_loss: 0.0766
step 81390; step_loss: 0.0561
step 81400; step_loss: 0.0656
step 81410; step_loss: 0.0688
step 81420; step_loss: 0.0737
step 81430; step_loss: 0.1350
step 81440; step_loss: 0.0975
step 81450; step_loss: 0.0765
step 81460; step_loss: 0.0743
step 81470; step_loss: 0.0654
step 81480; step_loss: 0.0669
step 81490; step_loss: 0.0913
step 81500; step_loss: 0.0598
step 81510; step_loss: 0.0685
step 81520; step_loss: 0.0712
step 81530; step_loss: 0.0782
step 81540; step_loss: 0.0691
step 81550; step_loss: 0.0924
step 81560; step_loss: 0.0688
step 81570; step_loss: 0.0994
step 81580; step_loss: 0.0696
step 81590; step_loss: 0.0678
step 81600; step_loss: 0.1476
step 81610; step_loss: 0.0570
step 81620; step_loss: 0.1838
step 81630; step_loss: 0.1018
step 81640; step_loss: 0.0569
step 81650; step_loss: 0.0846
step 81660; step_loss: 0.0796
step 81670; step_loss: 0.0869
step 81680; step_loss: 0.0630
step 81690; step_loss: 0.0867
step 81700; step_loss: 0.0973
step 81710; step_loss: 0.0785
step 81720; step_loss: 0.0667
step 81730; step_loss: 0.0557
step 81740; step_loss: 0.0558
step 81750; step_loss: 0.0652
step 81760; step_loss: 0.0869
step 81770; step_loss: 0.0856
step 81780; step_loss: 0.0971
step 81790; step_loss: 0.1018
step 81800; step_loss: 0.0982
step 81810; step_loss: 0.2689
step 81820; step_loss: 0.0627
step 81830; step_loss: 0.0704
step 81840; step_loss: 0.0655
step 81850; step_loss: 0.0656
step 81860; step_loss: 0.0740
step 81870; step_loss: 0.0752
step 81880; step_loss: 0.1161
step 81890; step_loss: 0.1011
step 81900; step_loss: 0.0869
step 81910; step_loss: 0.0668
step 81920; step_loss: 0.0921
step 81930; step_loss: 0.1033
step 81940; step_loss: 0.0722
step 81950; step_loss: 0.0819
step 81960; step_loss: 0.0830
step 81970; step_loss: 0.1026
step 81980; step_loss: 0.1123
step 81990; step_loss: 0.0582

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 0.432 | 0.753 | 1.166 | 1.313 |   n/a |   n/a |

============================
Global step:         82000
Learning rate:       0.0033
Step-time (ms):     27.1349
Train loss avg:      0.0882
--------------------------
Val loss:            0.4433
srnn loss:           0.3894
============================

Saving the model...
done in 272.72 ms
step 82000; step_loss: 0.0658
step 82010; step_loss: 0.1075
step 82020; step_loss: 0.0743
step 82030; step_loss: 0.0949
step 82040; step_loss: 0.0876
step 82050; step_loss: 0.0681
step 82060; step_loss: 0.0724
step 82070; step_loss: 0.0633
step 82080; step_loss: 0.0738
step 82090; step_loss: 0.0823
step 82100; step_loss: 0.0822
step 82110; step_loss: 0.0563
step 82120; step_loss: 0.1225
step 82130; step_loss: 0.0523
step 82140; step_loss: 0.1165
step 82150; step_loss: 0.1060
step 82160; step_loss: 0.1317
step 82170; step_loss: 0.0568
step 82180; step_loss: 0.0566
step 82190; step_loss: 0.0952
step 82200; step_loss: 0.0699
step 82210; step_loss: 0.0752
step 82220; step_loss: 0.0922
step 82230; step_loss: 0.0746
step 82240; step_loss: 0.0767
step 82250; step_loss: 0.0809
step 82260; step_loss: 0.0676
step 82270; step_loss: 0.0971
step 82280; step_loss: 0.0912
step 82290; step_loss: 0.0720
step 82300; step_loss: 0.0842
step 82310; step_loss: 0.0889
step 82320; step_loss: 0.0767
step 82330; step_loss: 0.0695
step 82340; step_loss: 0.2086
step 82350; step_loss: 0.1000
step 82360; step_loss: 0.1403
step 82370; step_loss: 0.0768
step 82380; step_loss: 0.0590
step 82390; step_loss: 0.0775
step 82400; step_loss: 0.0863
step 82410; step_loss: 0.1239
step 82420; step_loss: 0.0878
step 82430; step_loss: 0.1149
step 82440; step_loss: 0.0900
step 82450; step_loss: 0.1127
step 82460; step_loss: 0.1000
step 82470; step_loss: 0.2110
step 82480; step_loss: 0.0738
step 82490; step_loss: 0.0667
step 82500; step_loss: 0.0711
step 82510; step_loss: 0.1021
step 82520; step_loss: 0.0883
step 82530; step_loss: 0.0569
step 82540; step_loss: 0.0603
step 82550; step_loss: 0.0968
step 82560; step_loss: 0.0837
step 82570; step_loss: 0.1032
step 82580; step_loss: 0.0835
step 82590; step_loss: 0.1118
step 82600; step_loss: 0.0717
step 82610; step_loss: 0.0770
step 82620; step_loss: 0.0955
step 82630; step_loss: 0.0824
step 82640; step_loss: 0.0785
step 82650; step_loss: 0.0989
step 82660; step_loss: 0.0629
step 82670; step_loss: 0.1320
step 82680; step_loss: 0.0994
step 82690; step_loss: 0.0870
step 82700; step_loss: 0.1505
step 82710; step_loss: 0.0688
step 82720; step_loss: 0.0837
step 82730; step_loss: 0.0752
step 82740; step_loss: 0.1386
step 82750; step_loss: 0.0771
step 82760; step_loss: 0.0890
step 82770; step_loss: 0.0723
step 82780; step_loss: 0.1027
step 82790; step_loss: 0.0740
step 82800; step_loss: 0.1015
step 82810; step_loss: 0.0842
step 82820; step_loss: 0.1319
step 82830; step_loss: 0.0689
step 82840; step_loss: 0.0607
step 82850; step_loss: 0.1067
step 82860; step_loss: 0.0956
step 82870; step_loss: 0.0728
step 82880; step_loss: 0.1202
step 82890; step_loss: 0.0839
step 82900; step_loss: 0.1144
step 82910; step_loss: 0.1001
step 82920; step_loss: 0.0533
step 82930; step_loss: 0.0682
step 82940; step_loss: 0.0956
step 82950; step_loss: 0.0640
step 82960; step_loss: 0.0969
step 82970; step_loss: 0.0696
step 82980; step_loss: 0.0850
step 82990; step_loss: 0.0587

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 0.431 | 0.752 | 1.164 | 1.311 |   n/a |   n/a |

============================
Global step:         83000
Learning rate:       0.0033
Step-time (ms):     27.1781
Train loss avg:      0.0866
--------------------------
Val loss:            0.4392
srnn loss:           0.3878
============================

Saving the model...
done in 278.35 ms
step 83000; step_loss: 0.0766
step 83010; step_loss: 0.0615
step 83020; step_loss: 0.1058
step 83030; step_loss: 0.0854
step 83040; step_loss: 0.1599
step 83050; step_loss: 0.1053
step 83060; step_loss: 0.1048
step 83070; step_loss: 0.0655
step 83080; step_loss: 0.1040
step 83090; step_loss: 0.0553
step 83100; step_loss: 0.0541
step 83110; step_loss: 0.0945
step 83120; step_loss: 0.0739
step 83130; step_loss: 0.0861
step 83140; step_loss: 0.0690
step 83150; step_loss: 0.0802
step 83160; step_loss: 0.0707
step 83170; step_loss: 0.0529
step 83180; step_loss: 0.1303
step 83190; step_loss: 0.0809
step 83200; step_loss: 0.0761
step 83210; step_loss: 0.0829
step 83220; step_loss: 0.0797
step 83230; step_loss: 0.1271
step 83240; step_loss: 0.0806
step 83250; step_loss: 0.0741
step 83260; step_loss: 0.0778
step 83270; step_loss: 0.0891
step 83280; step_loss: 0.0695
step 83290; step_loss: 0.0944
step 83300; step_loss: 0.0865
step 83310; step_loss: 0.0631
step 83320; step_loss: 0.0932
step 83330; step_loss: 0.1039
step 83340; step_loss: 0.0695
step 83350; step_loss: 0.0701
step 83360; step_loss: 0.0850
step 83370; step_loss: 0.0745
step 83380; step_loss: 0.1132
step 83390; step_loss: 0.0531
step 83400; step_loss: 0.1499
step 83410; step_loss: 0.0604
step 83420; step_loss: 0.0800
step 83430; step_loss: 0.0976
step 83440; step_loss: 0.0842
step 83450; step_loss: 0.0767
step 83460; step_loss: 0.1161
step 83470; step_loss: 0.1954
step 83480; step_loss: 0.0776
step 83490; step_loss: 0.0691
step 83500; step_loss: 0.0560
step 83510; step_loss: 0.0704
step 83520; step_loss: 0.1124
step 83530; step_loss: 0.0867
step 83540; step_loss: 0.0691
step 83550; step_loss: 0.0945
step 83560; step_loss: 0.0741
step 83570; step_loss: 0.1859
step 83580; step_loss: 0.1015
step 83590; step_loss: 0.0551
step 83600; step_loss: 0.0815
step 83610; step_loss: 0.0756
step 83620; step_loss: 0.0982
step 83630; step_loss: 0.0596
step 83640; step_loss: 0.0984
step 83650; step_loss: 0.0831
step 83660; step_loss: 0.0962
step 83670; step_loss: 0.0777
step 83680; step_loss: 0.0496
step 83690; step_loss: 0.0749
step 83700; step_loss: 0.0582
step 83710; step_loss: 0.0866
step 83720; step_loss: 0.0778
step 83730; step_loss: 0.0970
step 83740; step_loss: 0.0742
step 83750; step_loss: 0.0869
step 83760; step_loss: 0.0975
step 83770; step_loss: 0.0566
step 83780; step_loss: 0.0905
step 83790; step_loss: 0.0636
step 83800; step_loss: 0.0681
step 83810; step_loss: 0.0742
step 83820; step_loss: 0.1094
step 83830; step_loss: 0.0740
step 83840; step_loss: 0.1149
step 83850; step_loss: 0.0832
step 83860; step_loss: 0.0618
step 83870; step_loss: 0.0892
step 83880; step_loss: 0.0770
step 83890; step_loss: 0.0763
step 83900; step_loss: 0.0799
step 83910; step_loss: 0.0748
step 83920; step_loss: 0.0702
step 83930; step_loss: 0.1008
step 83940; step_loss: 0.0719
step 83950; step_loss: 0.0744
step 83960; step_loss: 0.0965
step 83970; step_loss: 0.0746
step 83980; step_loss: 0.0844
step 83990; step_loss: 0.0711

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 0.430 | 0.750 | 1.159 | 1.307 |   n/a |   n/a |

============================
Global step:         84000
Learning rate:       0.0033
Step-time (ms):     27.1575
Train loss avg:      0.0857
--------------------------
Val loss:            0.4346
srnn loss:           0.3922
============================

Saving the model...
done in 270.65 ms
step 84000; step_loss: 0.1202
step 84010; step_loss: 0.0962
step 84020; step_loss: 0.0816
step 84030; step_loss: 0.0944
step 84040; step_loss: 0.0959
step 84050; step_loss: 0.0561
step 84060; step_loss: 0.0633
step 84070; step_loss: 0.0476
step 84080; step_loss: 0.0812
step 84090; step_loss: 0.0686
step 84100; step_loss: 0.0950
step 84110; step_loss: 0.0627
step 84120; step_loss: 0.1093
step 84130; step_loss: 0.0734
step 84140; step_loss: 0.0798
step 84150; step_loss: 0.0738
step 84160; step_loss: 0.1315
step 84170; step_loss: 0.0583
step 84180; step_loss: 0.1051
step 84190; step_loss: 0.0736
step 84200; step_loss: 0.0685
step 84210; step_loss: 0.1119
step 84220; step_loss: 0.0934
step 84230; step_loss: 0.0836
step 84240; step_loss: 0.0781
step 84250; step_loss: 0.0936
step 84260; step_loss: 0.0998
step 84270; step_loss: 0.0998
step 84280; step_loss: 0.1382
step 84290; step_loss: 0.0833
step 84300; step_loss: 0.0736
step 84310; step_loss: 0.0890
step 84320; step_loss: 0.0748
step 84330; step_loss: 0.0622
step 84340; step_loss: 0.0618
step 84350; step_loss: 0.0915
step 84360; step_loss: 0.0959
step 84370; step_loss: 0.1041
step 84380; step_loss: 0.0702
step 84390; step_loss: 0.0729
step 84400; step_loss: 0.2305
step 84410; step_loss: 0.0755
step 84420; step_loss: 0.0704
step 84430; step_loss: 0.0659
step 84440; step_loss: 0.1902
step 84450; step_loss: 0.0602
step 84460; step_loss: 0.0635
step 84470; step_loss: 0.1069
step 84480; step_loss: 0.0604
step 84490; step_loss: 0.0850
step 84500; step_loss: 0.0802
step 84510; step_loss: 0.0765
step 84520; step_loss: 0.0799
step 84530; step_loss: 0.0665
step 84540; step_loss: 0.0978
step 84550; step_loss: 0.0672
step 84560; step_loss: 0.0854
step 84570; step_loss: 0.0701
step 84580; step_loss: 0.0806
step 84590; step_loss: 0.0779
step 84600; step_loss: 0.1224
step 84610; step_loss: 0.0751
step 84620; step_loss: 0.1012
step 84630; step_loss: 0.1174
step 84640; step_loss: 0.0864
step 84650; step_loss: 0.1016
step 84660; step_loss: 0.0787
step 84670; step_loss: 0.0596
step 84680; step_loss: 0.0615
step 84690; step_loss: 0.0670
step 84700; step_loss: 0.0813
step 84710; step_loss: 0.0766
step 84720; step_loss: 0.0598
step 84730; step_loss: 0.0599
step 84740; step_loss: 0.1026
step 84750; step_loss: 0.0639
step 84760; step_loss: 0.0772
step 84770; step_loss: 0.1111
step 84780; step_loss: 0.0757
step 84790; step_loss: 0.0594
step 84800; step_loss: 0.0889
step 84810; step_loss: 0.0723
step 84820; step_loss: 0.1331
step 84830; step_loss: 0.0681
step 84840; step_loss: 0.0963
step 84850; step_loss: 0.0606
step 84860; step_loss: 0.0776
step 84870; step_loss: 0.1504
step 84880; step_loss: 0.0848
step 84890; step_loss: 0.0834
step 84900; step_loss: 0.0963
step 84910; step_loss: 0.0476
step 84920; step_loss: 0.0677
step 84930; step_loss: 0.0703
step 84940; step_loss: 0.1038
step 84950; step_loss: 0.0754
step 84960; step_loss: 0.0877
step 84970; step_loss: 0.0951
step 84980; step_loss: 0.1033
step 84990; step_loss: 0.0657

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 0.436 | 0.758 | 1.171 | 1.314 |   n/a |   n/a |

============================
Global step:         85000
Learning rate:       0.0033
Step-time (ms):     27.1066
Train loss avg:      0.0858
--------------------------
Val loss:            0.4311
srnn loss:           0.3974
============================

Saving the model...
done in 282.15 ms
step 85000; step_loss: 0.0638
step 85010; step_loss: 0.0866
step 85020; step_loss: 0.0569
step 85030; step_loss: 0.1105
step 85040; step_loss: 0.0596
step 85050; step_loss: 0.0801
step 85060; step_loss: 0.0838
step 85070; step_loss: 0.0982
step 85080; step_loss: 0.0736
step 85090; step_loss: 0.0669
step 85100; step_loss: 0.0844
step 85110; step_loss: 0.0915
step 85120; step_loss: 0.1088
step 85130; step_loss: 0.0749
step 85140; step_loss: 0.0849
step 85150; step_loss: 0.0946
step 85160; step_loss: 0.0772
step 85170; step_loss: 0.0627
step 85180; step_loss: 0.0671
step 85190; step_loss: 0.0962
step 85200; step_loss: 0.0776
step 85210; step_loss: 0.0873
step 85220; step_loss: 0.0846
step 85230; step_loss: 0.0893
step 85240; step_loss: 0.0767
step 85250; step_loss: 0.1445
step 85260; step_loss: 0.0693
step 85270; step_loss: 0.1856
step 85280; step_loss: 0.0725
step 85290; step_loss: 0.0689
step 85300; step_loss: 0.0948
step 85310; step_loss: 0.0674
step 85320; step_loss: 0.1350
step 85330; step_loss: 0.1278
step 85340; step_loss: 0.0714
step 85350; step_loss: 0.0605
step 85360; step_loss: 0.1588
step 85370; step_loss: 0.0936
step 85380; step_loss: 0.0840
step 85390; step_loss: 0.0588
step 85400; step_loss: 0.0670
step 85410; step_loss: 0.0906
step 85420; step_loss: 0.0969
step 85430; step_loss: 0.1033
step 85440; step_loss: 0.2211
step 85450; step_loss: 0.0660
step 85460; step_loss: 0.1265
step 85470; step_loss: 0.0914
step 85480; step_loss: 0.0814
step 85490; step_loss: 0.0832
step 85500; step_loss: 0.0497
step 85510; step_loss: 0.0835
step 85520; step_loss: 0.0847
step 85530; step_loss: 0.0737
step 85540; step_loss: 0.0794
step 85550; step_loss: 0.0750
step 85560; step_loss: 0.0738
step 85570; step_loss: 0.0926
step 85580; step_loss: 0.0572
step 85590; step_loss: 0.0767
step 85600; step_loss: 0.1134
step 85610; step_loss: 0.0747
step 85620; step_loss: 0.0792
step 85630; step_loss: 0.0958
step 85640; step_loss: 0.0728
step 85650; step_loss: 0.0895
step 85660; step_loss: 0.0822
step 85670; step_loss: 0.1055
step 85680; step_loss: 0.0949
step 85690; step_loss: 0.0807
step 85700; step_loss: 0.0744
step 85710; step_loss: 0.0593
step 85720; step_loss: 0.0926
step 85730; step_loss: 0.0732
step 85740; step_loss: 0.0960
step 85750; step_loss: 0.0710
step 85760; step_loss: 0.0650
step 85770; step_loss: 0.0730
step 85780; step_loss: 0.0997
step 85790; step_loss: 0.0764
step 85800; step_loss: 0.0745
step 85810; step_loss: 0.1179
step 85820; step_loss: 0.0786
step 85830; step_loss: 0.0767
step 85840; step_loss: 0.0660
step 85850; step_loss: 0.0756
step 85860; step_loss: 0.0768
step 85870; step_loss: 0.0980
step 85880; step_loss: 0.1269
step 85890; step_loss: 0.1267
step 85900; step_loss: 0.0706
step 85910; step_loss: 0.0774
step 85920; step_loss: 0.0860
step 85930; step_loss: 0.0687
step 85940; step_loss: 0.0631
step 85950; step_loss: 0.0694
step 85960; step_loss: 0.0662
step 85970; step_loss: 0.1443
step 85980; step_loss: 0.0833
step 85990; step_loss: 0.0657

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 0.439 | 0.764 | 1.173 | 1.315 |   n/a |   n/a |

============================
Global step:         86000
Learning rate:       0.0033
Step-time (ms):     27.0929
Train loss avg:      0.0849
--------------------------
Val loss:            0.4162
srnn loss:           0.3969
============================

Saving the model...
done in 284.05 ms
step 86000; step_loss: 0.0553
step 86010; step_loss: 0.0757
step 86020; step_loss: 0.1070
step 86030; step_loss: 0.0761
step 86040; step_loss: 0.0908
step 86050; step_loss: 0.0736
step 86060; step_loss: 0.0573
step 86070; step_loss: 0.1526
step 86080; step_loss: 0.0704
step 86090; step_loss: 0.0624
step 86100; step_loss: 0.0851
step 86110; step_loss: 0.0792
step 86120; step_loss: 0.1020
step 86130; step_loss: 0.1138
step 86140; step_loss: 0.0677
step 86150; step_loss: 0.0834
step 86160; step_loss: 0.0746
step 86170; step_loss: 0.1223
step 86180; step_loss: 0.0948
step 86190; step_loss: 0.0944
step 86200; step_loss: 0.0706
step 86210; step_loss: 0.0749
step 86220; step_loss: 0.0611
step 86230; step_loss: 0.0680
step 86240; step_loss: 0.0639
step 86250; step_loss: 0.0701
step 86260; step_loss: 0.0672
step 86270; step_loss: 0.0585
step 86280; step_loss: 0.1259
step 86290; step_loss: 0.1067
step 86300; step_loss: 0.1213
step 86310; step_loss: 0.0806
step 86320; step_loss: 0.0835
step 86330; step_loss: 0.0772
step 86340; step_loss: 0.0963
step 86350; step_loss: 0.1127
step 86360; step_loss: 0.0805
step 86370; step_loss: 0.0869
step 86380; step_loss: 0.0685
step 86390; step_loss: 0.1003
step 86400; step_loss: 0.0729
step 86410; step_loss: 0.0801
step 86420; step_loss: 0.0751
step 86430; step_loss: 0.0608
step 86440; step_loss: 0.0537
step 86450; step_loss: 0.0685
step 86460; step_loss: 0.1132
step 86470; step_loss: 0.0624
step 86480; step_loss: 0.0760
step 86490; step_loss: 0.0900
step 86500; step_loss: 0.0739
step 86510; step_loss: 0.0771
step 86520; step_loss: 0.1061
step 86530; step_loss: 0.0734
step 86540; step_loss: 0.0715
step 86550; step_loss: 0.0653
step 86560; step_loss: 0.0627
step 86570; step_loss: 0.0779
step 86580; step_loss: 0.0727
step 86590; step_loss: 0.0487
step 86600; step_loss: 0.1912
step 86610; step_loss: 0.1299
step 86620; step_loss: 0.0692
step 86630; step_loss: 0.0873
step 86640; step_loss: 0.1011
step 86650; step_loss: 0.0779
step 86660; step_loss: 0.0746
step 86670; step_loss: 0.0639
step 86680; step_loss: 0.1016
step 86690; step_loss: 0.0679
step 86700; step_loss: 0.0583
step 86710; step_loss: 0.1618
step 86720; step_loss: 0.1032
step 86730; step_loss: 0.0667
step 86740; step_loss: 0.1270
step 86750; step_loss: 0.0752
step 86760; step_loss: 0.0796
step 86770; step_loss: 0.0587
step 86780; step_loss: 0.0635
step 86790; step_loss: 0.0569
step 86800; step_loss: 0.0667
step 86810; step_loss: 0.0588
step 86820; step_loss: 0.1061
step 86830; step_loss: 0.0656
step 86840; step_loss: 0.0562
step 86850; step_loss: 0.0798
step 86860; step_loss: 0.0736
step 86870; step_loss: 0.0947
step 86880; step_loss: 0.0820
step 86890; step_loss: 0.0658
step 86900; step_loss: 0.0653
step 86910; step_loss: 0.0845
step 86920; step_loss: 0.0972
step 86930; step_loss: 0.0654
step 86940; step_loss: 0.0838
step 86950; step_loss: 0.0797
step 86960; step_loss: 0.0813
step 86970; step_loss: 0.0685
step 86980; step_loss: 0.0680
step 86990; step_loss: 0.0670

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 0.428 | 0.743 | 1.142 | 1.283 |   n/a |   n/a |

============================
Global step:         87000
Learning rate:       0.0033
Step-time (ms):     27.0471
Train loss avg:      0.0845
--------------------------
Val loss:            0.4784
srnn loss:           0.3862
============================

Saving the model...
done in 275.48 ms
step 87000; step_loss: 0.0784
step 87010; step_loss: 0.1180
step 87020; step_loss: 0.0882
step 87030; step_loss: 0.0883
step 87040; step_loss: 0.0628
step 87050; step_loss: 0.1265
step 87060; step_loss: 0.1431
step 87070; step_loss: 0.0858
step 87080; step_loss: 0.0636
step 87090; step_loss: 0.0584
step 87100; step_loss: 0.0851
step 87110; step_loss: 0.0706
step 87120; step_loss: 0.1556
step 87130; step_loss: 0.0995
step 87140; step_loss: 0.0558
step 87150; step_loss: 0.1755
step 87160; step_loss: 0.1638
step 87170; step_loss: 0.1323
step 87180; step_loss: 0.0721
step 87190; step_loss: 0.0640
step 87200; step_loss: 0.0905
step 87210; step_loss: 0.0588
step 87220; step_loss: 0.0531
step 87230; step_loss: 0.0729
step 87240; step_loss: 0.0635
step 87250; step_loss: 0.0531
step 87260; step_loss: 0.0627
step 87270; step_loss: 0.0789
step 87280; step_loss: 0.0792
step 87290; step_loss: 0.0985
step 87300; step_loss: 0.0690
step 87310; step_loss: 0.1651
step 87320; step_loss: 0.0644
step 87330; step_loss: 0.0626
step 87340; step_loss: 0.0654
step 87350; step_loss: 0.0630
step 87360; step_loss: 0.0798
step 87370; step_loss: 0.0826
step 87380; step_loss: 0.0760
step 87390; step_loss: 0.0523
step 87400; step_loss: 0.0896
step 87410; step_loss: 0.0733
step 87420; step_loss: 0.0845
step 87430; step_loss: 0.0682
step 87440; step_loss: 0.0625
step 87450; step_loss: 0.1082
step 87460; step_loss: 0.0803
step 87470; step_loss: 0.0990
step 87480; step_loss: 0.1050
step 87490; step_loss: 0.0579
step 87500; step_loss: 0.0885
step 87510; step_loss: 0.0600
step 87520; step_loss: 0.0994
step 87530; step_loss: 0.0692
step 87540; step_loss: 0.0996
step 87550; step_loss: 0.0650
step 87560; step_loss: 0.1886
step 87570; step_loss: 0.1091
step 87580; step_loss: 0.0558
step 87590; step_loss: 0.2358
step 87600; step_loss: 0.1247
step 87610; step_loss: 0.0703
step 87620; step_loss: 0.0735
step 87630; step_loss: 0.0597
step 87640; step_loss: 0.1024
step 87650; step_loss: 0.0752
step 87660; step_loss: 0.0644
step 87670; step_loss: 0.0711
step 87680; step_loss: 0.0850
step 87690; step_loss: 0.0684
step 87700; step_loss: 0.0703
step 87710; step_loss: 0.0835
step 87720; step_loss: 0.0655
step 87730; step_loss: 0.0455
step 87740; step_loss: 0.0631
step 87750; step_loss: 0.0569
step 87760; step_loss: 0.0802
step 87770; step_loss: 0.0542
step 87780; step_loss: 0.0696
step 87790; step_loss: 0.0763
step 87800; step_loss: 0.0865
step 87810; step_loss: 0.0860
step 87820; step_loss: 0.1113
step 87830; step_loss: 0.0941
step 87840; step_loss: 0.0865
step 87850; step_loss: 0.0705
step 87860; step_loss: 0.0996
step 87870; step_loss: 0.0979
step 87880; step_loss: 0.0975
step 87890; step_loss: 0.1143
step 87900; step_loss: 0.0680
step 87910; step_loss: 0.0779
step 87920; step_loss: 0.1067
step 87930; step_loss: 0.1252
step 87940; step_loss: 0.0699
step 87950; step_loss: 0.1107
step 87960; step_loss: 0.1296
step 87970; step_loss: 0.0766
step 87980; step_loss: 0.0656
step 87990; step_loss: 0.0626

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 0.435 | 0.758 | 1.167 | 1.310 |   n/a |   n/a |

============================
Global step:         88000
Learning rate:       0.0033
Step-time (ms):     26.9968
Train loss avg:      0.0865
--------------------------
Val loss:            0.4594
srnn loss:           0.3976
============================

Saving the model...
done in 276.93 ms
step 88000; step_loss: 0.0879
step 88010; step_loss: 0.0823
step 88020; step_loss: 0.0849
step 88030; step_loss: 0.0760
step 88040; step_loss: 0.0713
step 88050; step_loss: 0.0446
step 88060; step_loss: 0.1314
step 88070; step_loss: 0.1261
step 88080; step_loss: 0.0673
step 88090; step_loss: 0.0623
step 88100; step_loss: 0.1353
step 88110; step_loss: 0.1417
step 88120; step_loss: 0.0589
step 88130; step_loss: 0.0668
step 88140; step_loss: 0.1408
step 88150; step_loss: 0.0705
step 88160; step_loss: 0.0530
step 88170; step_loss: 0.1066
step 88180; step_loss: 0.0603
step 88190; step_loss: 0.0655
step 88200; step_loss: 0.0773
step 88210; step_loss: 0.0846
step 88220; step_loss: 0.0768
step 88230; step_loss: 0.0985
step 88240; step_loss: 0.1094
step 88250; step_loss: 0.0733
step 88260; step_loss: 0.0956
step 88270; step_loss: 0.0850
step 88280; step_loss: 0.0864
step 88290; step_loss: 0.0763
step 88300; step_loss: 0.1107
step 88310; step_loss: 0.0784
step 88320; step_loss: 0.0644
step 88330; step_loss: 0.0687
step 88340; step_loss: 0.0744
step 88350; step_loss: 0.0586
step 88360; step_loss: 0.0722
step 88370; step_loss: 0.0866
step 88380; step_loss: 0.0805
step 88390; step_loss: 0.0827
step 88400; step_loss: 0.0599
step 88410; step_loss: 0.0779
step 88420; step_loss: 0.0754
step 88430; step_loss: 0.1098
step 88440; step_loss: 0.0722
step 88450; step_loss: 0.0717
step 88460; step_loss: 0.0984
step 88470; step_loss: 0.0890
step 88480; step_loss: 0.1188
step 88490; step_loss: 0.1167
step 88500; step_loss: 0.1065
step 88510; step_loss: 0.1297
step 88520; step_loss: 0.0888
step 88530; step_loss: 0.0683
step 88540; step_loss: 0.0628
step 88550; step_loss: 0.0492
step 88560; step_loss: 0.0670
step 88570; step_loss: 0.0699
step 88580; step_loss: 0.0893
step 88590; step_loss: 0.0787
step 88600; step_loss: 0.1875
step 88610; step_loss: 0.0633
step 88620; step_loss: 0.0837
step 88630; step_loss: 0.0809
step 88640; step_loss: 0.1046
step 88650; step_loss: 0.0885
step 88660; step_loss: 0.0753
step 88670; step_loss: 0.0860
step 88680; step_loss: 0.0868
step 88690; step_loss: 0.0967
step 88700; step_loss: 0.1226
step 88710; step_loss: 0.1106
step 88720; step_loss: 0.0686
step 88730; step_loss: 0.0818
step 88740; step_loss: 0.0775
step 88750; step_loss: 0.0892
step 88760; step_loss: 0.0813
step 88770; step_loss: 0.1652
step 88780; step_loss: 0.0728
step 88790; step_loss: 0.0813
step 88800; step_loss: 0.0731
step 88810; step_loss: 0.0775
step 88820; step_loss: 0.1380
step 88830; step_loss: 0.0757
step 88840; step_loss: 0.1302
step 88850; step_loss: 0.0685
step 88860; step_loss: 0.0637
step 88870; step_loss: 0.0611
step 88880; step_loss: 0.0496
step 88890; step_loss: 0.0733
step 88900; step_loss: 0.0809
step 88910; step_loss: 0.0874
step 88920; step_loss: 0.0848
step 88930; step_loss: 0.0865
step 88940; step_loss: 0.0635
step 88950; step_loss: 0.0688
step 88960; step_loss: 0.0912
step 88970; step_loss: 0.0727
step 88980; step_loss: 0.0736
step 88990; step_loss: 0.1109

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 0.441 | 0.768 | 1.190 | 1.337 |   n/a |   n/a |

============================
Global step:         89000
Learning rate:       0.0033
Step-time (ms):     27.6765
Train loss avg:      0.0842
--------------------------
Val loss:            0.4287
srnn loss:           0.3999
============================

Saving the model...
done in 276.64 ms
step 89000; step_loss: 0.1011
step 89010; step_loss: 0.0705
step 89020; step_loss: 0.0770
step 89030; step_loss: 0.0599
step 89040; step_loss: 0.0766
step 89050; step_loss: 0.0864
step 89060; step_loss: 0.1112
step 89070; step_loss: 0.0814
step 89080; step_loss: 0.0594
step 89090; step_loss: 0.0742
step 89100; step_loss: 0.0500
step 89110; step_loss: 0.0645
step 89120; step_loss: 0.0813
step 89130; step_loss: 0.1080
step 89140; step_loss: 0.0876
step 89150; step_loss: 0.0945
step 89160; step_loss: 0.1452
step 89170; step_loss: 0.0804
step 89180; step_loss: 0.0784
step 89190; step_loss: 0.0756
step 89200; step_loss: 0.0673
step 89210; step_loss: 0.0616
step 89220; step_loss: 0.0693
step 89230; step_loss: 0.0860
step 89240; step_loss: 0.1142
step 89250; step_loss: 0.0862
step 89260; step_loss: 0.0892
step 89270; step_loss: 0.0784
step 89280; step_loss: 0.0799
step 89290; step_loss: 0.1139
step 89300; step_loss: 0.0714
step 89310; step_loss: 0.1496
step 89320; step_loss: 0.0818
step 89330; step_loss: 0.1561
step 89340; step_loss: 0.0739
step 89350; step_loss: 0.0732
step 89360; step_loss: 0.0829
step 89370; step_loss: 0.0892
step 89380; step_loss: 0.1378
step 89390; step_loss: 0.0547
step 89400; step_loss: 0.0855
step 89410; step_loss: 0.2215
step 89420; step_loss: 0.0772
step 89430; step_loss: 0.0640
step 89440; step_loss: 0.0649
step 89450; step_loss: 0.0433
step 89460; step_loss: 0.1199
step 89470; step_loss: 0.1071
step 89480; step_loss: 0.0796
step 89490; step_loss: 0.0928
step 89500; step_loss: 0.1283
step 89510; step_loss: 0.0679
step 89520; step_loss: 0.0753
step 89530; step_loss: 0.0830
step 89540; step_loss: 0.0837
step 89550; step_loss: 0.0838
step 89560; step_loss: 0.0588
step 89570; step_loss: 0.0665
step 89580; step_loss: 0.0718
step 89590; step_loss: 0.0728
step 89600; step_loss: 0.1022
step 89610; step_loss: 0.1122
step 89620; step_loss: 0.0780
step 89630; step_loss: 0.1325
step 89640; step_loss: 0.0694
step 89650; step_loss: 0.0653
step 89660; step_loss: 0.0702
step 89670; step_loss: 0.1333
step 89680; step_loss: 0.0914
step 89690; step_loss: 0.0939
step 89700; step_loss: 0.0949
step 89710; step_loss: 0.0710
step 89720; step_loss: 0.1132
step 89730; step_loss: 0.0566
step 89740; step_loss: 0.0865
step 89750; step_loss: 0.0687
step 89760; step_loss: 0.0883
step 89770; step_loss: 0.0732
step 89780; step_loss: 0.0794
step 89790; step_loss: 0.0865
step 89800; step_loss: 0.0982
step 89810; step_loss: 0.0887
step 89820; step_loss: 0.0556
step 89830; step_loss: 0.0967
step 89840; step_loss: 0.0848
step 89850; step_loss: 0.0645
step 89860; step_loss: 0.1142
step 89870; step_loss: 0.0874
step 89880; step_loss: 0.1201
step 89890; step_loss: 0.0821
step 89900; step_loss: 0.0611
step 89910; step_loss: 0.1099
step 89920; step_loss: 0.1348
step 89930; step_loss: 0.1253
step 89940; step_loss: 0.0661
step 89950; step_loss: 0.0661
step 89960; step_loss: 0.0629
step 89970; step_loss: 0.1070
step 89980; step_loss: 0.0631
step 89990; step_loss: 0.0973

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 0.440 | 0.768 | 1.193 | 1.344 |   n/a |   n/a |

============================
Global step:         90000
Learning rate:       0.0032
Step-time (ms):     27.1854
Train loss avg:      0.0842
--------------------------
Val loss:            0.4494
srnn loss:           0.4016
============================

Saving the model...
done in 269.00 ms
step 90000; step_loss: 0.0673
step 90010; step_loss: 0.0903
step 90020; step_loss: 0.0568
step 90030; step_loss: 0.0562
step 90040; step_loss: 0.0863
step 90050; step_loss: 0.0996
step 90060; step_loss: 0.0631
step 90070; step_loss: 0.1432
step 90080; step_loss: 0.1297
step 90090; step_loss: 0.0824
step 90100; step_loss: 0.0622
step 90110; step_loss: 0.0712
step 90120; step_loss: 0.0611
step 90130; step_loss: 0.0700
step 90140; step_loss: 0.0940
step 90150; step_loss: 0.0552
step 90160; step_loss: 0.0667
step 90170; step_loss: 0.0683
step 90180; step_loss: 0.1092
step 90190; step_loss: 0.1013
step 90200; step_loss: 0.0601
step 90210; step_loss: 0.0501
step 90220; step_loss: 0.0722
step 90230; step_loss: 0.0571
step 90240; step_loss: 0.0535
step 90250; step_loss: 0.0900
step 90260; step_loss: 0.1008
step 90270; step_loss: 0.0856
step 90280; step_loss: 0.0900
step 90290; step_loss: 0.1072
step 90300; step_loss: 0.0741
step 90310; step_loss: 0.0773
step 90320; step_loss: 0.0657
step 90330; step_loss: 0.1156
step 90340; step_loss: 0.0909
step 90350; step_loss: 0.1404
step 90360; step_loss: 0.1176
step 90370; step_loss: 0.0904
step 90380; step_loss: 0.0759
step 90390; step_loss: 0.0756
step 90400; step_loss: 0.0581
step 90410; step_loss: 0.0717
step 90420; step_loss: 0.0887
step 90430; step_loss: 0.0790
step 90440; step_loss: 0.0812
step 90450; step_loss: 0.0905
step 90460; step_loss: 0.0814
step 90470; step_loss: 0.0582
step 90480; step_loss: 0.1043
step 90490; step_loss: 0.1012
step 90500; step_loss: 0.0860
step 90510; step_loss: 0.0725
step 90520; step_loss: 0.0820
step 90530; step_loss: 0.0787
step 90540; step_loss: 0.1085
step 90550; step_loss: 0.1271
step 90560; step_loss: 0.0934
step 90570; step_loss: 0.0764
step 90580; step_loss: 0.0884
step 90590; step_loss: 0.0901
step 90600; step_loss: 0.0682
step 90610; step_loss: 0.0572
step 90620; step_loss: 0.0691
step 90630; step_loss: 0.1194
step 90640; step_loss: 0.0815
step 90650; step_loss: 0.0991
step 90660; step_loss: 0.1061
step 90670; step_loss: 0.0830
step 90680; step_loss: 0.0669
step 90690; step_loss: 0.0627
step 90700; step_loss: 0.0621
step 90710; step_loss: 0.0926
step 90720; step_loss: 0.0688
step 90730; step_loss: 0.0596
step 90740; step_loss: 0.0925
step 90750; step_loss: 0.1726
step 90760; step_loss: 0.0751
step 90770; step_loss: 0.0963
step 90780; step_loss: 0.0886
step 90790; step_loss: 0.0722
step 90800; step_loss: 0.0714
step 90810; step_loss: 0.0761
step 90820; step_loss: 0.0736
step 90830; step_loss: 0.0825
step 90840; step_loss: 0.0803
step 90850; step_loss: 0.0845
step 90860; step_loss: 0.1086
step 90870; step_loss: 0.0859
step 90880; step_loss: 0.0628
step 90890; step_loss: 0.0853
step 90900; step_loss: 0.0603
step 90910; step_loss: 0.1221
step 90920; step_loss: 0.1012
step 90930; step_loss: 0.0723
step 90940; step_loss: 0.0622
step 90950; step_loss: 0.0766
step 90960; step_loss: 0.0695
step 90970; step_loss: 0.0911
step 90980; step_loss: 0.0680
step 90990; step_loss: 0.0745

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 0.440 | 0.770 | 1.204 | 1.364 |   n/a |   n/a |

============================
Global step:         91000
Learning rate:       0.0032
Step-time (ms):     27.1260
Train loss avg:      0.0837
--------------------------
Val loss:            0.4735
srnn loss:           0.4044
============================

Saving the model...
done in 289.18 ms
step 91000; step_loss: 0.0870
step 91010; step_loss: 0.1054
step 91020; step_loss: 0.0554
step 91030; step_loss: 0.0868
step 91040; step_loss: 0.0699
step 91050; step_loss: 0.0711
step 91060; step_loss: 0.0954
step 91070; step_loss: 0.0639
step 91080; step_loss: 0.0861
step 91090; step_loss: 0.0769
step 91100; step_loss: 0.0730
step 91110; step_loss: 0.0679
step 91120; step_loss: 0.0771
step 91130; step_loss: 0.1362
step 91140; step_loss: 0.0799
step 91150; step_loss: 0.0770
step 91160; step_loss: 0.0662
step 91170; step_loss: 0.0525
step 91180; step_loss: 0.0953
step 91190; step_loss: 0.0900
step 91200; step_loss: 0.0907
step 91210; step_loss: 0.0866
step 91220; step_loss: 0.0895
step 91230; step_loss: 0.0655
step 91240; step_loss: 0.0810
step 91250; step_loss: 0.0649
step 91260; step_loss: 0.0762
step 91270; step_loss: 0.0764
step 91280; step_loss: 0.1258
step 91290; step_loss: 0.0740
step 91300; step_loss: 0.0882
step 91310; step_loss: 0.0891
step 91320; step_loss: 0.0633
step 91330; step_loss: 0.0620
step 91340; step_loss: 0.0665
step 91350; step_loss: 0.0738
step 91360; step_loss: 0.0794
step 91370; step_loss: 0.0801
step 91380; step_loss: 0.0876
step 91390; step_loss: 0.0656
step 91400; step_loss: 0.0548
step 91410; step_loss: 0.0994
step 91420; step_loss: 0.0683
step 91430; step_loss: 0.0553
step 91440; step_loss: 0.1040
step 91450; step_loss: 0.0913
step 91460; step_loss: 0.0795
step 91470; step_loss: 0.1049
step 91480; step_loss: 0.0612
step 91490; step_loss: 0.0613
step 91500; step_loss: 0.0682
step 91510; step_loss: 0.0877
step 91520; step_loss: 0.0550
step 91530; step_loss: 0.0931
step 91540; step_loss: 0.1007
step 91550; step_loss: 0.0736
step 91560; step_loss: 0.1254
step 91570; step_loss: 0.0743
step 91580; step_loss: 0.0849
step 91590; step_loss: 0.0700
step 91600; step_loss: 0.0839
step 91610; step_loss: 0.0578
step 91620; step_loss: 0.0845
step 91630; step_loss: 0.0771
step 91640; step_loss: 0.1023
step 91650; step_loss: 0.1532
step 91660; step_loss: 0.0909
step 91670; step_loss: 0.1526
step 91680; step_loss: 0.0840
step 91690; step_loss: 0.0628
step 91700; step_loss: 0.0684
step 91710; step_loss: 0.0529
step 91720; step_loss: 0.0638
step 91730; step_loss: 0.0779
step 91740; step_loss: 0.0766
step 91750; step_loss: 0.0916
step 91760; step_loss: 0.0710
step 91770; step_loss: 0.0762
step 91780; step_loss: 0.1056
step 91790; step_loss: 0.0977
step 91800; step_loss: 0.1193
step 91810; step_loss: 0.0623
step 91820; step_loss: 0.1207
step 91830; step_loss: 0.1123
step 91840; step_loss: 0.1059
step 91850; step_loss: 0.0854
step 91860; step_loss: 0.0549
step 91870; step_loss: 0.0861
step 91880; step_loss: 0.1549
step 91890; step_loss: 0.0669
step 91900; step_loss: 0.0614
step 91910; step_loss: 0.0618
step 91920; step_loss: 0.0847
step 91930; step_loss: 0.1652
step 91940; step_loss: 0.0576
step 91950; step_loss: 0.1023
step 91960; step_loss: 0.0701
step 91970; step_loss: 0.0963
step 91980; step_loss: 0.1030
step 91990; step_loss: 0.0621

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 0.441 | 0.768 | 1.194 | 1.352 |   n/a |   n/a |

============================
Global step:         92000
Learning rate:       0.0032
Step-time (ms):     27.1434
Train loss avg:      0.0845
--------------------------
Val loss:            0.4642
srnn loss:           0.3997
============================

Saving the model...
done in 274.67 ms
step 92000; step_loss: 0.0961
step 92010; step_loss: 0.0606
step 92020; step_loss: 0.0899
step 92030; step_loss: 0.0740
step 92040; step_loss: 0.0780
step 92050; step_loss: 0.0719
step 92060; step_loss: 0.0577
step 92070; step_loss: 0.1111
step 92080; step_loss: 0.0985
step 92090; step_loss: 0.0802
step 92100; step_loss: 0.0720
step 92110; step_loss: 0.1158
step 92120; step_loss: 0.0658
step 92130; step_loss: 0.0730
step 92140; step_loss: 0.0697
step 92150; step_loss: 0.1509
step 92160; step_loss: 0.1350
step 92170; step_loss: 0.0607
step 92180; step_loss: 0.0882
step 92190; step_loss: 0.1035
step 92200; step_loss: 0.0839
step 92210; step_loss: 0.0710
step 92220; step_loss: 0.1042
step 92230; step_loss: 0.0957
step 92240; step_loss: 0.0921
step 92250; step_loss: 0.0551
step 92260; step_loss: 0.0853
step 92270; step_loss: 0.0727
step 92280; step_loss: 0.0720
step 92290; step_loss: 0.0743
step 92300; step_loss: 0.0758
step 92310; step_loss: 0.1228
step 92320; step_loss: 0.0659
step 92330; step_loss: 0.1543
step 92340; step_loss: 0.0802
step 92350; step_loss: 0.0946
step 92360; step_loss: 0.0685
step 92370; step_loss: 0.1110
step 92380; step_loss: 0.1193
step 92390; step_loss: 0.1039
step 92400; step_loss: 0.0920
step 92410; step_loss: 0.0746
step 92420; step_loss: 0.0717
step 92430; step_loss: 0.0952
step 92440; step_loss: 0.0693
step 92450; step_loss: 0.0718
step 92460; step_loss: 0.0493
step 92470; step_loss: 0.1046
step 92480; step_loss: 0.0647
step 92490; step_loss: 0.0916
step 92500; step_loss: 0.1171
step 92510; step_loss: 0.0626
step 92520; step_loss: 0.0870
step 92530; step_loss: 0.0703
step 92540; step_loss: 0.0658
step 92550; step_loss: 0.0684
step 92560; step_loss: 0.0692
step 92570; step_loss: 0.0663
step 92580; step_loss: 0.0853
step 92590; step_loss: 0.0585
step 92600; step_loss: 0.0512
step 92610; step_loss: 0.0534
step 92620; step_loss: 0.0694
step 92630; step_loss: 0.0681
step 92640; step_loss: 0.0740
step 92650; step_loss: 0.1093
step 92660; step_loss: 0.0985
step 92670; step_loss: 0.0798
step 92680; step_loss: 0.1146
step 92690; step_loss: 0.0649
step 92700; step_loss: 0.1123
step 92710; step_loss: 0.0881
step 92720; step_loss: 0.0848
step 92730; step_loss: 0.0692
step 92740; step_loss: 0.0760
step 92750; step_loss: 0.1149
step 92760; step_loss: 0.0583
step 92770; step_loss: 0.0581
step 92780; step_loss: 0.0565
step 92790; step_loss: 0.1069
step 92800; step_loss: 0.0708
step 92810; step_loss: 0.0643
step 92820; step_loss: 0.0733
step 92830; step_loss: 0.0722
step 92840; step_loss: 0.0827
step 92850; step_loss: 0.1016
step 92860; step_loss: 0.0771
step 92870; step_loss: 0.0726
step 92880; step_loss: 0.0637
step 92890; step_loss: 0.1116
step 92900; step_loss: 0.2053
step 92910; step_loss: 0.0702
step 92920; step_loss: 0.1209
step 92930; step_loss: 0.0753
step 92940; step_loss: 0.0861
step 92950; step_loss: 0.1613
step 92960; step_loss: 0.0606
step 92970; step_loss: 0.0687
step 92980; step_loss: 0.0775
step 92990; step_loss: 0.0915

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 0.437 | 0.762 | 1.181 | 1.333 |   n/a |   n/a |

============================
Global step:         93000
Learning rate:       0.0032
Step-time (ms):     27.1208
Train loss avg:      0.0845
--------------------------
Val loss:            0.4664
srnn loss:           0.3994
============================

Saving the model...
done in 272.52 ms
step 93000; step_loss: 0.0683
step 93010; step_loss: 0.0700
step 93020; step_loss: 0.0718
step 93030; step_loss: 0.0702
step 93040; step_loss: 0.0939
step 93050; step_loss: 0.0731
step 93060; step_loss: 0.0991
step 93070; step_loss: 0.1103
step 93080; step_loss: 0.0821
step 93090; step_loss: 0.0842
step 93100; step_loss: 0.0666
step 93110; step_loss: 0.0587
step 93120; step_loss: 0.0715
step 93130; step_loss: 0.0766
step 93140; step_loss: 0.0505
step 93150; step_loss: 0.1572
step 93160; step_loss: 0.0772
step 93170; step_loss: 0.0674
step 93180; step_loss: 0.0894
step 93190; step_loss: 0.0887
step 93200; step_loss: 0.1240
step 93210; step_loss: 0.0565
step 93220; step_loss: 0.0954
step 93230; step_loss: 0.0505
step 93240; step_loss: 0.1029
step 93250; step_loss: 0.1585
step 93260; step_loss: 0.0773
step 93270; step_loss: 0.0773
step 93280; step_loss: 0.1106
step 93290; step_loss: 0.1200
step 93300; step_loss: 0.0630
step 93310; step_loss: 0.0858
step 93320; step_loss: 0.0573
step 93330; step_loss: 0.0847
step 93340; step_loss: 0.0695
step 93350; step_loss: 0.0733
step 93360; step_loss: 0.0794
step 93370; step_loss: 0.0948
step 93380; step_loss: 0.0644
step 93390; step_loss: 0.0589
step 93400; step_loss: 0.1040
step 93410; step_loss: 0.0785
step 93420; step_loss: 0.0749
step 93430; step_loss: 0.0971
step 93440; step_loss: 0.0967
step 93450; step_loss: 0.0518
step 93460; step_loss: 0.1220
step 93470; step_loss: 0.1560
step 93480; step_loss: 0.0761
step 93490; step_loss: 0.0590
step 93500; step_loss: 0.0730
step 93510; step_loss: 0.0706
step 93520; step_loss: 0.0824
step 93530; step_loss: 0.0540
step 93540; step_loss: 0.0678
step 93550; step_loss: 0.0653
step 93560; step_loss: 0.0812
step 93570; step_loss: 0.0665
step 93580; step_loss: 0.0823
step 93590; step_loss: 0.0767
step 93600; step_loss: 0.0651
step 93610; step_loss: 0.0902
step 93620; step_loss: 0.0779
step 93630; step_loss: 0.0589
step 93640; step_loss: 0.0856
step 93650; step_loss: 0.0974
step 93660; step_loss: 0.0770
step 93670; step_loss: 0.0594
step 93680; step_loss: 0.0664
step 93690; step_loss: 0.1064
step 93700; step_loss: 0.0632
step 93710; step_loss: 0.0685
step 93720; step_loss: 0.0597
step 93730; step_loss: 0.1080
step 93740; step_loss: 0.0981
step 93750; step_loss: 0.0845
step 93760; step_loss: 0.0933
step 93770; step_loss: 0.0943
step 93780; step_loss: 0.1107
step 93790; step_loss: 0.0582
step 93800; step_loss: 0.0697
step 93810; step_loss: 0.0780
step 93820; step_loss: 0.1451
step 93830; step_loss: 0.0747
step 93840; step_loss: 0.0817
step 93850; step_loss: 0.0812
step 93860; step_loss: 0.0720
step 93870; step_loss: 0.0662
step 93880; step_loss: 0.1779
step 93890; step_loss: 0.0652
step 93900; step_loss: 0.0521
step 93910; step_loss: 0.0583
step 93920; step_loss: 0.1296
step 93930; step_loss: 0.0700
step 93940; step_loss: 0.0659
step 93950; step_loss: 0.0793
step 93960; step_loss: 0.0678
step 93970; step_loss: 0.0765
step 93980; step_loss: 0.1053
step 93990; step_loss: 0.0684

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 0.445 | 0.776 | 1.212 | 1.370 |   n/a |   n/a |

============================
Global step:         94000
Learning rate:       0.0032
Step-time (ms):     27.1506
Train loss avg:      0.0843
--------------------------
Val loss:            0.5149
srnn loss:           0.4148
============================

Saving the model...
done in 274.49 ms
step 94000; step_loss: 0.0891
step 94010; step_loss: 0.0756
step 94020; step_loss: 0.0755
step 94030; step_loss: 0.1011
step 94040; step_loss: 0.0851
step 94050; step_loss: 0.0879
step 94060; step_loss: 0.0629
step 94070; step_loss: 0.0569
step 94080; step_loss: 0.0627
step 94090; step_loss: 0.0579
step 94100; step_loss: 0.1092
step 94110; step_loss: 0.0830
step 94120; step_loss: 0.0713
step 94130; step_loss: 0.0678
step 94140; step_loss: 0.1008
step 94150; step_loss: 0.0854
step 94160; step_loss: 0.0627
step 94170; step_loss: 0.0751
step 94180; step_loss: 0.0715
step 94190; step_loss: 0.0930
step 94200; step_loss: 0.0491
step 94210; step_loss: 0.0889
step 94220; step_loss: 0.0786
step 94230; step_loss: 0.0795
step 94240; step_loss: 0.0786
step 94250; step_loss: 0.0909
step 94260; step_loss: 0.0803
step 94270; step_loss: 0.0592
step 94280; step_loss: 0.0730
step 94290; step_loss: 0.1247
step 94300; step_loss: 0.1051
step 94310; step_loss: 0.0881
step 94320; step_loss: 0.0746
step 94330; step_loss: 0.0803
step 94340; step_loss: 0.1355
step 94350; step_loss: 0.1180
step 94360; step_loss: 0.0848
step 94370; step_loss: 0.0580
step 94380; step_loss: 0.0725
step 94390; step_loss: 0.0775
step 94400; step_loss: 0.1270
step 94410; step_loss: 0.0587
step 94420; step_loss: 0.0918
step 94430; step_loss: 0.1028
step 94440; step_loss: 0.0610
step 94450; step_loss: 0.0741
step 94460; step_loss: 0.0743
step 94470; step_loss: 0.0838
step 94480; step_loss: 0.0705
step 94490; step_loss: 0.0895
step 94500; step_loss: 0.0610
step 94510; step_loss: 0.1052
step 94520; step_loss: 0.0785
step 94530; step_loss: 0.0616
step 94540; step_loss: 0.0735
step 94550; step_loss: 0.0765
step 94560; step_loss: 0.0670
step 94570; step_loss: 0.1219
step 94580; step_loss: 0.0853
step 94590; step_loss: 0.1026
step 94600; step_loss: 0.0679
step 94610; step_loss: 0.0806
step 94620; step_loss: 0.0620
step 94630; step_loss: 0.0728
step 94640; step_loss: 0.0563
step 94650; step_loss: 0.0955
step 94660; step_loss: 0.0856
step 94670; step_loss: 0.0991
step 94680; step_loss: 0.0983
step 94690; step_loss: 0.1018
step 94700; step_loss: 0.0757
step 94710; step_loss: 0.1007
step 94720; step_loss: 0.0960
step 94730; step_loss: 0.0569
step 94740; step_loss: 0.1963
step 94750; step_loss: 0.0681
step 94760; step_loss: 0.0996
step 94770; step_loss: 0.0769
step 94780; step_loss: 0.1217
step 94790; step_loss: 0.0516
step 94800; step_loss: 0.0744
step 94810; step_loss: 0.0947
step 94820; step_loss: 0.1065
step 94830; step_loss: 0.1583
step 94840; step_loss: 0.0706
step 94850; step_loss: 0.0926
step 94860; step_loss: 0.1600
step 94870; step_loss: 0.1005
step 94880; step_loss: 0.0760
step 94890; step_loss: 0.0662
step 94900; step_loss: 0.0497
step 94910; step_loss: 0.0802
step 94920; step_loss: 0.0760
step 94930; step_loss: 0.0794
step 94940; step_loss: 0.0853
step 94950; step_loss: 0.0762
step 94960; step_loss: 0.0868
step 94970; step_loss: 0.0954
step 94980; step_loss: 0.0654
step 94990; step_loss: 0.0996

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 0.449 | 0.783 | 1.224 | 1.383 |   n/a |   n/a |

============================
Global step:         95000
Learning rate:       0.0032
Step-time (ms):     27.2645
Train loss avg:      0.0851
--------------------------
Val loss:            0.4868
srnn loss:           0.4116
============================

Saving the model...
done in 275.79 ms
step 95000; step_loss: 0.0884
step 95010; step_loss: 0.0831
step 95020; step_loss: 0.0535
step 95030; step_loss: 0.0693
step 95040; step_loss: 0.0580
step 95050; step_loss: 0.0694
step 95060; step_loss: 0.0727
step 95070; step_loss: 0.0605
step 95080; step_loss: 0.0855
step 95090; step_loss: 0.0652
step 95100; step_loss: 0.0703
step 95110; step_loss: 0.0534
step 95120; step_loss: 0.1473
step 95130; step_loss: 0.0533
step 95140; step_loss: 0.0432
step 95150; step_loss: 0.0660
step 95160; step_loss: 0.0784
step 95170; step_loss: 0.0732
step 95180; step_loss: 0.0843
step 95190; step_loss: 0.1348
step 95200; step_loss: 0.0598
step 95210; step_loss: 0.0676
step 95220; step_loss: 0.0688
step 95230; step_loss: 0.0638
step 95240; step_loss: 0.0874
step 95250; step_loss: 0.1157
step 95260; step_loss: 0.1301
step 95270; step_loss: 0.0645
step 95280; step_loss: 0.0439
step 95290; step_loss: 0.0800
step 95300; step_loss: 0.0829
step 95310; step_loss: 0.0673
step 95320; step_loss: 0.0693
step 95330; step_loss: 0.0625
step 95340; step_loss: 0.0711
step 95350; step_loss: 0.0799
step 95360; step_loss: 0.0901
step 95370; step_loss: 0.0577
step 95380; step_loss: 0.0692
step 95390; step_loss: 0.0563
step 95400; step_loss: 0.0412
step 95410; step_loss: 0.1693
step 95420; step_loss: 0.1019
step 95430; step_loss: 0.0810
step 95440; step_loss: 0.0641
step 95450; step_loss: 0.1130
step 95460; step_loss: 0.1083
step 95470; step_loss: 0.0635
step 95480; step_loss: 0.0673
step 95490; step_loss: 0.0753
step 95500; step_loss: 0.0824
step 95510; step_loss: 0.0726
step 95520; step_loss: 0.0747
step 95530; step_loss: 0.0893
step 95540; step_loss: 0.0979
step 95550; step_loss: 0.0691
step 95560; step_loss: 0.0675
step 95570; step_loss: 0.0605
step 95580; step_loss: 0.0768
step 95590; step_loss: 0.0701
step 95600; step_loss: 0.0722
step 95610; step_loss: 0.0882
step 95620; step_loss: 0.0849
step 95630; step_loss: 0.0831
step 95640; step_loss: 0.1273
step 95650; step_loss: 0.0923
step 95660; step_loss: 0.0773
step 95670; step_loss: 0.0634
step 95680; step_loss: 0.0748
step 95690; step_loss: 0.0790
step 95700; step_loss: 0.0738
step 95710; step_loss: 0.0575
step 95720; step_loss: 0.0823
step 95730; step_loss: 0.0832
step 95740; step_loss: 0.0772
step 95750; step_loss: 0.0631
step 95760; step_loss: 0.0844
step 95770; step_loss: 0.1724
step 95780; step_loss: 0.0833
step 95790; step_loss: 0.0708
step 95800; step_loss: 0.1103
step 95810; step_loss: 0.0732
step 95820; step_loss: 0.0814
step 95830; step_loss: 0.0595
step 95840; step_loss: 0.0461
step 95850; step_loss: 0.0724
step 95860; step_loss: 0.1785
step 95870; step_loss: 0.0581
step 95880; step_loss: 0.0619
step 95890; step_loss: 0.0700
step 95900; step_loss: 0.0687
step 95910; step_loss: 0.0594
step 95920; step_loss: 0.0805
step 95930; step_loss: 0.1846
step 95940; step_loss: 0.0587
step 95950; step_loss: 0.0844
step 95960; step_loss: 0.0541
step 95970; step_loss: 0.0713
step 95980; step_loss: 0.1016
step 95990; step_loss: 0.0905

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 0.451 | 0.787 | 1.229 | 1.395 |   n/a |   n/a |

============================
Global step:         96000
Learning rate:       0.0032
Step-time (ms):     27.1618
Train loss avg:      0.0833
--------------------------
Val loss:            0.5265
srnn loss:           0.4248
============================

Saving the model...
done in 274.60 ms
step 96000; step_loss: 0.0751
step 96010; step_loss: 0.0892
step 96020; step_loss: 0.0774
step 96030; step_loss: 0.0639
step 96040; step_loss: 0.1242
step 96050; step_loss: 0.1474
step 96060; step_loss: 0.0618
step 96070; step_loss: 0.0842
step 96080; step_loss: 0.0676
step 96090; step_loss: 0.1251
step 96100; step_loss: 0.1130
step 96110; step_loss: 0.0823
step 96120; step_loss: 0.0752
step 96130; step_loss: 0.1090
step 96140; step_loss: 0.0975
step 96150; step_loss: 0.0914
step 96160; step_loss: 0.0912
step 96170; step_loss: 0.0847
step 96180; step_loss: 0.0652
step 96190; step_loss: 0.0826
step 96200; step_loss: 0.1514
step 96210; step_loss: 0.1053
step 96220; step_loss: 0.0695
step 96230; step_loss: 0.0960
step 96240; step_loss: 0.0668
step 96250; step_loss: 0.0506
step 96260; step_loss: 0.0718
step 96270; step_loss: 0.0696
step 96280; step_loss: 0.1071
step 96290; step_loss: 0.0657
step 96300; step_loss: 0.0464
step 96310; step_loss: 0.0908
step 96320; step_loss: 0.0571
step 96330; step_loss: 0.0868
step 96340; step_loss: 0.0582
step 96350; step_loss: 0.0747
step 96360; step_loss: 0.0695
step 96370; step_loss: 0.0787
step 96380; step_loss: 0.1061
step 96390; step_loss: 0.1120
step 96400; step_loss: 0.2093
step 96410; step_loss: 0.0553
step 96420; step_loss: 0.1699
step 96430; step_loss: 0.0667
step 96440; step_loss: 0.0719
step 96450; step_loss: 0.1148
step 96460; step_loss: 0.0854
step 96470; step_loss: 0.1274
step 96480; step_loss: 0.0883
step 96490; step_loss: 0.0779
step 96500; step_loss: 0.1242
step 96510; step_loss: 0.2135
step 96520; step_loss: 0.0731
step 96530; step_loss: 0.1069
step 96540; step_loss: 0.0707
step 96550; step_loss: 0.0815
step 96560; step_loss: 0.0644
step 96570; step_loss: 0.1035
step 96580; step_loss: 0.0854
step 96590; step_loss: 0.0663
step 96600; step_loss: 0.0773
step 96610; step_loss: 0.0601
step 96620; step_loss: 0.0735
step 96630; step_loss: 0.0626
step 96640; step_loss: 0.0727
step 96650; step_loss: 0.0682
step 96660; step_loss: 0.1162
step 96670; step_loss: 0.0749
step 96680; step_loss: 0.0576
step 96690; step_loss: 0.1268
step 96700; step_loss: 0.1116
step 96710; step_loss: 0.0692
step 96720; step_loss: 0.0610
step 96730; step_loss: 0.0876
step 96740; step_loss: 0.0713
step 96750; step_loss: 0.1586
step 96760; step_loss: 0.0783
step 96770; step_loss: 0.1076
step 96780; step_loss: 0.0727
step 96790; step_loss: 0.0715
step 96800; step_loss: 0.0634
step 96810; step_loss: 0.0633
step 96820; step_loss: 0.0781
step 96830; step_loss: 0.1309
step 96840; step_loss: 0.0889
step 96850; step_loss: 0.0610
step 96860; step_loss: 0.0960
step 96870; step_loss: 0.1260
step 96880; step_loss: 0.0644
step 96890; step_loss: 0.0607
step 96900; step_loss: 0.0897
step 96910; step_loss: 0.0851
step 96920; step_loss: 0.0710
step 96930; step_loss: 0.0666
step 96940; step_loss: 0.0903
step 96950; step_loss: 0.0616
step 96960; step_loss: 0.0499
step 96970; step_loss: 0.0617
step 96980; step_loss: 0.0659
step 96990; step_loss: 0.0734

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 0.449 | 0.782 | 1.212 | 1.363 |   n/a |   n/a |

============================
Global step:         97000
Learning rate:       0.0032
Step-time (ms):     27.1205
Train loss avg:      0.0840
--------------------------
Val loss:            0.4606
srnn loss:           0.4168
============================

Saving the model...
done in 278.30 ms
step 97000; step_loss: 0.0824
step 97010; step_loss: 0.0583
step 97020; step_loss: 0.0714
step 97030; step_loss: 0.0868
step 97040; step_loss: 0.0483
step 97050; step_loss: 0.0495
step 97060; step_loss: 0.1069
step 97070; step_loss: 0.0861
step 97080; step_loss: 0.0897
step 97090; step_loss: 0.0695
step 97100; step_loss: 0.0867
step 97110; step_loss: 0.0863
step 97120; step_loss: 0.0798
step 97130; step_loss: 0.0661
step 97140; step_loss: 0.0644
step 97150; step_loss: 0.0773
step 97160; step_loss: 0.0903
step 97170; step_loss: 0.0707
step 97180; step_loss: 0.1040
step 97190; step_loss: 0.0540
step 97200; step_loss: 0.0826
step 97210; step_loss: 0.0755
step 97220; step_loss: 0.0545
step 97230; step_loss: 0.0497
step 97240; step_loss: 0.0670
step 97250; step_loss: 0.0758
step 97260; step_loss: 0.0751
step 97270; step_loss: 0.0656
step 97280; step_loss: 0.1111
step 97290; step_loss: 0.0697
step 97300; step_loss: 0.0717
step 97310; step_loss: 0.0493
step 97320; step_loss: 0.0692
step 97330; step_loss: 0.0792
step 97340; step_loss: 0.1077
step 97350; step_loss: 0.0800
step 97360; step_loss: 0.1025
step 97370; step_loss: 0.0545
step 97380; step_loss: 0.0787
step 97390; step_loss: 0.0794
step 97400; step_loss: 0.0707
step 97410; step_loss: 0.0697
step 97420; step_loss: 0.0497
step 97430; step_loss: 0.1247
step 97440; step_loss: 0.0634
step 97450; step_loss: 0.0846
step 97460; step_loss: 0.0890
step 97470; step_loss: 0.0756
step 97480; step_loss: 0.1065
step 97490; step_loss: 0.0880
step 97500; step_loss: 0.0966
step 97510; step_loss: 0.1113
step 97520; step_loss: 0.0647
step 97530; step_loss: 0.1090
step 97540; step_loss: 0.0696
step 97550; step_loss: 0.0811
step 97560; step_loss: 0.0674
step 97570; step_loss: 0.0763
step 97580; step_loss: 0.0654
step 97590; step_loss: 0.0803
step 97600; step_loss: 0.0840
step 97610; step_loss: 0.0583
step 97620; step_loss: 0.0624
step 97630; step_loss: 0.0723
step 97640; step_loss: 0.1051
step 97650; step_loss: 0.0381
step 97660; step_loss: 0.1935
step 97670; step_loss: 0.0558
step 97680; step_loss: 0.0966
step 97690; step_loss: 0.0763
step 97700; step_loss: 0.0940
step 97710; step_loss: 0.0767
step 97720; step_loss: 0.1283
step 97730; step_loss: 0.0638
step 97740; step_loss: 0.0911
step 97750; step_loss: 0.0766
step 97760; step_loss: 0.1458
step 97770; step_loss: 0.0677
step 97780; step_loss: 0.0977
step 97790; step_loss: 0.0841
step 97800; step_loss: 0.0641
step 97810; step_loss: 0.0803
step 97820; step_loss: 0.0585
step 97830; step_loss: 0.0652
step 97840; step_loss: 0.0676
step 97850; step_loss: 0.0703
step 97860; step_loss: 0.0525
step 97870; step_loss: 0.0823
step 97880; step_loss: 0.1067
step 97890; step_loss: 0.0743
step 97900; step_loss: 0.0591
step 97910; step_loss: 0.0884
step 97920; step_loss: 0.0796
step 97930; step_loss: 0.1001
step 97940; step_loss: 0.0774
step 97950; step_loss: 0.0746
step 97960; step_loss: 0.0810
step 97970; step_loss: 0.0698
step 97980; step_loss: 0.0584
step 97990; step_loss: 0.0533

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 0.452 | 0.789 | 1.231 | 1.391 |   n/a |   n/a |

============================
Global step:         98000
Learning rate:       0.0032
Step-time (ms):     27.1106
Train loss avg:      0.0831
--------------------------
Val loss:            0.4475
srnn loss:           0.4201
============================

Saving the model...
done in 282.73 ms
step 98000; step_loss: 0.0808
step 98010; step_loss: 0.1065
step 98020; step_loss: 0.0852
step 98030; step_loss: 0.0553
step 98040; step_loss: 0.0751
step 98050; step_loss: 0.1160
step 98060; step_loss: 0.0577
step 98070; step_loss: 0.1150
step 98080; step_loss: 0.0890
step 98090; step_loss: 0.1023
step 98100; step_loss: 0.0939
step 98110; step_loss: 0.0541
step 98120; step_loss: 0.0928
step 98130; step_loss: 0.0750
step 98140; step_loss: 0.0958
step 98150; step_loss: 0.0737
step 98160; step_loss: 0.1304
step 98170; step_loss: 0.1404
step 98180; step_loss: 0.0800
step 98190; step_loss: 0.0805
step 98200; step_loss: 0.0889
step 98210; step_loss: 0.1074
step 98220; step_loss: 0.0817
step 98230; step_loss: 0.1134
step 98240; step_loss: 0.1037
step 98250; step_loss: 0.0761
step 98260; step_loss: 0.0807
step 98270; step_loss: 0.0899
step 98280; step_loss: 0.0683
step 98290; step_loss: 0.1036
step 98300; step_loss: 0.0728
step 98310; step_loss: 0.1251
step 98320; step_loss: 0.0883
step 98330; step_loss: 0.0619
step 98340; step_loss: 0.0750
step 98350; step_loss: 0.0831
step 98360; step_loss: 0.0613
step 98370; step_loss: 0.0689
step 98380; step_loss: 0.0808
step 98390; step_loss: 0.0751
step 98400; step_loss: 0.0809
step 98410; step_loss: 0.0760
step 98420; step_loss: 0.0916
step 98430; step_loss: 0.0729
step 98440; step_loss: 0.0886
step 98450; step_loss: 0.0508
step 98460; step_loss: 0.0939
step 98470; step_loss: 0.0786
step 98480; step_loss: 0.0875
step 98490; step_loss: 0.0729
step 98500; step_loss: 0.0962
step 98510; step_loss: 0.0729
step 98520; step_loss: 0.1758
step 98530; step_loss: 0.0839
step 98540; step_loss: 0.0997
step 98550; step_loss: 0.0623
step 98560; step_loss: 0.0793
step 98570; step_loss: 0.0696
step 98580; step_loss: 0.0768
step 98590; step_loss: 0.1066
step 98600; step_loss: 0.0742
step 98610; step_loss: 0.0937
step 98620; step_loss: 0.0819
step 98630; step_loss: 0.0833
step 98640; step_loss: 0.0588
step 98650; step_loss: 0.0874
step 98660; step_loss: 0.0868
step 98670; step_loss: 0.0646
step 98680; step_loss: 0.1057
step 98690; step_loss: 0.0786
step 98700; step_loss: 0.0775
step 98710; step_loss: 0.0657
step 98720; step_loss: 0.0708
step 98730; step_loss: 0.0917
step 98740; step_loss: 0.0701
step 98750; step_loss: 0.0599
step 98760; step_loss: 0.0796
step 98770; step_loss: 0.0620
step 98780; step_loss: 0.0711
step 98790; step_loss: 0.0691
step 98800; step_loss: 0.0692
step 98810; step_loss: 0.0931
step 98820; step_loss: 0.0727
step 98830; step_loss: 0.0692
step 98840; step_loss: 0.0719
step 98850; step_loss: 0.0662
step 98860; step_loss: 0.0843
step 98870; step_loss: 0.0623
step 98880; step_loss: 0.0946
step 98890; step_loss: 0.1053
step 98900; step_loss: 0.1050
step 98910; step_loss: 0.0552
step 98920; step_loss: 0.0561
step 98930; step_loss: 0.0616
step 98940; step_loss: 0.0920
step 98950; step_loss: 0.0713
step 98960; step_loss: 0.0633
step 98970; step_loss: 0.0808
step 98980; step_loss: 0.0734
step 98990; step_loss: 0.0818

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 0.455 | 0.794 | 1.234 | 1.389 |   n/a |   n/a |

============================
Global step:         99000
Learning rate:       0.0032
Step-time (ms):     27.1385
Train loss avg:      0.0825
--------------------------
Val loss:            0.4664
srnn loss:           0.4233
============================

Saving the model...
done in 284.94 ms
step 99000; step_loss: 0.0738
step 99010; step_loss: 0.0717
step 99020; step_loss: 0.1188
step 99030; step_loss: 0.0683
step 99040; step_loss: 0.1963
step 99050; step_loss: 0.0600
step 99060; step_loss: 0.0575
step 99070; step_loss: 0.1723
step 99080; step_loss: 0.0997
step 99090; step_loss: 0.0673
step 99100; step_loss: 0.0779
step 99110; step_loss: 0.0629
step 99120; step_loss: 0.0742
step 99130; step_loss: 0.0791
step 99140; step_loss: 0.0660
step 99150; step_loss: 0.0567
step 99160; step_loss: 0.0755
step 99170; step_loss: 0.0847
step 99180; step_loss: 0.1097
step 99190; step_loss: 0.0681
step 99200; step_loss: 0.1190
step 99210; step_loss: 0.0556
step 99220; step_loss: 0.0563
step 99230; step_loss: 0.0615
step 99240; step_loss: 0.0618
step 99250; step_loss: 0.0592
step 99260; step_loss: 0.0761
step 99270; step_loss: 0.0737
step 99280; step_loss: 0.0446
step 99290; step_loss: 0.1214
step 99300; step_loss: 0.1346
step 99310; step_loss: 0.1177
step 99320; step_loss: 0.0762
step 99330; step_loss: 0.1218
step 99340; step_loss: 0.0562
step 99350; step_loss: 0.0772
step 99360; step_loss: 0.1040
step 99370; step_loss: 0.0727
step 99380; step_loss: 0.0668
step 99390; step_loss: 0.0777
step 99400; step_loss: 0.0946
step 99410; step_loss: 0.0970
step 99420; step_loss: 0.0776
step 99430; step_loss: 0.0763
step 99440; step_loss: 0.0800
step 99450; step_loss: 0.0618
step 99460; step_loss: 0.0888
step 99470; step_loss: 0.0952
step 99480; step_loss: 0.1036
step 99490; step_loss: 0.0637
step 99500; step_loss: 0.0834
step 99510; step_loss: 0.0611
step 99520; step_loss: 0.0904
step 99530; step_loss: 0.0732
step 99540; step_loss: 0.0793
step 99550; step_loss: 0.0533
step 99560; step_loss: 0.0644
step 99570; step_loss: 0.0683
step 99580; step_loss: 0.1132
step 99590; step_loss: 0.0745
step 99600; step_loss: 0.1159
step 99610; step_loss: 0.0723
step 99620; step_loss: 0.0856
step 99630; step_loss: 0.0790
step 99640; step_loss: 0.0844
step 99650; step_loss: 0.1021
step 99660; step_loss: 0.0641
step 99670; step_loss: 0.0878
step 99680; step_loss: 0.0737
step 99690; step_loss: 0.0580
step 99700; step_loss: 0.0708
step 99710; step_loss: 0.0774
step 99720; step_loss: 0.1118
step 99730; step_loss: 0.0933
step 99740; step_loss: 0.1393
step 99750; step_loss: 0.0983
step 99760; step_loss: 0.0768
step 99770; step_loss: 0.1071
step 99780; step_loss: 0.0861
step 99790; step_loss: 0.0737
step 99800; step_loss: 0.0697
step 99810; step_loss: 0.1025
step 99820; step_loss: 0.0684
step 99830; step_loss: 0.0680
step 99840; step_loss: 0.0702
step 99850; step_loss: 0.0655
step 99860; step_loss: 0.0761
step 99870; step_loss: 0.1576
step 99880; step_loss: 0.0647
step 99890; step_loss: 0.0684
step 99900; step_loss: 0.0597
step 99910; step_loss: 0.0695
step 99920; step_loss: 0.0737
step 99930; step_loss: 0.0811
step 99940; step_loss: 0.0485
step 99950; step_loss: 0.0722
step 99960; step_loss: 0.0567
step 99970; step_loss: 0.0511
step 99980; step_loss: 0.0771
step 99990; step_loss: 0.0653

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 0.450 | 0.784 | 1.214 | 1.363 |   n/a |   n/a |

============================
Global step:         100000
Learning rate:       0.0030
Step-time (ms):     27.1635
Train loss avg:      0.0823
--------------------------
Val loss:            0.4846
srnn loss:           0.4207
============================

Saving the model...
done in 277.09 ms