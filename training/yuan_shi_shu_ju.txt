WARNING:tensorflow:From src/translate.py:701: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.

Reading training data (seq_len_in: 50, seq_len_out 25).
Reading subject 1, action walking, subaction 1
Reading subject 1, action walking, subaction 2
Reading subject 6, action walking, subaction 1
Reading subject 6, action walking, subaction 2
Reading subject 7, action walking, subaction 1
Reading subject 7, action walking, subaction 2
Reading subject 8, action walking, subaction 1
Reading subject 8, action walking, subaction 2
Reading subject 9, action walking, subaction 1
Reading subject 9, action walking, subaction 2
Reading subject 11, action walking, subaction 1
Reading subject 11, action walking, subaction 2
Reading subject 5, action walking, subaction 1
Reading subject 5, action walking, subaction 2
done reading data.
WARNING:tensorflow:From src/translate.py:124: The name tf.GPUOptions is deprecated. Please use tf.compat.v1.GPUOptions instead.

W1128 21:00:12.319542 140392724273024 module_wrapper.py:139] From src/translate.py:124: The name tf.GPUOptions is deprecated. Please use tf.compat.v1.GPUOptions instead.

WARNING:tensorflow:From src/translate.py:127: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.

W1128 21:00:12.319935 140392724273024 module_wrapper.py:139] From src/translate.py:127: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.

WARNING:tensorflow:From src/translate.py:127: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.

W1128 21:00:12.320133 140392724273024 module_wrapper.py:139] From src/translate.py:127: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.

2019-11-28 21:00:12.325731: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz
2019-11-28 21:00:12.325980: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x263b100 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2019-11-28 21:00:12.326053: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2019-11-28 21:00:12.328187: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2019-11-28 21:00:12.437407: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-11-28 21:00:12.438542: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x263b9c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2019-11-28 21:00:12.438574: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0
2019-11-28 21:00:12.438769: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-11-28 21:00:12.439638: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285
pciBusID: 0000:00:04.0
2019-11-28 21:00:12.462201: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2019-11-28 21:00:12.707940: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2019-11-28 21:00:12.840289: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2019-11-28 21:00:12.866273: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2019-11-28 21:00:13.143019: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2019-11-28 21:00:13.175978: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2019-11-28 21:00:13.674656: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2019-11-28 21:00:13.674923: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-11-28 21:00:13.676009: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-11-28 21:00:13.676857: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2019-11-28 21:00:13.685694: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2019-11-28 21:00:13.687385: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-11-28 21:00:13.687421: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 
2019-11-28 21:00:13.687439: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N 
2019-11-28 21:00:13.690302: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-11-28 21:00:13.691286: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-11-28 21:00:13.692140: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2019-11-28 21:00:13.692208: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 16280 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)
Creating 1 layers of 1024 units.
One hot is  True
Input size is 55
WARNING:tensorflow:From /content/ml2test/human-motion-prediction/src/seq2seq_model.py:70: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.

W1128 21:00:13.693388 140392724273024 module_wrapper.py:139] From /content/ml2test/human-motion-prediction/src/seq2seq_model.py:70: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.

rnn_size = 1024
WARNING:tensorflow:
The TensorFlow contrib module will not be included in TensorFlow 2.0.
For more information, please see:
  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md
  * https://github.com/tensorflow/addons
  * https://github.com/tensorflow/io (for I/O related ops)
If you depend on functionality not listed there, please file an issue.

W1128 21:00:13.705698 140392724273024 lazy_loader.py:50] 
The TensorFlow contrib module will not be included in TensorFlow 2.0.
For more information, please see:
  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md
  * https://github.com/tensorflow/addons
  * https://github.com/tensorflow/io (for I/O related ops)
If you depend on functionality not listed there, please file an issue.

WARNING:tensorflow:From /content/ml2test/human-motion-prediction/src/seq2seq_model.py:83: GRUCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.
Instructions for updating:
This class is equivalent as tf.keras.layers.GRUCell, and will be replaced by that in Tensorflow 2.0.
W1128 21:00:13.705893 140392724273024 deprecation.py:323] From /content/ml2test/human-motion-prediction/src/seq2seq_model.py:83: GRUCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.
Instructions for updating:
This class is equivalent as tf.keras.layers.GRUCell, and will be replaced by that in Tensorflow 2.0.
WARNING:tensorflow:From /content/ml2test/human-motion-prediction/src/seq2seq_model.py:91: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

W1128 21:00:13.707129 140392724273024 module_wrapper.py:139] From /content/ml2test/human-motion-prediction/src/seq2seq_model.py:91: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

output_size = 55
 state_size = 1024
WARNING:tensorflow:From /content/ml2test/human-motion-prediction/src/rnn_cell_extensions.py:95: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.

W1128 21:00:13.722278 140392724273024 module_wrapper.py:139] From /content/ml2test/human-motion-prediction/src/rnn_cell_extensions.py:95: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.

WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/legacy_seq2seq/python/ops/seq2seq.py:223: static_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell, unroll=True)`, which is equivalent to this API
W1128 21:00:13.737980 140392724273024 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/legacy_seq2seq/python/ops/seq2seq.py:223: static_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell, unroll=True)`, which is equivalent to this API
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/rnn_cell_impl.py:559: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `layer.add_weight` method instead.
W1128 21:00:13.750889 140392724273024 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/rnn_cell_impl.py:559: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `layer.add_weight` method instead.
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/rnn_cell_impl.py:565: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W1128 21:00:13.760203 140392724273024 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/rnn_cell_impl.py:565: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/rnn_cell_impl.py:575: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W1128 21:00:13.772354 140392724273024 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/rnn_cell_impl.py:575: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:From /content/ml2test/human-motion-prediction/src/seq2seq_model.py:148: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.

W1128 21:00:14.866733 140392724273024 module_wrapper.py:139] From /content/ml2test/human-motion-prediction/src/seq2seq_model.py:148: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.

WARNING:tensorflow:From /content/ml2test/human-motion-prediction/src/seq2seq_model.py:151: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.

W1128 21:00:14.868404 140392724273024 module_wrapper.py:139] From /content/ml2test/human-motion-prediction/src/seq2seq_model.py:151: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.

WARNING:tensorflow:From /content/ml2test/human-motion-prediction/src/seq2seq_model.py:153: The name tf.train.GradientDescentOptimizer is deprecated. Please use tf.compat.v1.train.GradientDescentOptimizer instead.

W1128 21:00:14.868659 140392724273024 module_wrapper.py:139] From /content/ml2test/human-motion-prediction/src/seq2seq_model.py:153: The name tf.train.GradientDescentOptimizer is deprecated. Please use tf.compat.v1.train.GradientDescentOptimizer instead.

WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/clip_ops.py:301: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
W1128 21:00:18.729490 140392724273024 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/clip_ops.py:301: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
WARNING:tensorflow:From /content/ml2test/human-motion-prediction/src/seq2seq_model.py:378: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.

W1128 21:00:18.899001 140392724273024 module_wrapper.py:139] From /content/ml2test/human-motion-prediction/src/seq2seq_model.py:378: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.

WARNING:tensorflow:From /content/ml2test/human-motion-prediction/src/seq2seq_model.py:378: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.

W1128 21:00:18.899387 140392724273024 module_wrapper.py:139] From /content/ml2test/human-motion-prediction/src/seq2seq_model.py:378: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.

Creating model with fresh parameters.
WARNING:tensorflow:From src/translate.py:87: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.

W1128 21:00:18.914750 140392724273024 module_wrapper.py:139] From src/translate.py:87: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.

Model created
2019-11-28 21:00:25.069553: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
step 0000; step_loss: 0.8611
step 0010; step_loss: 1.1863
step 0020; step_loss: 1.0936
step 0030; step_loss: 1.0535
step 0040; step_loss: 0.7718
step 0050; step_loss: 1.1695
step 0060; step_loss: 0.9132
step 0070; step_loss: 0.8164
step 0080; step_loss: 0.9812
step 0090; step_loss: 0.8918
step 0100; step_loss: 0.6870
step 0110; step_loss: 1.0532
step 0120; step_loss: 0.7644
step 0130; step_loss: 1.1916
step 0140; step_loss: 1.0006
step 0150; step_loss: 1.1619
step 0160; step_loss: 0.8045
step 0170; step_loss: 0.7880
step 0180; step_loss: 0.8179
step 0190; step_loss: 0.8237
step 0200; step_loss: 0.8039
step 0210; step_loss: 0.7264
step 0220; step_loss: 0.8008
step 0230; step_loss: 0.7759
step 0240; step_loss: 0.7386
step 0250; step_loss: 0.8032
step 0260; step_loss: 0.7739
step 0270; step_loss: 0.6498
step 0280; step_loss: 0.8136
step 0290; step_loss: 0.7643
step 0300; step_loss: 0.8201
step 0310; step_loss: 0.7835
step 0320; step_loss: 0.6501
step 0330; step_loss: 0.7657
step 0340; step_loss: 0.6744
step 0350; step_loss: 0.9516
step 0360; step_loss: 0.9600
step 0370; step_loss: 0.6908
step 0380; step_loss: 0.8244
step 0390; step_loss: 0.7801
step 0400; step_loss: 0.6910
step 0410; step_loss: 0.6147
step 0420; step_loss: 0.7650
step 0430; step_loss: 0.5743
step 0440; step_loss: 0.9067
step 0450; step_loss: 0.7061
step 0460; step_loss: 0.7399
step 0470; step_loss: 0.6672
step 0480; step_loss: 0.7363
step 0490; step_loss: 0.7329
step 0500; step_loss: 0.6004
step 0510; step_loss: 0.4649
step 0520; step_loss: 0.7503
step 0530; step_loss: 0.7435
step 0540; step_loss: 0.6443
step 0550; step_loss: 0.6755
step 0560; step_loss: 0.7032
step 0570; step_loss: 0.6355
step 0580; step_loss: 0.6083
step 0590; step_loss: 0.5569
step 0600; step_loss: 0.8456
step 0610; step_loss: 0.7300
step 0620; step_loss: 0.4642
step 0630; step_loss: 0.4791
step 0640; step_loss: 0.7092
step 0650; step_loss: 0.7295
step 0660; step_loss: 0.5574
step 0670; step_loss: 0.7103
step 0680; step_loss: 0.6207
step 0690; step_loss: 0.6872
step 0700; step_loss: 0.6096
step 0710; step_loss: 0.5512
step 0720; step_loss: 0.5929
step 0730; step_loss: 0.6427
step 0740; step_loss: 0.7034
step 0750; step_loss: 0.8675
step 0760; step_loss: 0.5145
step 0770; step_loss: 0.7536
step 0780; step_loss: 0.6155
step 0790; step_loss: 0.6010
step 0800; step_loss: 0.6042
step 0810; step_loss: 0.7156
step 0820; step_loss: 0.6359
step 0830; step_loss: 0.4225
step 0840; step_loss: 0.4797
step 0850; step_loss: 0.7990
step 0860; step_loss: 0.4741
step 0870; step_loss: 0.5352
step 0880; step_loss: 0.5268
step 0890; step_loss: 0.4848
step 0900; step_loss: 0.5466
step 0910; step_loss: 0.6982
step 0920; step_loss: 0.5111
step 0930; step_loss: 0.5623
step 0940; step_loss: 0.5169
step 0950; step_loss: 0.5982
step 0960; step_loss: 0.5558
step 0970; step_loss: 0.4011
step 0980; step_loss: 0.5519
step 0990; step_loss: 0.5672

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 1.571 | 1.601 | 1.631 | 1.650 | 1.675 | 1.722 |

============================
Global step:         1000
Learning rate:       0.0050
Step-time (ms):     39.1496
Train loss avg:      0.7078
--------------------------
Val loss:            1.1895
srnn loss:           0.9994
============================

Saving the model...
done in 478.56 ms
step 1000; step_loss: 0.6438
step 1010; step_loss: 0.6171
step 1020; step_loss: 0.7175
step 1030; step_loss: 0.6191
step 1040; step_loss: 0.4444
step 1050; step_loss: 0.6230
step 1060; step_loss: 0.4751
step 1070; step_loss: 0.4702
step 1080; step_loss: 0.5312
step 1090; step_loss: 0.4780
step 1100; step_loss: 0.5094
step 1110; step_loss: 0.4883
step 1120; step_loss: 0.5364
step 1130; step_loss: 0.6048
step 1140; step_loss: 0.5490
step 1150; step_loss: 0.5378
step 1160; step_loss: 0.3928
step 1170; step_loss: 0.4219
step 1180; step_loss: 0.4807
step 1190; step_loss: 0.4683
step 1200; step_loss: 0.5293
step 1210; step_loss: 0.5962
step 1220; step_loss: 0.5808
step 1230; step_loss: 0.5632
step 1240; step_loss: 0.6759
step 1250; step_loss: 0.5474
step 1260; step_loss: 0.5373
step 1270; step_loss: 0.4771
step 1280; step_loss: 0.5263
step 1290; step_loss: 0.5039
step 1300; step_loss: 0.5687
step 1310; step_loss: 0.5867
step 1320; step_loss: 0.4734
step 1330; step_loss: 0.3793
step 1340; step_loss: 0.5661
step 1350; step_loss: 0.4829
step 1360; step_loss: 0.5349
step 1370; step_loss: 0.4866
step 1380; step_loss: 0.5402
step 1390; step_loss: 0.4967
step 1400; step_loss: 0.6334
step 1410; step_loss: 0.6655
step 1420; step_loss: 0.4964
step 1430; step_loss: 0.5694
step 1440; step_loss: 0.4213
step 1450; step_loss: 0.3852
step 1460; step_loss: 0.5816
step 1470; step_loss: 0.4930
step 1480; step_loss: 0.4018
step 1490; step_loss: 0.6998
step 1500; step_loss: 0.4744
step 1510; step_loss: 0.3427
step 1520; step_loss: 0.4550
step 1530; step_loss: 0.6578
step 1540; step_loss: 0.3940
step 1550; step_loss: 0.3726
step 1560; step_loss: 0.4850
step 1570; step_loss: 0.3748
step 1580; step_loss: 0.4931
step 1590; step_loss: 0.3727
step 1600; step_loss: 0.3985
step 1610; step_loss: 0.5428
step 1620; step_loss: 0.3949
step 1630; step_loss: 0.4250
step 1640; step_loss: 0.4206
step 1650; step_loss: 0.5894
step 1660; step_loss: 0.4227
step 1670; step_loss: 0.5034
step 1680; step_loss: 0.4585
step 1690; step_loss: 0.5992
step 1700; step_loss: 0.6378
step 1710; step_loss: 0.5183
step 1720; step_loss: 0.4308
step 1730; step_loss: 0.4052
step 1740; step_loss: 0.3373
step 1750; step_loss: 0.4262
step 1760; step_loss: 0.5085
step 1770; step_loss: 0.4621
step 1780; step_loss: 0.3066
step 1790; step_loss: 0.4188
step 1800; step_loss: 0.4133
step 1810; step_loss: 0.4163
step 1820; step_loss: 0.4032
step 1830; step_loss: 0.3448
step 1840; step_loss: 0.3899
step 1850; step_loss: 0.3167
step 1860; step_loss: 0.4122
step 1870; step_loss: 0.5429
step 1880; step_loss: 0.4336
step 1890; step_loss: 0.4104
step 1900; step_loss: 0.4667
step 1910; step_loss: 0.4191
step 1920; step_loss: 0.4215
step 1930; step_loss: 0.4162
step 1940; step_loss: 0.3600
step 1950; step_loss: 0.5792
step 1960; step_loss: 0.4404
step 1970; step_loss: 0.5137
step 1980; step_loss: 0.4702
step 1990; step_loss: 0.3706

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 1.539 | 1.574 | 1.621 | 1.643 | 1.661 | 1.750 |

============================
Global step:         2000
Learning rate:       0.0050
Step-time (ms):     35.9019
Train loss avg:      0.4823
--------------------------
Val loss:            1.0695
srnn loss:           0.8565
============================

Saving the model...
done in 410.73 ms
step 2000; step_loss: 0.3673
step 2010; step_loss: 0.4020
step 2020; step_loss: 0.2960
step 2030; step_loss: 0.4894
step 2040; step_loss: 0.3098
step 2050; step_loss: 0.4689
step 2060; step_loss: 0.3252
step 2070; step_loss: 0.3987
step 2080; step_loss: 0.5036
step 2090; step_loss: 0.3669
step 2100; step_loss: 0.3342
step 2110; step_loss: 0.4027
step 2120; step_loss: 0.4252
step 2130; step_loss: 0.4290
step 2140; step_loss: 0.5317
step 2150; step_loss: 0.3533
step 2160; step_loss: 0.3708
step 2170; step_loss: 0.5296
step 2180; step_loss: 0.4981
step 2190; step_loss: 0.2353
step 2200; step_loss: 0.3257
step 2210; step_loss: 0.3831
step 2220; step_loss: 0.4405
step 2230; step_loss: 0.5343
step 2240; step_loss: 0.4335
step 2250; step_loss: 0.3251
step 2260; step_loss: 0.3271
step 2270; step_loss: 0.3709
step 2280; step_loss: 0.3346
step 2290; step_loss: 0.4598
step 2300; step_loss: 0.3320
step 2310; step_loss: 0.4239
step 2320; step_loss: 0.4592
step 2330; step_loss: 0.3654
step 2340; step_loss: 0.4086
step 2350; step_loss: 0.4636
step 2360; step_loss: 0.4150
step 2370; step_loss: 0.4389
step 2380; step_loss: 0.3862
step 2390; step_loss: 0.4656
step 2400; step_loss: 0.4614
step 2410; step_loss: 0.3893
step 2420; step_loss: 0.4775
step 2430; step_loss: 0.4354
step 2440; step_loss: 0.3620
step 2450; step_loss: 0.4299
step 2460; step_loss: 0.3604
step 2470; step_loss: 0.4278
step 2480; step_loss: 0.3982
step 2490; step_loss: 0.3885
step 2500; step_loss: 0.3479
step 2510; step_loss: 0.3741
step 2520; step_loss: 0.3184
step 2530; step_loss: 0.5378
step 2540; step_loss: 0.3159
step 2550; step_loss: 0.4329
step 2560; step_loss: 0.3995
step 2570; step_loss: 0.3897
step 2580; step_loss: 0.4319
step 2590; step_loss: 0.4291
step 2600; step_loss: 0.3667
step 2610; step_loss: 0.4397
step 2620; step_loss: 0.3250
step 2630; step_loss: 0.3667
step 2640; step_loss: 0.2722
step 2650; step_loss: 0.2811
step 2660; step_loss: 0.3902
step 2670; step_loss: 0.3343
step 2680; step_loss: 0.3429
step 2690; step_loss: 0.3160
step 2700; step_loss: 0.5507
step 2710; step_loss: 0.3698
step 2720; step_loss: 0.3088
step 2730; step_loss: 0.3129
step 2740; step_loss: 0.3458
step 2750; step_loss: 0.3606
step 2760; step_loss: 0.4309
step 2770; step_loss: 0.4859
step 2780; step_loss: 0.4046
step 2790; step_loss: 0.3269
step 2800; step_loss: 0.4031
step 2810; step_loss: 0.2915
step 2820; step_loss: 0.7994
step 2830; step_loss: 0.4339
step 2840; step_loss: 0.4429
step 2850; step_loss: 0.4952
step 2860; step_loss: 0.3488
step 2870; step_loss: 0.3781
step 2880; step_loss: 0.3822
step 2890; step_loss: 0.5009
step 2900; step_loss: 0.3641
step 2910; step_loss: 0.4339
step 2920; step_loss: 0.3893
step 2930; step_loss: 0.3633
step 2940; step_loss: 0.5262
step 2950; step_loss: 0.3471
step 2960; step_loss: 0.3521
step 2970; step_loss: 0.4767
step 2980; step_loss: 0.3182
step 2990; step_loss: 0.4345

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 1.504 | 1.541 | 1.583 | 1.601 | 1.617 | 1.722 |

============================
Global step:         3000
Learning rate:       0.0050
Step-time (ms):     35.9540
Train loss avg:      0.3925
--------------------------
Val loss:            1.0611
srnn loss:           0.8323
============================

Saving the model...
done in 391.89 ms
step 3000; step_loss: 0.3955
step 3010; step_loss: 0.4320
step 3020; step_loss: 0.2769
step 3030; step_loss: 0.4082
step 3040; step_loss: 0.3712
step 3050; step_loss: 0.3764
step 3060; step_loss: 0.4078
step 3070; step_loss: 0.3773
step 3080; step_loss: 0.2931
step 3090; step_loss: 0.3247
step 3100; step_loss: 0.4221
step 3110; step_loss: 0.5696
step 3120; step_loss: 0.3325
step 3130; step_loss: 0.5127
step 3140; step_loss: 0.4505
step 3150; step_loss: 0.2634
step 3160; step_loss: 0.3843
step 3170; step_loss: 0.3603
step 3180; step_loss: 0.3518
step 3190; step_loss: 0.3573
step 3200; step_loss: 0.3044
step 3210; step_loss: 0.7077
step 3220; step_loss: 0.3262
step 3230; step_loss: 0.3198
step 3240; step_loss: 0.3441
step 3250; step_loss: 0.3608
step 3260; step_loss: 0.3587
step 3270; step_loss: 0.2438
step 3280; step_loss: 0.3891
step 3290; step_loss: 0.3299
step 3300; step_loss: 0.4576
step 3310; step_loss: 0.4812
step 3320; step_loss: 0.2539
step 3330; step_loss: 0.2582
step 3340; step_loss: 0.4024
step 3350; step_loss: 0.3083
step 3360; step_loss: 0.3716
step 3370; step_loss: 0.3157
step 3380; step_loss: 0.3522
step 3390; step_loss: 0.3241
step 3400; step_loss: 0.2939
step 3410; step_loss: 0.4544
step 3420; step_loss: 0.3969
step 3430; step_loss: 0.4081
step 3440; step_loss: 0.4091
step 3450; step_loss: 0.3245
step 3460; step_loss: 0.2880
step 3470; step_loss: 0.5394
step 3480; step_loss: 0.4480
step 3490; step_loss: 0.4395
step 3500; step_loss: 0.3029
step 3510; step_loss: 0.3102
step 3520; step_loss: 0.2387
step 3530; step_loss: 0.3143
step 3540; step_loss: 0.3490
step 3550; step_loss: 0.3332
step 3560; step_loss: 0.3201
step 3570; step_loss: 0.3632
step 3580; step_loss: 0.3345
step 3590; step_loss: 0.3697
step 3600; step_loss: 0.5059
step 3610; step_loss: 0.3044
step 3620; step_loss: 0.3380
step 3630; step_loss: 0.2255
step 3640; step_loss: 0.4375
step 3650; step_loss: 0.3444
step 3660; step_loss: 0.3617
step 3670; step_loss: 0.3912
step 3680; step_loss: 0.4504
step 3690; step_loss: 0.3210
step 3700; step_loss: 0.2812
step 3710; step_loss: 0.3055
step 3720; step_loss: 0.4607
step 3730; step_loss: 0.3959
step 3740; step_loss: 0.3120
step 3750; step_loss: 0.3932
step 3760; step_loss: 0.3512
step 3770; step_loss: 0.4223
step 3780; step_loss: 0.3898
step 3790; step_loss: 0.3839
step 3800; step_loss: 0.3630
step 3810; step_loss: 0.2904
step 3820; step_loss: 0.3593
step 3830; step_loss: 0.2973
step 3840; step_loss: 0.2932
step 3850; step_loss: 0.3752
step 3860; step_loss: 0.2608
step 3870; step_loss: 0.3053
step 3880; step_loss: 0.3019
step 3890; step_loss: 0.2980
step 3900; step_loss: 0.4772
step 3910; step_loss: 0.3048
step 3920; step_loss: 0.3636
step 3930; step_loss: 0.3139
step 3940; step_loss: 0.2856
step 3950; step_loss: 0.3242
step 3960; step_loss: 0.2514
step 3970; step_loss: 0.3627
step 3980; step_loss: 0.3298
step 3990; step_loss: 0.3070

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 1.473 | 1.512 | 1.555 | 1.576 | 1.591 | 1.695 |

============================
Global step:         4000
Learning rate:       0.0050
Step-time (ms):     35.8653
Train loss avg:      0.3600
--------------------------
Val loss:            0.9621
srnn loss:           0.8094
============================

Saving the model...
done in 391.21 ms
step 4000; step_loss: 0.3718
step 4010; step_loss: 0.3450
step 4020; step_loss: 0.4650
step 4030; step_loss: 0.3460
step 4040; step_loss: 0.3400
step 4050; step_loss: 0.3435
step 4060; step_loss: 0.2909
step 4070; step_loss: 0.4450
step 4080; step_loss: 0.4095
step 4090; step_loss: 0.3148
step 4100; step_loss: 0.4459
step 4110; step_loss: 0.3148
step 4120; step_loss: 0.4157
step 4130; step_loss: 0.3097
step 4140; step_loss: 0.3461
step 4150; step_loss: 0.2724
step 4160; step_loss: 0.4360
step 4170; step_loss: 0.3083
step 4180; step_loss: 0.2971
step 4190; step_loss: 0.2774
step 4200; step_loss: 0.2752
step 4210; step_loss: 0.3767
step 4220; step_loss: 0.3283
step 4230; step_loss: 0.3070
step 4240; step_loss: 0.2603
step 4250; step_loss: 0.3399
step 4260; step_loss: 0.3131
step 4270; step_loss: 0.4501
step 4280; step_loss: 0.3496
step 4290; step_loss: 0.2767
step 4300; step_loss: 0.3362
step 4310; step_loss: 0.3691
step 4320; step_loss: 0.3322
step 4330; step_loss: 0.3050
step 4340; step_loss: 0.3366
step 4350; step_loss: 0.3663
step 4360; step_loss: 0.3268
step 4370; step_loss: 0.2874
step 4380; step_loss: 0.3289
step 4390; step_loss: 0.2343
step 4400; step_loss: 0.3409
step 4410; step_loss: 0.2728
step 4420; step_loss: 0.3042
step 4430; step_loss: 0.3655
step 4440; step_loss: 0.3143
step 4450; step_loss: 0.2682
step 4460; step_loss: 0.5384
step 4470; step_loss: 0.2747
step 4480; step_loss: 0.4218
step 4490; step_loss: 0.4024
step 4500; step_loss: 0.2924
step 4510; step_loss: 0.3379
step 4520; step_loss: 0.4560
step 4530; step_loss: 0.4701
step 4540; step_loss: 0.3171
step 4550; step_loss: 0.2572
step 4560; step_loss: 0.3945
step 4570; step_loss: 0.2931
step 4580; step_loss: 0.2609
step 4590; step_loss: 0.2917
step 4600; step_loss: 0.2893
step 4610; step_loss: 0.3270
step 4620; step_loss: 0.3641
step 4630; step_loss: 0.2835
step 4640; step_loss: 0.3082
step 4650; step_loss: 0.2680
step 4660; step_loss: 0.2840
step 4670; step_loss: 0.2795
step 4680; step_loss: 0.2927
step 4690; step_loss: 0.4021
step 4700; step_loss: 0.3543
step 4710; step_loss: 0.3007
step 4720; step_loss: 0.4481
step 4730; step_loss: 0.3536
step 4740; step_loss: 0.3579
step 4750; step_loss: 0.2675
step 4760; step_loss: 0.3815
step 4770; step_loss: 0.3396
step 4780; step_loss: 0.3773
step 4790; step_loss: 0.3207
step 4800; step_loss: 0.4719
step 4810; step_loss: 0.4071
step 4820; step_loss: 0.3655
step 4830; step_loss: 0.2886
step 4840; step_loss: 0.3860
step 4850; step_loss: 0.3605
step 4860; step_loss: 0.5349
step 4870; step_loss: 0.3345
step 4880; step_loss: 0.4287
step 4890; step_loss: 0.2816
step 4900; step_loss: 0.3269
step 4910; step_loss: 0.3117
step 4920; step_loss: 0.3143
step 4930; step_loss: 0.3650
step 4940; step_loss: 0.3268
step 4950; step_loss: 0.4585
step 4960; step_loss: 0.3673
step 4970; step_loss: 0.3416
step 4980; step_loss: 0.2770
step 4990; step_loss: 0.2773

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 1.448 | 1.488 | 1.529 | 1.550 | 1.566 | 1.671 |

============================
Global step:         5000
Learning rate:       0.0050
Step-time (ms):     36.0081
Train loss avg:      0.3430
--------------------------
Val loss:            0.8746
srnn loss:           0.7891
============================

Saving the model...
done in 393.33 ms
step 5000; step_loss: 0.3536
step 5010; step_loss: 0.3102
step 5020; step_loss: 0.2966
step 5030; step_loss: 0.5502
step 5040; step_loss: 0.3001
step 5050; step_loss: 0.3896
step 5060; step_loss: 0.4596
step 5070; step_loss: 0.3059
step 5080; step_loss: 0.2850
step 5090; step_loss: 0.3267
step 5100; step_loss: 0.2692
step 5110; step_loss: 0.3862
step 5120; step_loss: 0.3199
step 5130; step_loss: 0.3248
step 5140; step_loss: 0.2769
step 5150; step_loss: 0.3556
step 5160; step_loss: 0.3013
step 5170; step_loss: 0.3312
step 5180; step_loss: 0.3536
step 5190; step_loss: 0.3490
step 5200; step_loss: 0.4302
step 5210; step_loss: 0.3045
step 5220; step_loss: 0.3050
step 5230; step_loss: 0.4660
step 5240; step_loss: 0.2867
step 5250; step_loss: 0.3326
step 5260; step_loss: 0.2567
step 5270; step_loss: 0.3550
step 5280; step_loss: 0.3243
step 5290; step_loss: 0.3896
step 5300; step_loss: 0.3807
step 5310; step_loss: 0.2942
step 5320; step_loss: 0.3481
step 5330; step_loss: 0.2734
step 5340; step_loss: 0.3267
step 5350; step_loss: 0.4201
step 5360; step_loss: 0.3130
step 5370; step_loss: 0.3635
step 5380; step_loss: 0.2545
step 5390; step_loss: 0.2879
step 5400; step_loss: 0.2624
step 5410; step_loss: 0.3509
step 5420; step_loss: 0.3634
step 5430; step_loss: 0.3524
step 5440; step_loss: 0.2806
step 5450; step_loss: 0.3120
step 5460; step_loss: 0.2771
step 5470; step_loss: 0.2351
step 5480; step_loss: 0.3807
step 5490; step_loss: 0.2450
step 5500; step_loss: 0.3106
step 5510; step_loss: 0.3565
step 5520; step_loss: 0.3226
step 5530; step_loss: 0.3127
step 5540; step_loss: 0.2320
step 5550; step_loss: 0.3002
step 5560; step_loss: 0.2216
step 5570; step_loss: 0.3187
step 5580; step_loss: 0.3131
step 5590; step_loss: 0.3237
step 5600; step_loss: 0.2722
step 5610; step_loss: 0.2836
step 5620; step_loss: 0.2738
step 5630; step_loss: 0.3360
step 5640; step_loss: 0.2953
step 5650; step_loss: 0.2925
step 5660; step_loss: 0.3319
step 5670; step_loss: 0.2646
step 5680; step_loss: 0.4677
step 5690; step_loss: 0.2482
step 5700; step_loss: 0.3057
step 5710; step_loss: 0.3776
step 5720; step_loss: 0.3327
step 5730; step_loss: 0.3163
step 5740; step_loss: 0.3200
step 5750; step_loss: 0.3788
step 5760; step_loss: 0.2607
step 5770; step_loss: 0.3046
step 5780; step_loss: 0.2407
step 5790; step_loss: 0.3133
step 5800; step_loss: 0.2745
step 5810; step_loss: 0.4330
step 5820; step_loss: 0.2833
step 5830; step_loss: 0.2716
step 5840; step_loss: 0.2252
step 5850; step_loss: 0.4088
step 5860; step_loss: 0.3756
step 5870; step_loss: 0.3031
step 5880; step_loss: 0.3349
step 5890; step_loss: 0.2967
step 5900; step_loss: 0.5130
step 5910; step_loss: 0.2941
step 5920; step_loss: 0.2866
step 5930; step_loss: 0.2865
step 5940; step_loss: 0.5675
step 5950; step_loss: 0.2627
step 5960; step_loss: 0.3305
step 5970; step_loss: 0.2289
step 5980; step_loss: 0.3345
step 5990; step_loss: 0.2413

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 1.421 | 1.465 | 1.510 | 1.533 | 1.553 | 1.674 |

============================
Global step:         6000
Learning rate:       0.0050
Step-time (ms):     35.7425
Train loss avg:      0.3302
--------------------------
Val loss:            0.9801
srnn loss:           0.7704
============================

Saving the model...
done in 391.31 ms
step 6000; step_loss: 0.3411
step 6010; step_loss: 0.2297
step 6020; step_loss: 0.3418
step 6030; step_loss: 0.4100
step 6040; step_loss: 0.3465
step 6050; step_loss: 0.3174
step 6060; step_loss: 0.2648
step 6070; step_loss: 0.3967
step 6080; step_loss: 0.3979
step 6090; step_loss: 0.3157
step 6100; step_loss: 0.3801
step 6110; step_loss: 0.2776
step 6120; step_loss: 0.3004
step 6130; step_loss: 0.3028
step 6140; step_loss: 0.2302
step 6150; step_loss: 0.2689
step 6160; step_loss: 0.2146
step 6170; step_loss: 0.3312
step 6180; step_loss: 0.3131
step 6190; step_loss: 0.2974
step 6200; step_loss: 0.2822
step 6210; step_loss: 0.3308
step 6220; step_loss: 0.3029
step 6230; step_loss: 0.3469
step 6240; step_loss: 0.3814
step 6250; step_loss: 0.2967
step 6260; step_loss: 0.3498
step 6270; step_loss: 0.3606
step 6280; step_loss: 0.3265
step 6290; step_loss: 0.2838
step 6300; step_loss: 0.4368
step 6310; step_loss: 0.2794
step 6320; step_loss: 0.3269
step 6330; step_loss: 0.3413
step 6340; step_loss: 0.3206
step 6350; step_loss: 0.2660
step 6360; step_loss: 0.3530
step 6370; step_loss: 0.3599
step 6380; step_loss: 0.4637
step 6390; step_loss: 0.3970
step 6400; step_loss: 0.2075
step 6410; step_loss: 0.3399
step 6420; step_loss: 0.2439
step 6430; step_loss: 0.2888
step 6440; step_loss: 0.5544
step 6450; step_loss: 0.3267
step 6460; step_loss: 0.2773
step 6470; step_loss: 0.3468
step 6480; step_loss: 0.3482
step 6490; step_loss: 0.2652
step 6500; step_loss: 0.3288
step 6510; step_loss: 0.3226
step 6520; step_loss: 0.2918
step 6530; step_loss: 0.2517
step 6540; step_loss: 0.2741
step 6550; step_loss: 0.4557
step 6560; step_loss: 0.2868
step 6570; step_loss: 0.2522
step 6580; step_loss: 0.3610
step 6590; step_loss: 0.5274
step 6600; step_loss: 0.3088
step 6610; step_loss: 0.4057
step 6620; step_loss: 0.3143
step 6630; step_loss: 0.3080
step 6640; step_loss: 0.2743
step 6650; step_loss: 0.3252
step 6660; step_loss: 0.3166
step 6670; step_loss: 0.3249
step 6680; step_loss: 0.3703
step 6690; step_loss: 0.3247
step 6700; step_loss: 0.2521
step 6710; step_loss: 0.3494
step 6720; step_loss: 0.3314
step 6730; step_loss: 0.2690
step 6740; step_loss: 0.2957
step 6750; step_loss: 0.2959
step 6760; step_loss: 0.4262
step 6770; step_loss: 0.2798
step 6780; step_loss: 0.3321
step 6790; step_loss: 0.2829
step 6800; step_loss: 0.4671
step 6810; step_loss: 0.3249
step 6820; step_loss: 0.4121
step 6830; step_loss: 0.2827
step 6840; step_loss: 0.3706
step 6850; step_loss: 0.4404
step 6860; step_loss: 0.3087
step 6870; step_loss: 0.2491
step 6880; step_loss: 0.2946
step 6890; step_loss: 0.2806
step 6900; step_loss: 0.4009
step 6910; step_loss: 0.2663
step 6920; step_loss: 0.4017
step 6930; step_loss: 0.2677
step 6940; step_loss: 0.2972
step 6950; step_loss: 0.2666
step 6960; step_loss: 0.3504
step 6970; step_loss: 0.2923
step 6980; step_loss: 0.2951
step 6990; step_loss: 0.3906

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 1.390 | 1.437 | 1.485 | 1.510 | 1.531 | 1.671 |

============================
Global step:         7000
Learning rate:       0.0050
Step-time (ms):     35.8030
Train loss avg:      0.3215
--------------------------
Val loss:            0.9551
srnn loss:           0.7563
============================

Saving the model...
done in 379.79 ms
step 7000; step_loss: 0.3371
step 7010; step_loss: 0.3329
step 7020; step_loss: 0.3396
step 7030; step_loss: 0.2592
step 7040; step_loss: 0.3114
step 7050; step_loss: 0.3034
step 7060; step_loss: 0.2420
step 7070; step_loss: 0.3514
step 7080; step_loss: 0.3240
step 7090; step_loss: 0.2725
step 7100; step_loss: 0.3788
step 7110; step_loss: 0.2354
step 7120; step_loss: 0.3904
step 7130; step_loss: 0.3034
step 7140; step_loss: 0.2640
step 7150; step_loss: 0.2433
step 7160; step_loss: 0.3529
step 7170; step_loss: 0.2382
step 7180; step_loss: 0.3460
step 7190; step_loss: 0.3178
step 7200; step_loss: 0.3663
step 7210; step_loss: 0.3426
step 7220; step_loss: 0.4629
step 7230; step_loss: 0.2212
step 7240; step_loss: 0.2658
step 7250; step_loss: 0.3336
step 7260; step_loss: 0.2334
step 7270; step_loss: 0.2407
step 7280; step_loss: 0.3676
step 7290; step_loss: 0.3352
step 7300; step_loss: 0.2752
step 7310; step_loss: 0.2603
step 7320; step_loss: 0.2839
step 7330; step_loss: 0.3181
step 7340; step_loss: 0.3710
step 7350; step_loss: 0.3312
step 7360; step_loss: 0.3195
step 7370; step_loss: 0.3400
step 7380; step_loss: 0.3127
step 7390; step_loss: 0.2819
step 7400; step_loss: 0.2471
step 7410; step_loss: 0.3227
step 7420; step_loss: 0.2546
step 7430; step_loss: 0.2299
step 7440; step_loss: 0.3410
step 7450; step_loss: 0.3394
step 7460; step_loss: 0.2943
step 7470; step_loss: 0.3734
step 7480; step_loss: 0.2997
step 7490; step_loss: 0.2760
step 7500; step_loss: 0.2507
step 7510; step_loss: 0.2841
step 7520; step_loss: 0.3463
step 7530; step_loss: 0.3644
step 7540; step_loss: 0.3137
step 7550; step_loss: 0.3266
step 7560; step_loss: 0.2781
step 7570; step_loss: 0.3166
step 7580; step_loss: 0.3512
step 7590; step_loss: 0.3264
step 7600; step_loss: 0.2815
step 7610; step_loss: 0.3177
step 7620; step_loss: 0.3776
step 7630; step_loss: 0.2234
step 7640; step_loss: 0.3223
step 7650; step_loss: 0.2881
step 7660; step_loss: 0.3018
step 7670; step_loss: 0.2831
step 7680; step_loss: 0.2412
step 7690; step_loss: 0.2341
step 7700; step_loss: 0.3551
step 7710; step_loss: 0.3365
step 7720; step_loss: 0.2863
step 7730; step_loss: 0.2633
step 7740; step_loss: 0.3407
step 7750; step_loss: 0.2795
step 7760; step_loss: 0.3760
step 7770; step_loss: 0.2793
step 7780; step_loss: 0.2173
step 7790; step_loss: 0.2959
step 7800; step_loss: 0.2848
step 7810; step_loss: 0.3699
step 7820; step_loss: 0.3300
step 7830; step_loss: 0.2706
step 7840; step_loss: 0.2479
step 7850; step_loss: 0.2811
step 7860; step_loss: 0.2436
step 7870; step_loss: 0.2822
step 7880; step_loss: 0.2341
step 7890; step_loss: 0.2514
step 7900; step_loss: 0.3504
step 7910; step_loss: 0.3675
step 7920; step_loss: 0.3099
step 7930; step_loss: 0.2686
step 7940; step_loss: 0.2307
step 7950; step_loss: 0.3412
step 7960; step_loss: 0.3490
step 7970; step_loss: 0.3179
step 7980; step_loss: 0.2697
step 7990; step_loss: 0.2538

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 1.368 | 1.418 | 1.466 | 1.494 | 1.522 | 1.693 |

============================
Global step:         8000
Learning rate:       0.0050
Step-time (ms):     35.9129
Train loss avg:      0.3049
--------------------------
Val loss:            0.8208
srnn loss:           0.7429
============================

Saving the model...
done in 377.95 ms
step 8000; step_loss: 0.2496
step 8010; step_loss: 0.2289
step 8020; step_loss: 0.2848
step 8030; step_loss: 0.2411
step 8040; step_loss: 0.3330
step 8050; step_loss: 0.2069
step 8060; step_loss: 0.2450
step 8070; step_loss: 0.2464
step 8080; step_loss: 0.2712
step 8090; step_loss: 0.3126
step 8100; step_loss: 0.3196
step 8110; step_loss: 0.2197
step 8120; step_loss: 0.3727
step 8130; step_loss: 0.4333
step 8140; step_loss: 0.4203
step 8150; step_loss: 0.2813
step 8160; step_loss: 0.2919
step 8170; step_loss: 0.2106
step 8180; step_loss: 0.3340
step 8190; step_loss: 0.2871
step 8200; step_loss: 0.5974
step 8210; step_loss: 0.2695
step 8220; step_loss: 0.2290
step 8230; step_loss: 0.3670
step 8240; step_loss: 0.3664
step 8250; step_loss: 0.4450
step 8260; step_loss: 0.3754
step 8270; step_loss: 0.2712
step 8280; step_loss: 0.3261
step 8290; step_loss: 0.2555
step 8300; step_loss: 0.2943
step 8310; step_loss: 0.2439
step 8320; step_loss: 0.2817
step 8330; step_loss: 0.2475
step 8340; step_loss: 0.2336
step 8350; step_loss: 0.2204
step 8360; step_loss: 0.4406
step 8370; step_loss: 0.2464
step 8380; step_loss: 0.1955
step 8390; step_loss: 0.2521
step 8400; step_loss: 0.2119
step 8410; step_loss: 0.2279
step 8420; step_loss: 0.2621
step 8430; step_loss: 0.2634
step 8440; step_loss: 0.2652
step 8450; step_loss: 0.2482
step 8460; step_loss: 0.2667
step 8470; step_loss: 0.2445
step 8480; step_loss: 0.3912
step 8490; step_loss: 0.2836
step 8500; step_loss: 0.2197
step 8510; step_loss: 0.3765
step 8520; step_loss: 0.2803
step 8530; step_loss: 0.2826
step 8540; step_loss: 0.2367
step 8550; step_loss: 0.3875
step 8560; step_loss: 0.2688
step 8570; step_loss: 0.3223
step 8580; step_loss: 0.3541
step 8590; step_loss: 0.2755
step 8600; step_loss: 0.2550
step 8610; step_loss: 0.2655
step 8620; step_loss: 0.3184
step 8630; step_loss: 0.3338
step 8640; step_loss: 0.4457
step 8650; step_loss: 0.2746
step 8660; step_loss: 0.3075
step 8670; step_loss: 0.2532
step 8680; step_loss: 0.2801
step 8690; step_loss: 0.3024
step 8700; step_loss: 0.2374
step 8710; step_loss: 0.2273
step 8720; step_loss: 0.2848
step 8730; step_loss: 0.3615
step 8740; step_loss: 0.2815
step 8750; step_loss: 0.3333
step 8760; step_loss: 0.2878
step 8770; step_loss: 0.2601
step 8780; step_loss: 0.1918
step 8790; step_loss: 0.2918
step 8800; step_loss: 0.4976
step 8810; step_loss: 0.3296
step 8820; step_loss: 0.2081
step 8830; step_loss: 0.3222
step 8840; step_loss: 0.3168
step 8850; step_loss: 0.2734
step 8860; step_loss: 0.3753
step 8870; step_loss: 0.2497
step 8880; step_loss: 0.2949
step 8890; step_loss: 0.2889
step 8900; step_loss: 0.4079
step 8910; step_loss: 0.2576
step 8920; step_loss: 0.2984
step 8930; step_loss: 0.2336
step 8940; step_loss: 0.3080
step 8950; step_loss: 0.3807
step 8960; step_loss: 0.2783
step 8970; step_loss: 0.3121
step 8980; step_loss: 0.4380
step 8990; step_loss: 0.2317

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 1.344 | 1.396 | 1.442 | 1.470 | 1.497 | 1.674 |

============================
Global step:         9000
Learning rate:       0.0050
Step-time (ms):     35.8083
Train loss avg:      0.2977
--------------------------
Val loss:            0.7690
srnn loss:           0.7253
============================

Saving the model...
done in 424.23 ms
step 9000; step_loss: 0.2908
step 9010; step_loss: 0.3432
step 9020; step_loss: 0.2362
step 9030; step_loss: 0.3081
step 9040; step_loss: 0.2029
step 9050; step_loss: 0.2481
step 9060; step_loss: 0.2885
step 9070; step_loss: 0.2883
step 9080; step_loss: 0.2796
step 9090; step_loss: 0.2722
step 9100; step_loss: 0.3143
step 9110; step_loss: 0.2930
step 9120; step_loss: 0.2793
step 9130; step_loss: 0.3813
step 9140; step_loss: 0.6041
step 9150; step_loss: 0.3769
step 9160; step_loss: 0.2299
step 9170; step_loss: 0.3267
step 9180; step_loss: 0.2496
step 9190; step_loss: 0.3320
step 9200; step_loss: 0.2549
step 9210; step_loss: 0.3284
step 9220; step_loss: 0.2742
step 9230; step_loss: 0.2958
step 9240; step_loss: 0.3743
step 9250; step_loss: 0.2245
step 9260; step_loss: 0.3110
step 9270; step_loss: 0.2459
step 9280; step_loss: 0.2481
step 9290; step_loss: 0.2967
step 9300; step_loss: 0.2480
step 9310; step_loss: 0.3016
step 9320; step_loss: 0.2477
step 9330; step_loss: 0.3337
step 9340; step_loss: 0.2474
step 9350; step_loss: 0.2706
step 9360; step_loss: 0.2513
step 9370; step_loss: 0.3106
step 9380; step_loss: 0.3657
step 9390; step_loss: 0.2457
step 9400; step_loss: 0.2847
step 9410; step_loss: 0.3326
step 9420; step_loss: 0.3319
step 9430; step_loss: 0.1988
step 9440; step_loss: 0.3076
step 9450; step_loss: 0.2867
step 9460; step_loss: 0.3259
step 9470; step_loss: 0.3968
step 9480; step_loss: 0.2804
step 9490; step_loss: 0.2801
step 9500; step_loss: 0.2475
step 9510; step_loss: 0.2645
step 9520; step_loss: 0.2503
step 9530; step_loss: 0.2886
step 9540; step_loss: 0.3195
step 9550; step_loss: 0.2935
step 9560; step_loss: 0.2487
step 9570; step_loss: 0.3255
step 9580; step_loss: 0.2299
step 9590; step_loss: 0.2818
step 9600; step_loss: 0.2826
step 9610; step_loss: 0.2735
step 9620; step_loss: 0.2295
step 9630; step_loss: 0.2576
step 9640; step_loss: 0.3066
step 9650; step_loss: 0.2753
step 9660; step_loss: 0.2423
step 9670; step_loss: 0.3128
step 9680; step_loss: 0.2241
step 9690; step_loss: 0.2062
step 9700; step_loss: 0.3533
step 9710; step_loss: 0.3141
step 9720; step_loss: 0.3174
step 9730; step_loss: 0.2295
step 9740; step_loss: 0.2442
step 9750; step_loss: 0.3161
step 9760; step_loss: 0.2117
step 9770; step_loss: 0.2826
step 9780; step_loss: 0.2197
step 9790; step_loss: 0.3024
step 9800; step_loss: 0.3419
step 9810; step_loss: 0.2491
step 9820; step_loss: 0.2779
step 9830; step_loss: 0.2419
step 9840; step_loss: 0.2382
step 9850; step_loss: 0.2909
step 9860; step_loss: 0.3098
step 9870; step_loss: 0.2943
step 9880; step_loss: 0.2845
step 9890; step_loss: 0.2934
step 9900; step_loss: 0.2888
step 9910; step_loss: 0.3150
step 9920; step_loss: 0.2033
step 9930; step_loss: 0.4339
step 9940; step_loss: 0.3552
step 9950; step_loss: 0.2745
step 9960; step_loss: 0.2923
step 9970; step_loss: 0.2471
step 9980; step_loss: 0.2592
step 9990; step_loss: 0.2575

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 1.321 | 1.374 | 1.418 | 1.446 | 1.476 | 1.670 |

============================
Global step:         10000
Learning rate:       0.0047
Step-time (ms):     35.8735
Train loss avg:      0.2919
--------------------------
Val loss:            0.8493
srnn loss:           0.7140
============================

Saving the model...
done in 387.83 ms
step 10000; step_loss: 0.2632
step 10010; step_loss: 0.3247
step 10020; step_loss: 0.2957
step 10030; step_loss: 0.4660
step 10040; step_loss: 0.3128
step 10050; step_loss: 0.2354
step 10060; step_loss: 0.2767
step 10070; step_loss: 0.3181
step 10080; step_loss: 0.2443
step 10090; step_loss: 0.2367
step 10100; step_loss: 0.2865
step 10110; step_loss: 0.3992
step 10120; step_loss: 0.2818
step 10130; step_loss: 0.2003
step 10140; step_loss: 0.2311
step 10150; step_loss: 0.2467
step 10160; step_loss: 0.2562
step 10170; step_loss: 0.2556
step 10180; step_loss: 0.2801
step 10190; step_loss: 0.2958
step 10200; step_loss: 0.2719
step 10210; step_loss: 0.2446
step 10220; step_loss: 0.2746
step 10230; step_loss: 0.2838
step 10240; step_loss: 0.2272
step 10250; step_loss: 0.3451
step 10260; step_loss: 0.3507
step 10270; step_loss: 0.2006
step 10280; step_loss: 0.2785
step 10290; step_loss: 0.2430
step 10300; step_loss: 0.3104
step 10310; step_loss: 0.2655
step 10320; step_loss: 0.2675
step 10330; step_loss: 0.2808
step 10340; step_loss: 0.2703
step 10350; step_loss: 0.3350
step 10360; step_loss: 0.3191
step 10370; step_loss: 0.2680
step 10380; step_loss: 0.3430
step 10390; step_loss: 0.2978
step 10400; step_loss: 0.2549
step 10410; step_loss: 0.2801
step 10420; step_loss: 0.2848
step 10430; step_loss: 0.2567
step 10440; step_loss: 0.2397
step 10450; step_loss: 0.2883
step 10460; step_loss: 0.2175
step 10470; step_loss: 0.2889
step 10480; step_loss: 0.3657
step 10490; step_loss: 0.2726
step 10500; step_loss: 0.2781
step 10510; step_loss: 0.3227
step 10520; step_loss: 0.2953
step 10530; step_loss: 0.2972
step 10540; step_loss: 0.3539
step 10550; step_loss: 0.2910
step 10560; step_loss: 0.2873
step 10570; step_loss: 0.2474
step 10580; step_loss: 0.2033
step 10590; step_loss: 0.3478
step 10600; step_loss: 0.2080
step 10610; step_loss: 0.2742
step 10620; step_loss: 0.3375
step 10630; step_loss: 0.2618
step 10640; step_loss: 0.4127
step 10650; step_loss: 0.2612
step 10660; step_loss: 0.2405
step 10670; step_loss: 0.3874
step 10680; step_loss: 0.2815
step 10690; step_loss: 0.2662
step 10700; step_loss: 0.2189
step 10710; step_loss: 0.3035
step 10720; step_loss: 0.3109
step 10730; step_loss: 0.3224
step 10740; step_loss: 0.3296
step 10750; step_loss: 0.2522
step 10760; step_loss: 0.2625
step 10770; step_loss: 0.2462
step 10780; step_loss: 0.2371
step 10790; step_loss: 0.3902
step 10800; step_loss: 0.3137
step 10810; step_loss: 0.2465
step 10820; step_loss: 0.2693
step 10830; step_loss: 0.2439
step 10840; step_loss: 0.2648
step 10850; step_loss: 0.2388
step 10860; step_loss: 0.2119
step 10870; step_loss: 0.2328
step 10880; step_loss: 0.2735
step 10890; step_loss: 0.3001
step 10900; step_loss: 0.2158
step 10910; step_loss: 0.3791
step 10920; step_loss: 0.3274
step 10930; step_loss: 0.2786
step 10940; step_loss: 0.3145
step 10950; step_loss: 0.3888
step 10960; step_loss: 0.2251
step 10970; step_loss: 0.3219
step 10980; step_loss: 0.2634
step 10990; step_loss: 0.2484

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 1.306 | 1.362 | 1.408 | 1.437 | 1.470 | 1.668 |

============================
Global step:         11000
Learning rate:       0.0047
Step-time (ms):     35.9514
Train loss avg:      0.2871
--------------------------
Val loss:            0.8036
srnn loss:           0.7026
============================

Saving the model...
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py:963: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to delete files with this prefix.
W1128 21:07:11.708347 140392724273024 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py:963: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to delete files with this prefix.
done in 382.28 ms
step 11000; step_loss: 0.2737
step 11010; step_loss: 0.2511
step 11020; step_loss: 0.2791
step 11030; step_loss: 0.2746
step 11040; step_loss: 0.3698
step 11050; step_loss: 0.3278
step 11060; step_loss: 0.2321
step 11070; step_loss: 0.2457
step 11080; step_loss: 0.2828
step 11090; step_loss: 0.2726
step 11100; step_loss: 0.4665
step 11110; step_loss: 0.2674
step 11120; step_loss: 0.2109
step 11130; step_loss: 0.2359
step 11140; step_loss: 0.2626
step 11150; step_loss: 0.2225
step 11160; step_loss: 0.2958
step 11170; step_loss: 0.4231
step 11180; step_loss: 0.2324
step 11190; step_loss: 0.2735
step 11200; step_loss: 0.2562
step 11210; step_loss: 0.3271
step 11220; step_loss: 0.2219
step 11230; step_loss: 0.2371
step 11240; step_loss: 0.2339
step 11250; step_loss: 0.3254
step 11260; step_loss: 0.1849
step 11270; step_loss: 0.2218
step 11280; step_loss: 0.2711
step 11290; step_loss: 0.4073
step 11300; step_loss: 0.2696
step 11310; step_loss: 0.3247
step 11320; step_loss: 0.3393
step 11330; step_loss: 0.3134
step 11340; step_loss: 0.2537
step 11350; step_loss: 0.2191
step 11360; step_loss: 0.2758
step 11370; step_loss: 0.2068
step 11380; step_loss: 0.2962
step 11390; step_loss: 0.3264
step 11400; step_loss: 0.2205
step 11410; step_loss: 0.3245
step 11420; step_loss: 0.3074
step 11430; step_loss: 0.2299
step 11440; step_loss: 0.2959
step 11450; step_loss: 0.3464
step 11460; step_loss: 0.3126
step 11470; step_loss: 0.4166
step 11480; step_loss: 0.3687
step 11490; step_loss: 0.2299
step 11500; step_loss: 0.3229
step 11510; step_loss: 0.2713
step 11520; step_loss: 0.1990
step 11530; step_loss: 0.2332
step 11540; step_loss: 0.3631
step 11550; step_loss: 0.2300
step 11560; step_loss: 0.2929
step 11570; step_loss: 0.2575
step 11580; step_loss: 0.2264
step 11590; step_loss: 0.2674
step 11600; step_loss: 0.2457
step 11610; step_loss: 0.2251
step 11620; step_loss: 0.3346
step 11630; step_loss: 0.3332
step 11640; step_loss: 0.2441
step 11650; step_loss: 0.2777
step 11660; step_loss: 0.3085
step 11670; step_loss: 0.2754
step 11680; step_loss: 0.2973
step 11690; step_loss: 0.2911
step 11700; step_loss: 0.3069
step 11710; step_loss: 0.3399
step 11720; step_loss: 0.2310
step 11730; step_loss: 0.3760
step 11740; step_loss: 0.2902
step 11750; step_loss: 0.2731
step 11760; step_loss: 0.2764
step 11770; step_loss: 0.3020
step 11780; step_loss: 0.2731
step 11790; step_loss: 0.2159
step 11800; step_loss: 0.2030
step 11810; step_loss: 0.2585
step 11820; step_loss: 0.2272
step 11830; step_loss: 0.2102
step 11840; step_loss: 0.3601
step 11850; step_loss: 0.3623
step 11860; step_loss: 0.3260
step 11870; step_loss: 0.3235
step 11880; step_loss: 0.2798
step 11890; step_loss: 0.2139
step 11900; step_loss: 0.2829
step 11910; step_loss: 0.2404
step 11920; step_loss: 0.2735
step 11930; step_loss: 0.3069
step 11940; step_loss: 0.2528
step 11950; step_loss: 0.2980
step 11960; step_loss: 0.2763
step 11970; step_loss: 0.2310
step 11980; step_loss: 0.2469
step 11990; step_loss: 0.1868

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 1.285 | 1.343 | 1.389 | 1.418 | 1.453 | 1.656 |

============================
Global step:         12000
Learning rate:       0.0047
Step-time (ms):     35.9120
Train loss avg:      0.2823
--------------------------
Val loss:            0.7833
srnn loss:           0.6949
============================

Saving the model...
done in 378.88 ms
step 12000; step_loss: 0.3073
step 12010; step_loss: 0.3291
step 12020; step_loss: 0.2731
step 12030; step_loss: 0.2930
step 12040; step_loss: 0.3774
step 12050; step_loss: 0.2429
step 12060; step_loss: 0.3162
step 12070; step_loss: 0.2086
step 12080; step_loss: 0.3076
step 12090; step_loss: 0.2635
step 12100; step_loss: 0.2818
step 12110; step_loss: 0.2690
step 12120; step_loss: 0.2887
step 12130; step_loss: 0.2353
step 12140; step_loss: 0.1909
step 12150; step_loss: 0.3218
step 12160; step_loss: 0.2438
step 12170; step_loss: 0.2474
step 12180; step_loss: 0.3054
step 12190; step_loss: 0.2858
step 12200; step_loss: 0.2306
step 12210; step_loss: 0.2346
step 12220; step_loss: 0.2275
step 12230; step_loss: 0.2583
step 12240; step_loss: 0.2433
step 12250; step_loss: 0.1959
step 12260; step_loss: 0.3126
step 12270; step_loss: 0.2288
step 12280; step_loss: 0.2547
step 12290; step_loss: 0.2569
step 12300; step_loss: 0.2287
step 12310; step_loss: 0.4451
step 12320; step_loss: 0.2754
step 12330; step_loss: 0.1979
step 12340; step_loss: 0.3888
step 12350; step_loss: 0.2891
step 12360; step_loss: 0.3458
step 12370; step_loss: 0.2755
step 12380; step_loss: 0.3132
step 12390; step_loss: 0.3007
step 12400; step_loss: 0.2984
step 12410; step_loss: 0.3040
step 12420; step_loss: 0.2375
step 12430; step_loss: 0.2206
step 12440; step_loss: 0.2638
step 12450; step_loss: 0.2457
step 12460; step_loss: 0.2499
step 12470; step_loss: 0.3707
step 12480; step_loss: 0.2723
step 12490; step_loss: 0.2960
step 12500; step_loss: 0.2336
step 12510; step_loss: 0.2793
step 12520; step_loss: 0.4147
step 12530; step_loss: 0.2656
step 12540; step_loss: 0.2500
step 12550; step_loss: 0.2877
step 12560; step_loss: 0.2652
step 12570; step_loss: 0.2379
step 12580; step_loss: 0.2835
step 12590; step_loss: 0.2546
step 12600; step_loss: 0.3606
step 12610; step_loss: 0.2932
step 12620; step_loss: 0.3068
step 12630; step_loss: 0.2478
step 12640; step_loss: 0.2310
step 12650; step_loss: 0.2808
step 12660; step_loss: 0.2130
step 12670; step_loss: 0.3829
step 12680; step_loss: 0.3307
step 12690; step_loss: 0.2808
step 12700; step_loss: 0.3195
step 12710; step_loss: 0.2651
step 12720; step_loss: 0.3486
step 12730; step_loss: 0.3129
step 12740; step_loss: 0.2862
step 12750; step_loss: 0.2460
step 12760; step_loss: 0.1793
step 12770; step_loss: 0.2927
step 12780; step_loss: 0.4271
step 12790; step_loss: 0.3243
step 12800; step_loss: 0.3050
step 12810; step_loss: 0.2749
step 12820; step_loss: 0.2316
step 12830; step_loss: 0.2293
step 12840; step_loss: 0.3174
step 12850; step_loss: 0.2447
step 12860; step_loss: 0.2087
step 12870; step_loss: 0.2418
step 12880; step_loss: 0.2610
step 12890; step_loss: 0.3492
step 12900; step_loss: 0.2311
step 12910; step_loss: 0.3350
step 12920; step_loss: 0.2446
step 12930; step_loss: 0.3362
step 12940; step_loss: 0.3261
step 12950; step_loss: 0.2669
step 12960; step_loss: 0.2401
step 12970; step_loss: 0.2353
step 12980; step_loss: 0.3020
step 12990; step_loss: 0.2903

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 1.267 | 1.328 | 1.379 | 1.411 | 1.453 | 1.666 |

============================
Global step:         13000
Learning rate:       0.0047
Step-time (ms):     35.7712
Train loss avg:      0.2757
--------------------------
Val loss:            0.8051
srnn loss:           0.6872
============================

Saving the model...
done in 368.63 ms
step 13000; step_loss: 0.2316
step 13010; step_loss: 0.2203
step 13020; step_loss: 0.2177
step 13030; step_loss: 0.2772
step 13040; step_loss: 0.2702
step 13050; step_loss: 0.2111
step 13060; step_loss: 0.2514
step 13070; step_loss: 0.3217
step 13080; step_loss: 0.2273
step 13090; step_loss: 0.2424
step 13100; step_loss: 0.2430
step 13110; step_loss: 0.2928
step 13120; step_loss: 0.3548
step 13130; step_loss: 0.2281
step 13140; step_loss: 0.3055
step 13150; step_loss: 0.2816
step 13160; step_loss: 0.2938
step 13170; step_loss: 0.2363
step 13180; step_loss: 0.2416
step 13190; step_loss: 0.2449
step 13200; step_loss: 0.2291
step 13210; step_loss: 0.2608
step 13220; step_loss: 0.2926
step 13230; step_loss: 0.2805
step 13240; step_loss: 0.3003
step 13250; step_loss: 0.2300
step 13260; step_loss: 0.2383
step 13270; step_loss: 0.2313
step 13280; step_loss: 0.2047
step 13290; step_loss: 0.3066
step 13300; step_loss: 0.3628
step 13310; step_loss: 0.2023
step 13320; step_loss: 0.3938
step 13330; step_loss: 0.2891
step 13340; step_loss: 0.2609
step 13350; step_loss: 0.2701
step 13360; step_loss: 0.1700
step 13370; step_loss: 0.3438
step 13380; step_loss: 0.2008
step 13390; step_loss: 0.3320
step 13400; step_loss: 0.2413
step 13410; step_loss: 0.2266
step 13420; step_loss: 0.2504
step 13430; step_loss: 0.2290
step 13440; step_loss: 0.4090
step 13450; step_loss: 0.2853
step 13460; step_loss: 0.2692
step 13470; step_loss: 0.3463
step 13480; step_loss: 0.2665
step 13490; step_loss: 0.2393
step 13500; step_loss: 0.2938
step 13510; step_loss: 0.2812
step 13520; step_loss: 0.2068
step 13530; step_loss: 0.2629
step 13540; step_loss: 0.2755
step 13550; step_loss: 0.3360
step 13560; step_loss: 0.3010
step 13570; step_loss: 0.2536
step 13580; step_loss: 0.2790
step 13590; step_loss: 0.3656
step 13600; step_loss: 0.3106
step 13610; step_loss: 0.2819
step 13620; step_loss: 0.2095
step 13630; step_loss: 0.2686
step 13640; step_loss: 0.2136
step 13650; step_loss: 0.1795
step 13660; step_loss: 0.2172
step 13670; step_loss: 0.2024
step 13680; step_loss: 0.2702
step 13690; step_loss: 0.3400
step 13700; step_loss: 0.2327
step 13710; step_loss: 0.2144
step 13720; step_loss: 0.4164
step 13730; step_loss: 0.2498
step 13740; step_loss: 0.2882
step 13750; step_loss: 0.3124
step 13760; step_loss: 0.2779
step 13770; step_loss: 0.2758
step 13780; step_loss: 0.2113
step 13790; step_loss: 0.2153
step 13800; step_loss: 0.2207
step 13810; step_loss: 0.2709
step 13820; step_loss: 0.2015
step 13830; step_loss: 0.2233
step 13840; step_loss: 0.3168
step 13850; step_loss: 0.3467
step 13860; step_loss: 0.3112
step 13870; step_loss: 0.3217
step 13880; step_loss: 0.2890
step 13890; step_loss: 0.2482
step 13900; step_loss: 0.2775
step 13910; step_loss: 0.3955
step 13920; step_loss: 0.2423
step 13930; step_loss: 0.2526
step 13940; step_loss: 0.2752
step 13950; step_loss: 0.2918
step 13960; step_loss: 0.1983
step 13970; step_loss: 0.2219
step 13980; step_loss: 0.2549
step 13990; step_loss: 0.2515

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 1.246 | 1.308 | 1.364 | 1.398 | 1.441 | 1.668 |

============================
Global step:         14000
Learning rate:       0.0047
Step-time (ms):     35.7465
Train loss avg:      0.2683
--------------------------
Val loss:            0.7858
srnn loss:           0.6846
============================

Saving the model...
done in 378.03 ms
step 14000; step_loss: 0.2257
step 14010; step_loss: 0.2542
step 14020; step_loss: 0.2507
step 14030; step_loss: 0.2177
step 14040; step_loss: 0.2626
step 14050; step_loss: 0.3900
step 14060; step_loss: 0.1972
step 14070; step_loss: 0.2522
step 14080; step_loss: 0.2831
step 14090; step_loss: 0.2828
step 14100; step_loss: 0.2934
step 14110; step_loss: 0.3110
step 14120; step_loss: 0.2241
step 14130; step_loss: 0.2911
step 14140; step_loss: 0.3075
step 14150; step_loss: 0.2167
step 14160; step_loss: 0.2739
step 14170; step_loss: 0.2241
step 14180; step_loss: 0.2292
step 14190; step_loss: 0.2751
step 14200; step_loss: 0.2845
step 14210; step_loss: 0.2692
step 14220; step_loss: 0.2604
step 14230; step_loss: 0.2034
step 14240; step_loss: 0.2341
step 14250; step_loss: 0.2304
step 14260; step_loss: 0.2930
step 14270; step_loss: 0.2513
step 14280; step_loss: 0.2830
step 14290; step_loss: 0.4491
step 14300; step_loss: 0.3255
step 14310; step_loss: 0.2665
step 14320; step_loss: 0.2281
step 14330; step_loss: 0.2822
step 14340; step_loss: 0.2360
step 14350; step_loss: 0.3436
step 14360; step_loss: 0.2413
step 14370; step_loss: 0.2502
step 14380; step_loss: 0.2563
step 14390; step_loss: 0.2594
step 14400; step_loss: 0.2916
step 14410; step_loss: 0.2226
step 14420; step_loss: 0.2350
step 14430; step_loss: 0.2270
step 14440; step_loss: 0.2289
step 14450; step_loss: 0.2156
step 14460; step_loss: 0.2315
step 14470; step_loss: 0.2266
step 14480; step_loss: 0.2996
step 14490; step_loss: 0.3858
step 14500; step_loss: 0.1945
step 14510; step_loss: 0.2910
step 14520; step_loss: 0.2924
step 14530; step_loss: 0.2329
step 14540; step_loss: 0.3005
step 14550; step_loss: 0.3487
step 14560; step_loss: 0.2226
step 14570; step_loss: 0.2386
step 14580; step_loss: 0.1942
step 14590; step_loss: 0.2641
step 14600; step_loss: 0.1667
step 14610; step_loss: 0.2304
step 14620; step_loss: 0.3508
step 14630; step_loss: 0.2764
step 14640; step_loss: 0.2405
step 14650; step_loss: 0.2492
step 14660; step_loss: 0.2678
step 14670; step_loss: 0.2808
step 14680; step_loss: 0.2620
step 14690; step_loss: 0.2411
step 14700; step_loss: 0.3321
step 14710; step_loss: 0.2279
step 14720; step_loss: 0.1736
step 14730; step_loss: 0.2750
step 14740; step_loss: 0.3235
step 14750; step_loss: 0.2618
step 14760; step_loss: 0.2746
step 14770; step_loss: 0.3331
step 14780; step_loss: 0.3111
step 14790; step_loss: 0.2102
step 14800; step_loss: 0.3055
step 14810; step_loss: 0.2548
step 14820; step_loss: 0.2338
step 14830; step_loss: 0.2430
step 14840; step_loss: 0.2672
step 14850; step_loss: 0.2793
step 14860; step_loss: 0.2414
step 14870; step_loss: 0.2544
step 14880; step_loss: 0.2244
step 14890; step_loss: 0.2940
step 14900; step_loss: 0.1584
step 14910; step_loss: 0.2276
step 14920; step_loss: 0.2083
step 14930; step_loss: 0.3414
step 14940; step_loss: 0.2765
step 14950; step_loss: 0.2335
step 14960; step_loss: 0.2329
step 14970; step_loss: 0.2426
step 14980; step_loss: 0.2908
step 14990; step_loss: 0.2746

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 1.227 | 1.290 | 1.349 | 1.384 | 1.434 | 1.663 |

============================
Global step:         15000
Learning rate:       0.0047
Step-time (ms):     35.7589
Train loss avg:      0.2623
--------------------------
Val loss:            0.8153
srnn loss:           0.6738
============================

Saving the model...
done in 380.95 ms
step 15000; step_loss: 0.2931
step 15010; step_loss: 0.3220
step 15020; step_loss: 0.3237
step 15030; step_loss: 0.2581
step 15040; step_loss: 0.2699
step 15050; step_loss: 0.2495
step 15060; step_loss: 0.2195
step 15070; step_loss: 0.2154
step 15080; step_loss: 0.2701
step 15090; step_loss: 0.2158
step 15100; step_loss: 0.2013
step 15110; step_loss: 0.2445
step 15120; step_loss: 0.2153
step 15130; step_loss: 0.2488
step 15140; step_loss: 0.2605
step 15150; step_loss: 0.2315
step 15160; step_loss: 0.2442
step 15170; step_loss: 0.2643
step 15180; step_loss: 0.4217
step 15190; step_loss: 0.3010
step 15200; step_loss: 0.2539
step 15210; step_loss: 0.2578
step 15220; step_loss: 0.2462
step 15230; step_loss: 0.2373
step 15240; step_loss: 0.2600
step 15250; step_loss: 0.2356
step 15260; step_loss: 0.2802
step 15270; step_loss: 0.2072
step 15280; step_loss: 0.2154
step 15290; step_loss: 0.3440
step 15300; step_loss: 0.2545
step 15310; step_loss: 0.2334
step 15320; step_loss: 0.2503
step 15330; step_loss: 0.2631
step 15340; step_loss: 0.2471
step 15350; step_loss: 0.3026
step 15360; step_loss: 0.2203
step 15370; step_loss: 0.2087
step 15380; step_loss: 0.4775
step 15390; step_loss: 0.2960
step 15400; step_loss: 0.2169
step 15410; step_loss: 0.2276
step 15420; step_loss: 0.2666
step 15430; step_loss: 0.3164
step 15440; step_loss: 0.2083
step 15450; step_loss: 0.2767
step 15460; step_loss: 0.2679
step 15470; step_loss: 0.2266
step 15480; step_loss: 0.2231
step 15490; step_loss: 0.2259
step 15500; step_loss: 0.2700
step 15510; step_loss: 0.3389
step 15520; step_loss: 0.2643
step 15530; step_loss: 0.2190
step 15540; step_loss: 0.1930
step 15550; step_loss: 0.1620
step 15560; step_loss: 0.3324
step 15570; step_loss: 0.1842
step 15580; step_loss: 0.2473
step 15590; step_loss: 0.2380
step 15600; step_loss: 0.2376
step 15610; step_loss: 0.2397
step 15620; step_loss: 0.3019
step 15630; step_loss: 0.4293
step 15640; step_loss: 0.2185
step 15650; step_loss: 0.3400
step 15660; step_loss: 0.3390
step 15670; step_loss: 0.3044
step 15680; step_loss: 0.2821
step 15690; step_loss: 0.3043
step 15700; step_loss: 0.2331
step 15710; step_loss: 0.2108
step 15720; step_loss: 0.3099
step 15730; step_loss: 0.2284
step 15740; step_loss: 0.1830
step 15750; step_loss: 0.2491
step 15760; step_loss: 0.2702
step 15770; step_loss: 0.1822
step 15780; step_loss: 0.2671
step 15790; step_loss: 0.2351
step 15800; step_loss: 0.2411
step 15810; step_loss: 0.2978
step 15820; step_loss: 0.2325
step 15830; step_loss: 0.2911
step 15840; step_loss: 0.2827
step 15850; step_loss: 0.3519
step 15860; step_loss: 0.2379
step 15870; step_loss: 0.4061
step 15880; step_loss: 0.2269
step 15890; step_loss: 0.2993
step 15900; step_loss: 0.2429
step 15910; step_loss: 0.4463
step 15920; step_loss: 0.3842
step 15930; step_loss: 0.2524
step 15940; step_loss: 0.3112
step 15950; step_loss: 0.2867
step 15960; step_loss: 0.2065
step 15970; step_loss: 0.2089
step 15980; step_loss: 0.2802
step 15990; step_loss: 0.2792

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 1.204 | 1.268 | 1.329 | 1.366 | 1.417 | 1.650 |

============================
Global step:         16000
Learning rate:       0.0047
Step-time (ms):     35.8229
Train loss avg:      0.2572
--------------------------
Val loss:            0.6962
srnn loss:           0.6700
============================

Saving the model...
done in 399.12 ms
step 16000; step_loss: 0.2676
step 16010; step_loss: 0.2466
step 16020; step_loss: 0.3908
step 16030; step_loss: 0.2465
step 16040; step_loss: 0.2447
step 16050; step_loss: 0.2301
step 16060; step_loss: 0.2494
step 16070; step_loss: 0.2351
step 16080; step_loss: 0.2366
step 16090; step_loss: 0.2097
step 16100; step_loss: 0.2304
step 16110; step_loss: 0.1958
step 16120; step_loss: 0.2770
step 16130; step_loss: 0.2028
step 16140; step_loss: 0.3097
step 16150; step_loss: 0.2444
step 16160; step_loss: 0.1995
step 16170; step_loss: 0.2432
step 16180; step_loss: 0.2609
step 16190; step_loss: 0.2737
step 16200; step_loss: 0.2575
step 16210; step_loss: 0.2375
step 16220; step_loss: 0.1739
step 16230; step_loss: 0.1826
step 16240; step_loss: 0.2124
step 16250; step_loss: 0.2091
step 16260; step_loss: 0.2562
step 16270; step_loss: 0.3022
step 16280; step_loss: 0.3727
step 16290; step_loss: 0.2138
step 16300; step_loss: 0.2221
step 16310; step_loss: 0.2329
step 16320; step_loss: 0.2425
step 16330; step_loss: 0.2860
step 16340; step_loss: 0.1856
step 16350; step_loss: 0.2196
step 16360; step_loss: 0.3125
step 16370; step_loss: 0.2475
step 16380; step_loss: 0.2225
step 16390; step_loss: 0.2306
step 16400; step_loss: 0.1836
step 16410; step_loss: 0.1944
step 16420; step_loss: 0.3045
step 16430; step_loss: 0.3580
step 16440; step_loss: 0.2307
step 16450; step_loss: 0.3040
step 16460; step_loss: 0.2636
step 16470; step_loss: 0.2296
step 16480; step_loss: 0.3097
step 16490; step_loss: 0.2290
step 16500; step_loss: 0.2470
step 16510; step_loss: 0.2721
step 16520; step_loss: 0.3447
step 16530; step_loss: 0.2714
step 16540; step_loss: 0.2218
step 16550; step_loss: 0.2232
step 16560; step_loss: 0.2819
step 16570; step_loss: 0.3081
step 16580; step_loss: 0.1706
step 16590; step_loss: 0.2454
step 16600; step_loss: 0.2125
step 16610; step_loss: 0.2151
step 16620; step_loss: 0.1897
step 16630; step_loss: 0.2122
step 16640; step_loss: 0.2839
step 16650; step_loss: 0.2162
step 16660; step_loss: 0.2144
step 16670; step_loss: 0.1818
step 16680; step_loss: 0.3019
step 16690; step_loss: 0.2501
step 16700; step_loss: 0.2759
step 16710; step_loss: 0.2177
step 16720; step_loss: 0.2385
step 16730; step_loss: 0.2513
step 16740; step_loss: 0.1968
step 16750; step_loss: 0.3312
step 16760; step_loss: 0.2700
step 16770; step_loss: 0.2422
step 16780; step_loss: 0.2170
step 16790; step_loss: 0.2611
step 16800; step_loss: 0.2832
step 16810; step_loss: 0.1614
step 16820; step_loss: 0.1851
step 16830; step_loss: 0.2354
step 16840; step_loss: 0.2187
step 16850; step_loss: 0.2326
step 16860; step_loss: 0.2671
step 16870; step_loss: 0.3290
step 16880; step_loss: 0.2243
step 16890; step_loss: 0.2736
step 16900; step_loss: 0.2788
step 16910; step_loss: 0.1988
step 16920; step_loss: 0.2142
step 16930; step_loss: 0.4426
step 16940; step_loss: 0.2945
step 16950; step_loss: 0.2088
step 16960; step_loss: 0.2023
step 16970; step_loss: 0.2524
step 16980; step_loss: 0.3641
step 16990; step_loss: 0.2129

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 1.188 | 1.255 | 1.326 | 1.366 | 1.428 | 1.683 |

============================
Global step:         17000
Learning rate:       0.0047
Step-time (ms):     35.8873
Train loss avg:      0.2531
--------------------------
Val loss:            0.7393
srnn loss:           0.6629
============================

Saving the model...
done in 409.45 ms
step 17000; step_loss: 0.2363
step 17010; step_loss: 0.2676
step 17020; step_loss: 0.2466
step 17030; step_loss: 0.1666
step 17040; step_loss: 0.3451
step 17050; step_loss: 0.2590
step 17060; step_loss: 0.2179
step 17070; step_loss: 0.2834
step 17080; step_loss: 0.3085
step 17090; step_loss: 0.2336
step 17100; step_loss: 0.1684
step 17110; step_loss: 0.2240
step 17120; step_loss: 0.2028
step 17130; step_loss: 0.2879
step 17140; step_loss: 0.2097
step 17150; step_loss: 0.2079
step 17160; step_loss: 0.2286
step 17170; step_loss: 0.2322
step 17180; step_loss: 0.2150
step 17190; step_loss: 0.2818
step 17200; step_loss: 0.2928
step 17210; step_loss: 0.2991
step 17220; step_loss: 0.2320
step 17230; step_loss: 0.2662
step 17240; step_loss: 0.2426
step 17250; step_loss: 0.2300
step 17260; step_loss: 0.2367
step 17270; step_loss: 0.3069
step 17280; step_loss: 0.2220
step 17290; step_loss: 0.2423
step 17300; step_loss: 0.2235
step 17310; step_loss: 0.2845
step 17320; step_loss: 0.2212
step 17330; step_loss: 0.3317
step 17340; step_loss: 0.1618
step 17350; step_loss: 0.2688
step 17360; step_loss: 0.2180
step 17370; step_loss: 0.2924
step 17380; step_loss: 0.2372
step 17390; step_loss: 0.2952
step 17400; step_loss: 0.2519
step 17410; step_loss: 0.2414
step 17420; step_loss: 0.1769
step 17430; step_loss: 0.2565
step 17440; step_loss: 0.2867
step 17450; step_loss: 0.2383
step 17460; step_loss: 0.2811
step 17470; step_loss: 0.2518
step 17480; step_loss: 0.1980
step 17490; step_loss: 0.2338
step 17500; step_loss: 0.2125
step 17510; step_loss: 0.2397
step 17520; step_loss: 0.3801
step 17530; step_loss: 0.3637
step 17540; step_loss: 0.2574
step 17550; step_loss: 0.2397
step 17560; step_loss: 0.1992
step 17570; step_loss: 0.2191
step 17580; step_loss: 0.1831
step 17590; step_loss: 0.2254
step 17600; step_loss: 0.2057
step 17610; step_loss: 0.2437
step 17620; step_loss: 0.2836
step 17630; step_loss: 0.2551
step 17640; step_loss: 0.2998
step 17650; step_loss: 0.2927
step 17660; step_loss: 0.2702
step 17670; step_loss: 0.2114
step 17680; step_loss: 0.2532
step 17690; step_loss: 0.2773
step 17700; step_loss: 0.3269
step 17710; step_loss: 0.1889
step 17720; step_loss: 0.2100
step 17730; step_loss: 0.2533
step 17740; step_loss: 0.2513
step 17750; step_loss: 0.2584
step 17760; step_loss: 0.1780
step 17770; step_loss: 0.3672
step 17780; step_loss: 0.3267
step 17790; step_loss: 0.2593
step 17800; step_loss: 0.2903
step 17810; step_loss: 0.1948
step 17820; step_loss: 0.2595
step 17830; step_loss: 0.2045
step 17840; step_loss: 0.2184
step 17850; step_loss: 0.2424
step 17860; step_loss: 0.2783
step 17870; step_loss: 0.2167
step 17880; step_loss: 0.2260
step 17890; step_loss: 0.3185
step 17900; step_loss: 0.2271
step 17910; step_loss: 0.2293
step 17920; step_loss: 0.1471
step 17930; step_loss: 0.3638
step 17940; step_loss: 0.2266
step 17950; step_loss: 0.2764
step 17960; step_loss: 0.2303
step 17970; step_loss: 0.1619
step 17980; step_loss: 0.2765
step 17990; step_loss: 0.2705

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 1.170 | 1.237 | 1.310 | 1.352 | 1.416 | 1.668 |

============================
Global step:         18000
Learning rate:       0.0047
Step-time (ms):     36.0348
Train loss avg:      0.2459
--------------------------
Val loss:            0.7549
srnn loss:           0.6618
============================

Saving the model...
done in 392.67 ms
step 18000; step_loss: 0.2307
step 18010; step_loss: 0.2164
step 18020; step_loss: 0.2919
step 18030; step_loss: 0.1789
step 18040; step_loss: 0.2595
step 18050; step_loss: 0.1561
step 18060; step_loss: 0.2549
step 18070; step_loss: 0.1728
step 18080; step_loss: 0.2426
step 18090; step_loss: 0.3237
step 18100; step_loss: 0.2282
step 18110; step_loss: 0.2379
step 18120; step_loss: 0.1819
step 18130; step_loss: 0.2289
step 18140; step_loss: 0.2785
step 18150; step_loss: 0.2461
step 18160; step_loss: 0.2662
step 18170; step_loss: 0.2305
step 18180; step_loss: 0.1988
step 18190; step_loss: 0.2711
step 18200; step_loss: 0.3542
step 18210; step_loss: 0.3112
step 18220; step_loss: 0.2206
step 18230; step_loss: 0.3132
step 18240; step_loss: 0.2101
step 18250; step_loss: 0.3137
step 18260; step_loss: 0.2202
step 18270; step_loss: 0.2870
step 18280; step_loss: 0.2097
step 18290; step_loss: 0.2941
step 18300; step_loss: 0.2049
step 18310; step_loss: 0.2559
step 18320; step_loss: 0.2185
step 18330; step_loss: 0.2005
step 18340; step_loss: 0.1758
step 18350; step_loss: 0.2419
step 18360; step_loss: 0.2409
step 18370; step_loss: 0.1596
step 18380; step_loss: 0.2363
step 18390; step_loss: 0.2483
step 18400; step_loss: 0.2063
step 18410; step_loss: 0.2588
step 18420; step_loss: 0.2408
step 18430; step_loss: 0.2459
step 18440; step_loss: 0.3152
step 18450; step_loss: 0.2098
step 18460; step_loss: 0.2225
step 18470; step_loss: 0.1946
step 18480; step_loss: 0.2538
step 18490; step_loss: 0.2989
step 18500; step_loss: 0.2023
step 18510; step_loss: 0.2321
step 18520; step_loss: 0.1809
step 18530; step_loss: 0.1969
step 18540; step_loss: 0.3720
step 18550; step_loss: 0.2993
step 18560; step_loss: 0.2383
step 18570; step_loss: 0.2995
step 18580; step_loss: 0.2040
step 18590; step_loss: 0.2525
step 18600; step_loss: 0.2704
step 18610; step_loss: 0.2759
step 18620; step_loss: 0.2409
step 18630; step_loss: 0.2565
step 18640; step_loss: 0.2014
step 18650; step_loss: 0.2401
step 18660; step_loss: 0.2783
step 18670; step_loss: 0.2465
step 18680; step_loss: 0.2096
step 18690; step_loss: 0.2072
step 18700; step_loss: 0.2374
step 18710; step_loss: 0.2805
step 18720; step_loss: 0.2306
step 18730; step_loss: 0.2265
step 18740; step_loss: 0.2075
step 18750; step_loss: 0.2000
step 18760; step_loss: 0.2611
step 18770; step_loss: 0.3298
step 18780; step_loss: 0.1699
step 18790; step_loss: 0.2059
step 18800; step_loss: 0.2417
step 18810; step_loss: 0.1842
step 18820; step_loss: 0.3026
step 18830; step_loss: 0.1880
step 18840; step_loss: 0.2155
step 18850; step_loss: 0.2316
step 18860; step_loss: 0.2545
step 18870; step_loss: 0.1786
step 18880; step_loss: 0.1729
step 18890; step_loss: 0.2881
step 18900; step_loss: 0.2666
step 18910; step_loss: 0.2576
step 18920; step_loss: 0.2127
step 18930; step_loss: 0.2339
step 18940; step_loss: 0.1939
step 18950; step_loss: 0.1647
step 18960; step_loss: 0.2454
step 18970; step_loss: 0.2121
step 18980; step_loss: 0.3068
step 18990; step_loss: 0.2418

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 1.154 | 1.221 | 1.299 | 1.342 | 1.410 | 1.668 |

============================
Global step:         19000
Learning rate:       0.0047
Step-time (ms):     35.9602
Train loss avg:      0.2391
--------------------------
Val loss:            0.7734
srnn loss:           0.6598
============================

Saving the model...
done in 440.88 ms
step 19000; step_loss: 0.2463
step 19010; step_loss: 0.2070
step 19020; step_loss: 0.1922
step 19030; step_loss: 0.3882
step 19040; step_loss: 0.2573
step 19050; step_loss: 0.2841
step 19060; step_loss: 0.2243
step 19070; step_loss: 0.2265
step 19080; step_loss: 0.1737
step 19090; step_loss: 0.2018
step 19100; step_loss: 0.1808
step 19110; step_loss: 0.2218
step 19120; step_loss: 0.2036
step 19130; step_loss: 0.3060
step 19140; step_loss: 0.2155
step 19150; step_loss: 0.2883
step 19160; step_loss: 0.1788
step 19170; step_loss: 0.2182
step 19180; step_loss: 0.2478
step 19190; step_loss: 0.2223
step 19200; step_loss: 0.2025
step 19210; step_loss: 0.2654
step 19220; step_loss: 0.2198
step 19230; step_loss: 0.2703
step 19240; step_loss: 0.1618
step 19250; step_loss: 0.2639
step 19260; step_loss: 0.2112
step 19270; step_loss: 0.1901
step 19280; step_loss: 0.1448
step 19290; step_loss: 0.1863
step 19300; step_loss: 0.1857
step 19310; step_loss: 0.1927
step 19320; step_loss: 0.2148
step 19330; step_loss: 0.2153
step 19340; step_loss: 0.1963
step 19350; step_loss: 0.2445
step 19360; step_loss: 0.2090
step 19370; step_loss: 0.2269
step 19380; step_loss: 0.2996
step 19390; step_loss: 0.2192
step 19400; step_loss: 0.2014
step 19410; step_loss: 0.2065
step 19420; step_loss: 0.1756
step 19430; step_loss: 0.2633
step 19440; step_loss: 0.2176
step 19450; step_loss: 0.3436
step 19460; step_loss: 0.1934
step 19470; step_loss: 0.1689
step 19480; step_loss: 0.2961
step 19490; step_loss: 0.2774
step 19500; step_loss: 0.2324
step 19510; step_loss: 0.2542
step 19520; step_loss: 0.2458
step 19530; step_loss: 0.2055
step 19540; step_loss: 0.3049
step 19550; step_loss: 0.2095
step 19560; step_loss: 0.2981
step 19570; step_loss: 0.2151
step 19580; step_loss: 0.2321
step 19590; step_loss: 0.1815
step 19600; step_loss: 0.2091
step 19610; step_loss: 0.2845
step 19620; step_loss: 0.1931
step 19630; step_loss: 0.2079
step 19640; step_loss: 0.2630
step 19650; step_loss: 0.3213
step 19660; step_loss: 0.2686
step 19670; step_loss: 0.1921
step 19680; step_loss: 0.2883
step 19690; step_loss: 0.2790
step 19700; step_loss: 0.2238
step 19710; step_loss: 0.2837
step 19720; step_loss: 0.2344
step 19730; step_loss: 0.1821
step 19740; step_loss: 0.2076
step 19750; step_loss: 0.2585
step 19760; step_loss: 0.2280
step 19770; step_loss: 0.1798
step 19780; step_loss: 0.1895
step 19790; step_loss: 0.2224
step 19800; step_loss: 0.2250
step 19810; step_loss: 0.1911
step 19820; step_loss: 0.2327
step 19830; step_loss: 0.1780
step 19840; step_loss: 0.1949
step 19850; step_loss: 0.2642
step 19860; step_loss: 0.1899
step 19870; step_loss: 0.2130
step 19880; step_loss: 0.1847
step 19890; step_loss: 0.2400
step 19900; step_loss: 0.2201
step 19910; step_loss: 0.2157
step 19920; step_loss: 0.1861
step 19930; step_loss: 0.2406
step 19940; step_loss: 0.1717
step 19950; step_loss: 0.2666
step 19960; step_loss: 0.2972
step 19970; step_loss: 0.4402
step 19980; step_loss: 0.2508
step 19990; step_loss: 0.3572

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 1.140 | 1.207 | 1.288 | 1.331 | 1.400 | 1.634 |

============================
Global step:         20000
Learning rate:       0.0045
Step-time (ms):     35.8674
Train loss avg:      0.2327
--------------------------
Val loss:            0.7544
srnn loss:           0.6555
============================

Saving the model...
done in 433.92 ms
step 20000; step_loss: 0.2132
step 20010; step_loss: 0.1992
step 20020; step_loss: 0.2330
step 20030; step_loss: 0.2330
step 20040; step_loss: 0.2234
step 20050; step_loss: 0.2537
step 20060; step_loss: 0.2505
step 20070; step_loss: 0.2253
step 20080; step_loss: 0.1859
step 20090; step_loss: 0.1895
step 20100; step_loss: 0.1898
step 20110; step_loss: 0.1937
step 20120; step_loss: 0.2357
step 20130; step_loss: 0.4188
step 20140; step_loss: 0.2300
step 20150; step_loss: 0.2504
step 20160; step_loss: 0.2197
step 20170; step_loss: 0.2027
step 20180; step_loss: 0.2838
step 20190; step_loss: 0.2381
step 20200; step_loss: 0.2355
step 20210; step_loss: 0.2443
step 20220; step_loss: 0.1616
step 20230; step_loss: 0.2070
step 20240; step_loss: 0.2278
step 20250; step_loss: 0.1998
step 20260; step_loss: 0.2401
step 20270; step_loss: 0.1598
step 20280; step_loss: 0.1694
step 20290; step_loss: 0.1915
step 20300; step_loss: 0.1709
step 20310; step_loss: 0.1904
step 20320; step_loss: 0.3127
step 20330; step_loss: 0.1904
step 20340; step_loss: 0.2222
step 20350; step_loss: 0.1363
step 20360; step_loss: 0.1868
step 20370; step_loss: 0.2029
step 20380; step_loss: 0.2409
step 20390; step_loss: 0.2081
step 20400; step_loss: 0.1881
step 20410; step_loss: 0.2789
step 20420; step_loss: 0.2022
step 20430; step_loss: 0.1834
step 20440; step_loss: 0.1556
step 20450; step_loss: 0.2106
step 20460; step_loss: 0.1657
step 20470; step_loss: 0.2236
step 20480; step_loss: 0.2361
step 20490; step_loss: 0.2556
step 20500; step_loss: 0.2438
step 20510; step_loss: 0.1942
step 20520; step_loss: 0.1666
step 20530; step_loss: 0.1922
step 20540; step_loss: 0.2432
step 20550; step_loss: 0.2482
step 20560; step_loss: 0.2168
step 20570; step_loss: 0.1608
step 20580; step_loss: 0.2167
step 20590; step_loss: 0.2520
step 20600; step_loss: 0.2223
step 20610; step_loss: 0.1740
step 20620; step_loss: 0.1938
step 20630; step_loss: 0.2332
step 20640; step_loss: 0.2824
step 20650; step_loss: 0.2056
step 20660; step_loss: 0.2271
step 20670; step_loss: 0.1894
step 20680; step_loss: 0.2623
step 20690; step_loss: 0.2137
step 20700; step_loss: 0.2413
step 20710; step_loss: 0.2126
step 20720; step_loss: 0.1890
step 20730; step_loss: 0.2450
step 20740; step_loss: 0.2532
step 20750; step_loss: 0.1931
step 20760; step_loss: 0.3146
step 20770; step_loss: 0.1869
step 20780; step_loss: 0.2001
step 20790; step_loss: 0.2368
step 20800; step_loss: 0.1848
step 20810; step_loss: 0.2429
step 20820; step_loss: 0.1908
step 20830; step_loss: 0.3069
step 20840; step_loss: 0.1944
step 20850; step_loss: 0.1854
step 20860; step_loss: 0.2004
step 20870; step_loss: 0.3038
step 20880; step_loss: 0.2143
step 20890; step_loss: 0.2722
step 20900; step_loss: 0.2359
step 20910; step_loss: 0.2014
step 20920; step_loss: 0.1606
step 20930; step_loss: 0.2410
step 20940; step_loss: 0.1907
step 20950; step_loss: 0.1893
step 20960; step_loss: 0.2430
step 20970; step_loss: 0.3200
step 20980; step_loss: 0.2031
step 20990; step_loss: 0.1926

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 1.126 | 1.191 | 1.274 | 1.316 | 1.386 | 1.618 |

============================
Global step:         21000
Learning rate:       0.0045
Step-time (ms):     35.7313
Train loss avg:      0.2287
--------------------------
Val loss:            0.7306
srnn loss:           0.6538
============================

Saving the model...
done in 411.14 ms
step 21000; step_loss: 0.1960
step 21010; step_loss: 0.2586
step 21020; step_loss: 0.1967
step 21030; step_loss: 0.2376
step 21040; step_loss: 0.2391
step 21050; step_loss: 0.3163
step 21060; step_loss: 0.2710
step 21070; step_loss: 0.2458
step 21080; step_loss: 0.1669
step 21090; step_loss: 0.2514
step 21100; step_loss: 0.1954
step 21110; step_loss: 0.2471
step 21120; step_loss: 0.2787
step 21130; step_loss: 0.1737
step 21140; step_loss: 0.1811
step 21150; step_loss: 0.2157
step 21160; step_loss: 0.3601
step 21170; step_loss: 0.1785
step 21180; step_loss: 0.1567
step 21190; step_loss: 0.1776
step 21200; step_loss: 0.1562
step 21210; step_loss: 0.1379
step 21220; step_loss: 0.1903
step 21230; step_loss: 0.2063
step 21240; step_loss: 0.2360
step 21250; step_loss: 0.2936
step 21260; step_loss: 0.2569
step 21270; step_loss: 0.2726
step 21280; step_loss: 0.2170
step 21290; step_loss: 0.3008
step 21300; step_loss: 0.2894
step 21310; step_loss: 0.1828
step 21320; step_loss: 0.1668
step 21330; step_loss: 0.3599
step 21340; step_loss: 0.2199
step 21350; step_loss: 0.2824
step 21360; step_loss: 0.2086
step 21370; step_loss: 0.2051
step 21380; step_loss: 0.1613
step 21390; step_loss: 0.1728
step 21400; step_loss: 0.2531
step 21410; step_loss: 0.1823
step 21420; step_loss: 0.2055
step 21430; step_loss: 0.1671
step 21440; step_loss: 0.1756
step 21450; step_loss: 0.2169
step 21460; step_loss: 0.2139
step 21470; step_loss: 0.1983
step 21480; step_loss: 0.1696
step 21490; step_loss: 0.2184
step 21500; step_loss: 0.1748
step 21510; step_loss: 0.2859
step 21520; step_loss: 0.2030
step 21530; step_loss: 0.2158
step 21540; step_loss: 0.1759
step 21550; step_loss: 0.2996
step 21560; step_loss: 0.1908
step 21570; step_loss: 0.2641
step 21580; step_loss: 0.2144
step 21590; step_loss: 0.2511
step 21600; step_loss: 0.2627
step 21610; step_loss: 0.2068
step 21620; step_loss: 0.2643
step 21630; step_loss: 0.2494
step 21640; step_loss: 0.1806
step 21650; step_loss: 0.2345
step 21660; step_loss: 0.1954
step 21670; step_loss: 0.2629
step 21680; step_loss: 0.2621
step 21690; step_loss: 0.1953
step 21700; step_loss: 0.1800
step 21710; step_loss: 0.2402
step 21720; step_loss: 0.1886
step 21730; step_loss: 0.2472
step 21740; step_loss: 0.2129
step 21750; step_loss: 0.1696
step 21760; step_loss: 0.1680
step 21770; step_loss: 0.3700
step 21780; step_loss: 0.2033
step 21790; step_loss: 0.1491
step 21800; step_loss: 0.2245
step 21810; step_loss: 0.1852
step 21820; step_loss: 0.1678
step 21830; step_loss: 0.2180
step 21840; step_loss: 0.2485
step 21850; step_loss: 0.2111
step 21860; step_loss: 0.2277
step 21870; step_loss: 0.1730
step 21880; step_loss: 0.1957
step 21890; step_loss: 0.2464
step 21900; step_loss: 0.1599
step 21910; step_loss: 0.2067
step 21920; step_loss: 0.2002
step 21930; step_loss: 0.1744
step 21940; step_loss: 0.2195
step 21950; step_loss: 0.2072
step 21960; step_loss: 0.1930
step 21970; step_loss: 0.1397
step 21980; step_loss: 0.2194
step 21990; step_loss: 0.2757

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 1.120 | 1.185 | 1.277 | 1.320 | 1.393 | 1.650 |

============================
Global step:         22000
Learning rate:       0.0045
Step-time (ms):     35.8086
Train loss avg:      0.2210
--------------------------
Val loss:            0.8015
srnn loss:           0.6537
============================

Saving the model...
done in 420.89 ms
step 22000; step_loss: 0.2176
step 22010; step_loss: 0.1744
step 22020; step_loss: 0.2656
step 22030; step_loss: 0.2552
step 22040; step_loss: 0.1470
step 22050; step_loss: 0.1860
step 22060; step_loss: 0.2261
step 22070; step_loss: 0.2300
step 22080; step_loss: 0.1756
step 22090; step_loss: 0.3616
step 22100; step_loss: 0.2238
step 22110; step_loss: 0.1989
step 22120; step_loss: 0.2132
step 22130; step_loss: 0.1421
step 22140; step_loss: 0.2753
step 22150; step_loss: 0.1946
step 22160; step_loss: 0.2202
step 22170; step_loss: 0.2221
step 22180; step_loss: 0.1733
step 22190; step_loss: 0.1804
step 22200; step_loss: 0.2109
step 22210; step_loss: 0.1796
step 22220; step_loss: 0.2323
step 22230; step_loss: 0.2401
step 22240; step_loss: 0.1888
step 22250; step_loss: 0.1469
step 22260; step_loss: 0.2140
step 22270; step_loss: 0.1567
step 22280; step_loss: 0.1872
step 22290; step_loss: 0.2178
step 22300; step_loss: 0.2130
step 22310; step_loss: 0.1993
step 22320; step_loss: 0.2234
step 22330; step_loss: 0.2511
step 22340; step_loss: 0.1815
step 22350; step_loss: 0.2759
step 22360; step_loss: 0.2144
step 22370; step_loss: 0.1463
step 22380; step_loss: 0.1550
step 22390; step_loss: 0.1659
step 22400; step_loss: 0.1621
step 22410; step_loss: 0.2289
step 22420; step_loss: 0.2510
step 22430; step_loss: 0.2388
step 22440; step_loss: 0.1893
step 22450; step_loss: 0.2202
step 22460; step_loss: 0.1953
step 22470; step_loss: 0.2103
step 22480; step_loss: 0.2066
step 22490; step_loss: 0.2610
step 22500; step_loss: 0.2391
step 22510; step_loss: 0.2166
step 22520; step_loss: 0.1711
step 22530; step_loss: 0.2335
step 22540; step_loss: 0.2347
step 22550; step_loss: 0.1760
step 22560; step_loss: 0.1874
step 22570; step_loss: 0.1615
step 22580; step_loss: 0.1966
step 22590; step_loss: 0.2623
step 22600; step_loss: 0.2219
step 22610; step_loss: 0.1686
step 22620; step_loss: 0.1713
step 22630; step_loss: 0.3201
step 22640; step_loss: 0.1964
step 22650; step_loss: 0.1949
step 22660; step_loss: 0.2191
step 22670; step_loss: 0.1703
step 22680; step_loss: 0.2819
step 22690; step_loss: 0.2629
step 22700; step_loss: 0.2666
step 22710; step_loss: 0.4016
step 22720; step_loss: 0.1589
step 22730; step_loss: 0.1966
step 22740; step_loss: 0.1902
step 22750; step_loss: 0.2155
step 22760; step_loss: 0.2888
step 22770; step_loss: 0.1599
step 22780; step_loss: 0.1525
step 22790; step_loss: 0.2855
step 22800; step_loss: 0.2087
step 22810; step_loss: 0.2506
step 22820; step_loss: 0.1878
step 22830; step_loss: 0.1819
step 22840; step_loss: 0.2289
step 22850; step_loss: 0.1949
step 22860; step_loss: 0.2937
step 22870; step_loss: 0.2004
step 22880; step_loss: 0.2130
step 22890; step_loss: 0.2254
step 22900; step_loss: 0.1885
step 22910; step_loss: 0.2107
step 22920; step_loss: 0.2175
step 22930; step_loss: 0.2105
step 22940; step_loss: 0.1668
step 22950; step_loss: 0.2336
step 22960; step_loss: 0.1573
step 22970; step_loss: 0.2169
step 22980; step_loss: 0.1631
step 22990; step_loss: 0.2440

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 1.114 | 1.180 | 1.276 | 1.319 | 1.395 | 1.643 |

============================
Global step:         23000
Learning rate:       0.0045
Step-time (ms):     35.8296
Train loss avg:      0.2146
--------------------------
Val loss:            0.8874
srnn loss:           0.6575
============================

Saving the model...
done in 411.27 ms
step 23000; step_loss: 0.2233
step 23010; step_loss: 0.1697
step 23020; step_loss: 0.1588
step 23030; step_loss: 0.1193
step 23040; step_loss: 0.2390
step 23050; step_loss: 0.1869
step 23060; step_loss: 0.2288
step 23070; step_loss: 0.2462
step 23080; step_loss: 0.1815
step 23090; step_loss: 0.2065
step 23100; step_loss: 0.1905
step 23110; step_loss: 0.1736
step 23120; step_loss: 0.2409
step 23130; step_loss: 0.2841
step 23140; step_loss: 0.1626
step 23150; step_loss: 0.2272
step 23160; step_loss: 0.2505
step 23170; step_loss: 0.2540
step 23180; step_loss: 0.2363
step 23190; step_loss: 0.1927
step 23200; step_loss: 0.2671
step 23210; step_loss: 0.1376
step 23220; step_loss: 0.2645
step 23230; step_loss: 0.1829
step 23240; step_loss: 0.2847
step 23250; step_loss: 0.2359
step 23260; step_loss: 0.1860
step 23270; step_loss: 0.2875
step 23280; step_loss: 0.2496
step 23290; step_loss: 0.2109
step 23300; step_loss: 0.2925
step 23310; step_loss: 0.1672
step 23320; step_loss: 0.1982
step 23330; step_loss: 0.2318
step 23340; step_loss: 0.2030
step 23350; step_loss: 0.1466
step 23360; step_loss: 0.1983
step 23370; step_loss: 0.1740
step 23380; step_loss: 0.1533
step 23390; step_loss: 0.1541
step 23400; step_loss: 0.3483
step 23410; step_loss: 0.1754
step 23420; step_loss: 0.2283
step 23430; step_loss: 0.2037
step 23440; step_loss: 0.2000
step 23450; step_loss: 0.2035
step 23460; step_loss: 0.2424
step 23470; step_loss: 0.1337
step 23480; step_loss: 0.2199
step 23490; step_loss: 0.1876
step 23500; step_loss: 0.3105
step 23510; step_loss: 0.2296
step 23520; step_loss: 0.3994
step 23530; step_loss: 0.2811
step 23540; step_loss: 0.2493
step 23550; step_loss: 0.1734
step 23560; step_loss: 0.2263
step 23570; step_loss: 0.2037
step 23580; step_loss: 0.2220
step 23590; step_loss: 0.1830
step 23600; step_loss: 0.1797
step 23610; step_loss: 0.1809
step 23620; step_loss: 0.1767
step 23630; step_loss: 0.1763
step 23640; step_loss: 0.2732
step 23650; step_loss: 0.2530
step 23660; step_loss: 0.1842
step 23670; step_loss: 0.1836
step 23680; step_loss: 0.2315
step 23690; step_loss: 0.2280
step 23700; step_loss: 0.1123
step 23710; step_loss: 0.2895
step 23720; step_loss: 0.1359
step 23730; step_loss: 0.1692
step 23740; step_loss: 0.3068
step 23750; step_loss: 0.1878
step 23760; step_loss: 0.2454
step 23770; step_loss: 0.2294
step 23780; step_loss: 0.2585
step 23790; step_loss: 0.2190
step 23800; step_loss: 0.1663
step 23810; step_loss: 0.1687
step 23820; step_loss: 0.2085
step 23830; step_loss: 0.2046
step 23840; step_loss: 0.1940
step 23850; step_loss: 0.2074
step 23860; step_loss: 0.2007
step 23870; step_loss: 0.2699
step 23880; step_loss: 0.2525
step 23890; step_loss: 0.1492
step 23900; step_loss: 0.2446
step 23910; step_loss: 0.2065
step 23920; step_loss: 0.2385
step 23930; step_loss: 0.2016
step 23940; step_loss: 0.2163
step 23950; step_loss: 0.1724
step 23960; step_loss: 0.1985
step 23970; step_loss: 0.3528
step 23980; step_loss: 0.2275
step 23990; step_loss: 0.1553

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 1.106 | 1.172 | 1.273 | 1.317 | 1.393 | 1.630 |

============================
Global step:         24000
Learning rate:       0.0045
Step-time (ms):     35.9114
Train loss avg:      0.2129
--------------------------
Val loss:            0.7681
srnn loss:           0.6573
============================

Saving the model...
done in 404.30 ms
step 24000; step_loss: 0.1869
step 24010; step_loss: 0.2388
step 24020; step_loss: 0.2322
step 24030; step_loss: 0.2258
step 24040; step_loss: 0.2024
step 24050; step_loss: 0.2264
step 24060; step_loss: 0.1293
step 24070; step_loss: 0.2592
step 24080; step_loss: 0.2123
step 24090; step_loss: 0.2182
step 24100; step_loss: 0.2774
step 24110; step_loss: 0.2039
step 24120; step_loss: 0.1957
step 24130; step_loss: 0.1625
step 24140; step_loss: 0.2179
step 24150; step_loss: 0.1968
step 24160; step_loss: 0.1644
step 24170; step_loss: 0.2097
step 24180; step_loss: 0.1350
step 24190; step_loss: 0.2167
step 24200; step_loss: 0.1865
step 24210; step_loss: 0.2183
step 24220; step_loss: 0.1864
step 24230; step_loss: 0.2089
step 24240; step_loss: 0.2262
step 24250; step_loss: 0.2023
step 24260; step_loss: 0.2360
step 24270; step_loss: 0.2603
step 24280; step_loss: 0.1695
step 24290; step_loss: 0.1725
step 24300; step_loss: 0.2224
step 24310; step_loss: 0.2359
step 24320; step_loss: 0.1986
step 24330; step_loss: 0.1619
step 24340; step_loss: 0.2141
step 24350; step_loss: 0.1163
step 24360; step_loss: 0.1618
step 24370; step_loss: 0.1635
step 24380; step_loss: 0.2924
step 24390; step_loss: 0.2350
step 24400; step_loss: 0.1745
step 24410; step_loss: 0.2797
step 24420; step_loss: 0.2036
step 24430; step_loss: 0.2263
step 24440; step_loss: 0.1890
step 24450; step_loss: 0.1739
step 24460; step_loss: 0.2084
step 24470; step_loss: 0.3107
step 24480; step_loss: 0.2437
step 24490; step_loss: 0.1823
step 24500; step_loss: 0.1679
step 24510; step_loss: 0.1832
step 24520; step_loss: 0.2109
step 24530; step_loss: 0.2453
step 24540; step_loss: 0.2118
step 24550; step_loss: 0.2549
step 24560; step_loss: 0.1608
step 24570; step_loss: 0.2365
step 24580; step_loss: 0.1746
step 24590; step_loss: 0.2804
step 24600; step_loss: 0.2108
step 24610; step_loss: 0.1808
step 24620; step_loss: 0.1906
step 24630; step_loss: 0.2476
step 24640; step_loss: 0.2619
step 24650; step_loss: 0.2237
step 24660; step_loss: 0.2064
step 24670; step_loss: 0.1596
step 24680; step_loss: 0.2126
step 24690; step_loss: 0.1541
step 24700; step_loss: 0.1787
step 24710; step_loss: 0.2252
step 24720; step_loss: 0.1701
step 24730; step_loss: 0.1805
step 24740; step_loss: 0.2899
step 24750; step_loss: 0.1339
step 24760; step_loss: 0.1739
step 24770; step_loss: 0.1623
step 24780; step_loss: 0.2260
step 24790; step_loss: 0.1735
step 24800; step_loss: 0.2234
step 24810; step_loss: 0.2689
step 24820; step_loss: 0.1875
step 24830; step_loss: 0.1879
step 24840; step_loss: 0.1889
step 24850; step_loss: 0.1987
step 24860; step_loss: 0.1514
step 24870; step_loss: 0.2225
step 24880; step_loss: 0.1935
step 24890; step_loss: 0.2034
step 24900; step_loss: 0.1852
step 24910; step_loss: 0.1662
step 24920; step_loss: 0.2514
step 24930; step_loss: 0.1341
step 24940; step_loss: 0.2105
step 24950; step_loss: 0.2242
step 24960; step_loss: 0.2926
step 24970; step_loss: 0.2486
step 24980; step_loss: 0.1814
step 24990; step_loss: 0.1365

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 1.099 | 1.168 | 1.276 | 1.323 | 1.406 | 1.660 |

============================
Global step:         25000
Learning rate:       0.0045
Step-time (ms):     35.9617
Train loss avg:      0.2050
--------------------------
Val loss:            0.7210
srnn loss:           0.6543
============================

Saving the model...
done in 416.21 ms
step 25000; step_loss: 0.1980
step 25010; step_loss: 0.2325
step 25020; step_loss: 0.1641
step 25030; step_loss: 0.1914
step 25040; step_loss: 0.1815
step 25050; step_loss: 0.2457
step 25060; step_loss: 0.2133
step 25070; step_loss: 0.2499
step 25080; step_loss: 0.2258
step 25090; step_loss: 0.1538
step 25100; step_loss: 0.2136
step 25110; step_loss: 0.1964
step 25120; step_loss: 0.1957
step 25130; step_loss: 0.3190
step 25140; step_loss: 0.1921
step 25150; step_loss: 0.2343
step 25160; step_loss: 0.2209
step 25170; step_loss: 0.2445
step 25180; step_loss: 0.1772
step 25190; step_loss: 0.2050
step 25200; step_loss: 0.2267
step 25210; step_loss: 0.1887
step 25220; step_loss: 0.1620
step 25230; step_loss: 0.2151
step 25240; step_loss: 0.3644
step 25250; step_loss: 0.1980
step 25260; step_loss: 0.2394
step 25270; step_loss: 0.1755
step 25280; step_loss: 0.2107
step 25290; step_loss: 0.2908
step 25300; step_loss: 0.3048
step 25310; step_loss: 0.1959
step 25320; step_loss: 0.1524
step 25330; step_loss: 0.1740
step 25340; step_loss: 0.1859
step 25350; step_loss: 0.2155
step 25360; step_loss: 0.1520
step 25370; step_loss: 0.2238
step 25380; step_loss: 0.1748
step 25390; step_loss: 0.3813
step 25400; step_loss: 0.1596
step 25410; step_loss: 0.2270
step 25420; step_loss: 0.1789
step 25430; step_loss: 0.3902
step 25440; step_loss: 0.1330
step 25450; step_loss: 0.2211
step 25460; step_loss: 0.2048
step 25470; step_loss: 0.1954
step 25480; step_loss: 0.2617
step 25490; step_loss: 0.2721
step 25500; step_loss: 0.2620
step 25510; step_loss: 0.2066
step 25520; step_loss: 0.2262
step 25530; step_loss: 0.2522
step 25540; step_loss: 0.2063
step 25550; step_loss: 0.1771
step 25560; step_loss: 0.2362
step 25570; step_loss: 0.1591
step 25580; step_loss: 0.2278
step 25590; step_loss: 0.1722
step 25600; step_loss: 0.1577
step 25610; step_loss: 0.2027
step 25620; step_loss: 0.1917
step 25630; step_loss: 0.2665
step 25640; step_loss: 0.2173
step 25650; step_loss: 0.2973
step 25660; step_loss: 0.1639
step 25670; step_loss: 0.2731
step 25680; step_loss: 0.1625
step 25690; step_loss: 0.2603
step 25700; step_loss: 0.2140
step 25710; step_loss: 0.1845
step 25720; step_loss: 0.2436
step 25730; step_loss: 0.2053
step 25740; step_loss: 0.2073
step 25750; step_loss: 0.2215
step 25760; step_loss: 0.2127
step 25770; step_loss: 0.2859
step 25780; step_loss: 0.1967
step 25790; step_loss: 0.1777
step 25800; step_loss: 0.2987
step 25810; step_loss: 0.1602
step 25820; step_loss: 0.1614
step 25830; step_loss: 0.1933
step 25840; step_loss: 0.1753
step 25850; step_loss: 0.2536
step 25860; step_loss: 0.2725
step 25870; step_loss: 0.1697
step 25880; step_loss: 0.2308
step 25890; step_loss: 0.1576
step 25900; step_loss: 0.1497
step 25910; step_loss: 0.1963
step 25920; step_loss: 0.2290
step 25930; step_loss: 0.1786
step 25940; step_loss: 0.2158
step 25950; step_loss: 0.2245
step 25960; step_loss: 0.1350
step 25970; step_loss: 0.2209
step 25980; step_loss: 0.1857
step 25990; step_loss: 0.2838

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 1.096 | 1.169 | 1.285 | 1.335 | 1.426 | 1.679 |

============================
Global step:         26000
Learning rate:       0.0045
Step-time (ms):     35.9317
Train loss avg:      0.2045
--------------------------
Val loss:            0.7299
srnn loss:           0.6669
============================

Saving the model...
done in 404.89 ms
step 26000; step_loss: 0.1664
step 26010; step_loss: 0.2079
step 26020; step_loss: 0.1842
step 26030; step_loss: 0.1497
step 26040; step_loss: 0.2126
step 26050; step_loss: 0.2454
step 26060; step_loss: 0.2229
step 26070; step_loss: 0.1831
step 26080; step_loss: 0.1884
step 26090; step_loss: 0.2282
step 26100; step_loss: 0.2223
step 26110; step_loss: 0.2491
step 26120; step_loss: 0.2168
step 26130; step_loss: 0.2382
step 26140; step_loss: 0.2321
step 26150; step_loss: 0.2242
step 26160; step_loss: 0.1613
step 26170; step_loss: 0.2301
step 26180; step_loss: 0.2566
step 26190; step_loss: 0.1864
step 26200; step_loss: 0.1714
step 26210; step_loss: 0.2077
step 26220; step_loss: 0.1851
step 26230; step_loss: 0.2608
step 26240; step_loss: 0.2088
step 26250; step_loss: 0.1780
step 26260; step_loss: 0.1507
step 26270; step_loss: 0.2458
step 26280; step_loss: 0.2065
step 26290; step_loss: 0.1929
step 26300; step_loss: 0.1626
step 26310; step_loss: 0.1878
step 26320; step_loss: 0.2119
step 26330; step_loss: 0.2203
step 26340; step_loss: 0.2482
step 26350; step_loss: 0.1958
step 26360; step_loss: 0.1696
step 26370; step_loss: 0.1978
step 26380; step_loss: 0.1969
step 26390; step_loss: 0.1696
step 26400; step_loss: 0.1852
step 26410; step_loss: 0.1517
step 26420; step_loss: 0.1890
step 26430; step_loss: 0.1496
step 26440; step_loss: 0.1393
step 26450; step_loss: 0.2554
step 26460; step_loss: 0.2408
step 26470; step_loss: 0.2173
step 26480; step_loss: 0.2336
step 26490; step_loss: 0.1981
step 26500; step_loss: 0.1693
step 26510; step_loss: 0.1506
step 26520; step_loss: 0.2336
step 26530; step_loss: 0.1634
step 26540; step_loss: 0.2028
step 26550; step_loss: 0.1860
step 26560; step_loss: 0.2104
step 26570; step_loss: 0.1989
step 26580; step_loss: 0.1583
step 26590; step_loss: 0.1859
step 26600; step_loss: 0.2682
step 26610; step_loss: 0.2203
step 26620; step_loss: 0.1823
step 26630; step_loss: 0.1489
step 26640; step_loss: 0.2408
step 26650; step_loss: 0.2220
step 26660; step_loss: 0.1940
step 26670; step_loss: 0.1922
step 26680; step_loss: 0.2226
step 26690; step_loss: 0.1827
step 26700; step_loss: 0.1895
step 26710; step_loss: 0.2667
step 26720; step_loss: 0.1726
step 26730; step_loss: 0.2291
step 26740; step_loss: 0.1849
step 26750; step_loss: 0.1991
step 26760; step_loss: 0.2166
step 26770; step_loss: 0.3026
step 26780; step_loss: 0.2312
step 26790; step_loss: 0.2572
step 26800; step_loss: 0.1332
step 26810; step_loss: 0.1870
step 26820; step_loss: 0.1247
step 26830; step_loss: 0.1988
step 26840; step_loss: 0.1492
step 26850; step_loss: 0.1820
step 26860; step_loss: 0.2112
step 26870; step_loss: 0.2075
step 26880; step_loss: 0.1875
step 26890; step_loss: 0.1778
step 26900; step_loss: 0.2149
step 26910; step_loss: 0.1348
step 26920; step_loss: 0.1979
step 26930; step_loss: 0.1750
step 26940; step_loss: 0.2298
step 26950; step_loss: 0.2871
step 26960; step_loss: 0.2677
step 26970; step_loss: 0.2867
step 26980; step_loss: 0.1712
step 26990; step_loss: 0.1757

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 1.081 | 1.154 | 1.273 | 1.324 | 1.413 | 1.649 |

============================
Global step:         27000
Learning rate:       0.0045
Step-time (ms):     35.9706
Train loss avg:      0.2005
--------------------------
Val loss:            0.8100
srnn loss:           0.6654
============================

Saving the model...
done in 399.91 ms
step 27000; step_loss: 0.2003
step 27010; step_loss: 0.1624
step 27020; step_loss: 0.2242
step 27030; step_loss: 0.1618
step 27040; step_loss: 0.1413
step 27050; step_loss: 0.2370
step 27060; step_loss: 0.2971
step 27070; step_loss: 0.1964
step 27080; step_loss: 0.1304
step 27090; step_loss: 0.1892
step 27100; step_loss: 0.2326
step 27110; step_loss: 0.2172
step 27120; step_loss: 0.3455
step 27130; step_loss: 0.3843
step 27140; step_loss: 0.2498
step 27150; step_loss: 0.2080
step 27160; step_loss: 0.1623
step 27170; step_loss: 0.2355
step 27180; step_loss: 0.2325
step 27190; step_loss: 0.2549
step 27200; step_loss: 0.1584
step 27210; step_loss: 0.2001
step 27220; step_loss: 0.3329
step 27230; step_loss: 0.2278
step 27240; step_loss: 0.1644
step 27250; step_loss: 0.1681
step 27260; step_loss: 0.3100
step 27270; step_loss: 0.1640
step 27280; step_loss: 0.1693
step 27290; step_loss: 0.1745
step 27300; step_loss: 0.1267
step 27310; step_loss: 0.1505
step 27320; step_loss: 0.2263
step 27330; step_loss: 0.2170
step 27340; step_loss: 0.2895
step 27350; step_loss: 0.1777
step 27360; step_loss: 0.1554
step 27370; step_loss: 0.1766
step 27380; step_loss: 0.2251
step 27390; step_loss: 0.1622
step 27400; step_loss: 0.2705
step 27410; step_loss: 0.2549
step 27420; step_loss: 0.2100
step 27430; step_loss: 0.1831
step 27440; step_loss: 0.1315
step 27450; step_loss: 0.2184
step 27460; step_loss: 0.1450
step 27470; step_loss: 0.1408
step 27480; step_loss: 0.1795
step 27490; step_loss: 0.2155
step 27500; step_loss: 0.2149
step 27510; step_loss: 0.2590
step 27520; step_loss: 0.2365
step 27530; step_loss: 0.2451
step 27540; step_loss: 0.1496
step 27550; step_loss: 0.2194
step 27560; step_loss: 0.1792
step 27570; step_loss: 0.1467
step 27580; step_loss: 0.2164
step 27590; step_loss: 0.1926
step 27600; step_loss: 0.2255
step 27610; step_loss: 0.2787
step 27620; step_loss: 0.1545
step 27630; step_loss: 0.1822
step 27640; step_loss: 0.1467
step 27650; step_loss: 0.2249
step 27660; step_loss: 0.1728
step 27670; step_loss: 0.2516
step 27680; step_loss: 0.1924
step 27690; step_loss: 0.1416
step 27700; step_loss: 0.1882
step 27710; step_loss: 0.2239
step 27720; step_loss: 0.1546
step 27730; step_loss: 0.1750
step 27740; step_loss: 0.1675
step 27750; step_loss: 0.1879
step 27760; step_loss: 0.2442
step 27770; step_loss: 0.1776
step 27780; step_loss: 0.2456
step 27790; step_loss: 0.2271
step 27800; step_loss: 0.1759
step 27810; step_loss: 0.1893
step 27820; step_loss: 0.2057
step 27830; step_loss: 0.2368
step 27840; step_loss: 0.1315
step 27850; step_loss: 0.1729
step 27860; step_loss: 0.1593
step 27870; step_loss: 0.2272
step 27880; step_loss: 0.1633
step 27890; step_loss: 0.2294
step 27900; step_loss: 0.1905
step 27910; step_loss: 0.2314
step 27920; step_loss: 0.2389
step 27930; step_loss: 0.2109
step 27940; step_loss: 0.1850
step 27950; step_loss: 0.2055
step 27960; step_loss: 0.2134
step 27970; step_loss: 0.1564
step 27980; step_loss: 0.1631
step 27990; step_loss: 0.2309

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 1.076 | 1.152 | 1.283 | 1.339 | 1.438 | 1.717 |

============================
Global step:         28000
Learning rate:       0.0045
Step-time (ms):     35.9101
Train loss avg:      0.2028
--------------------------
Val loss:            0.7677
srnn loss:           0.6730
============================

Saving the model...
done in 400.67 ms
step 28000; step_loss: 0.1744
step 28010; step_loss: 0.1851
step 28020; step_loss: 0.2404
step 28030; step_loss: 0.1518
step 28040; step_loss: 0.1834
step 28050; step_loss: 0.1811
step 28060; step_loss: 0.1519
step 28070; step_loss: 0.1623
step 28080; step_loss: 0.1949
step 28090; step_loss: 0.2895
step 28100; step_loss: 0.1927
step 28110; step_loss: 0.2653
step 28120; step_loss: 0.3060
step 28130; step_loss: 0.2298
step 28140; step_loss: 0.2351
step 28150; step_loss: 0.1841
step 28160; step_loss: 0.2081
step 28170; step_loss: 0.1436
step 28180; step_loss: 0.1422
step 28190; step_loss: 0.2031
step 28200; step_loss: 0.2028
step 28210; step_loss: 0.2296
step 28220; step_loss: 0.1911
step 28230; step_loss: 0.2270
step 28240; step_loss: 0.1753
step 28250; step_loss: 0.2023
step 28260; step_loss: 0.1632
step 28270; step_loss: 0.1447
step 28280; step_loss: 0.2157
step 28290; step_loss: 0.1805
step 28300; step_loss: 0.1754
step 28310; step_loss: 0.1748
step 28320; step_loss: 0.1830
step 28330; step_loss: 0.1932
step 28340; step_loss: 0.1999
step 28350; step_loss: 0.1674
step 28360; step_loss: 0.1738
step 28370; step_loss: 0.1970
step 28380; step_loss: 0.1737
step 28390; step_loss: 0.1852
step 28400; step_loss: 0.1882
step 28410; step_loss: 0.2351
step 28420; step_loss: 0.2548
step 28430; step_loss: 0.1961
step 28440; step_loss: 0.2108
step 28450; step_loss: 0.1171
step 28460; step_loss: 0.1732
step 28470; step_loss: 0.2967
step 28480; step_loss: 0.1852
step 28490; step_loss: 0.1409
step 28500; step_loss: 0.1701
step 28510; step_loss: 0.1890
step 28520; step_loss: 0.1454
step 28530; step_loss: 0.1578
step 28540; step_loss: 0.1775
step 28550; step_loss: 0.1407
step 28560; step_loss: 0.2988
step 28570; step_loss: 0.2084
step 28580; step_loss: 0.1721
step 28590; step_loss: 0.1059
step 28600; step_loss: 0.2502
step 28610; step_loss: 0.1750
step 28620; step_loss: 0.2429
step 28630; step_loss: 0.1876
step 28640; step_loss: 0.1693
step 28650; step_loss: 0.1884
step 28660; step_loss: 0.1562
step 28670; step_loss: 0.2222
step 28680; step_loss: 0.2312
step 28690; step_loss: 0.2057
step 28700; step_loss: 0.1861
step 28710; step_loss: 0.1680
step 28720; step_loss: 0.2071
step 28730; step_loss: 0.1622
step 28740; step_loss: 0.1669
step 28750; step_loss: 0.1999
step 28760; step_loss: 0.2174
step 28770; step_loss: 0.2070
step 28780; step_loss: 0.1639
step 28790; step_loss: 0.2121
step 28800; step_loss: 0.1963
step 28810; step_loss: 0.1635
step 28820; step_loss: 0.2228
step 28830; step_loss: 0.2085
step 28840; step_loss: 0.1772
step 28850; step_loss: 0.2075
step 28860; step_loss: 0.2374
step 28870; step_loss: 0.2625
step 28880; step_loss: 0.1485
step 28890; step_loss: 0.2229
step 28900; step_loss: 0.1895
step 28910; step_loss: 0.1620
step 28920; step_loss: 0.2228
step 28930; step_loss: 0.2055
step 28940; step_loss: 0.1927
step 28950; step_loss: 0.1677
step 28960; step_loss: 0.1841
step 28970; step_loss: 0.2204
step 28980; step_loss: 0.1774
step 28990; step_loss: 0.2222

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 1.071 | 1.149 | 1.281 | 1.336 | 1.436 | 1.679 |

============================
Global step:         29000
Learning rate:       0.0045
Step-time (ms):     35.7904
Train loss avg:      0.1972
--------------------------
Val loss:            0.7492
srnn loss:           0.6707
============================

Saving the model...
done in 445.83 ms
step 29000; step_loss: 0.2339
step 29010; step_loss: 0.1540
step 29020; step_loss: 0.2281
step 29030; step_loss: 0.2489
step 29040; step_loss: 0.1834
step 29050; step_loss: 0.2415
step 29060; step_loss: 0.1732
step 29070; step_loss: 0.1497
step 29080; step_loss: 0.1805
step 29090; step_loss: 0.2087
step 29100; step_loss: 0.2275
step 29110; step_loss: 0.2088
step 29120; step_loss: 0.2052
step 29130; step_loss: 0.1616
step 29140; step_loss: 0.1851
step 29150; step_loss: 0.2218
step 29160; step_loss: 0.1755
step 29170; step_loss: 0.2048
step 29180; step_loss: 0.2141
step 29190; step_loss: 0.2871
step 29200; step_loss: 0.1711
step 29210; step_loss: 0.2327
step 29220; step_loss: 0.2038
step 29230; step_loss: 0.2252
step 29240; step_loss: 0.1169
step 29250; step_loss: 0.1615
step 29260; step_loss: 0.1749
step 29270; step_loss: 0.2069
step 29280; step_loss: 0.1845
step 29290; step_loss: 0.1682
step 29300; step_loss: 0.1601
step 29310; step_loss: 0.1808
step 29320; step_loss: 0.2021
step 29330; step_loss: 0.2894
step 29340; step_loss: 0.1466
step 29350; step_loss: 0.2742
step 29360; step_loss: 0.1243
step 29370; step_loss: 0.1441
step 29380; step_loss: 0.2110
step 29390; step_loss: 0.2496
step 29400; step_loss: 0.1690
step 29410; step_loss: 0.2014
step 29420; step_loss: 0.2092
step 29430; step_loss: 0.1459
step 29440; step_loss: 0.2442
step 29450; step_loss: 0.2462
step 29460; step_loss: 0.1772
step 29470; step_loss: 0.1304
step 29480; step_loss: 0.2418
step 29490; step_loss: 0.1306
step 29500; step_loss: 0.1823
step 29510; step_loss: 0.1789
step 29520; step_loss: 0.1464
step 29530; step_loss: 0.1508
step 29540; step_loss: 0.1524
step 29550; step_loss: 0.1620
step 29560; step_loss: 0.1079
step 29570; step_loss: 0.1550
step 29580; step_loss: 0.1189
step 29590; step_loss: 0.1646
step 29600; step_loss: 0.1593
step 29610; step_loss: 0.1941
step 29620; step_loss: 0.2501
step 29630; step_loss: 0.1655
step 29640; step_loss: 0.2466
step 29650; step_loss: 0.1402
step 29660; step_loss: 0.1624
step 29670; step_loss: 0.1520
step 29680; step_loss: 0.1604
step 29690; step_loss: 0.2030
step 29700; step_loss: 0.2672
step 29710; step_loss: 0.2016
step 29720; step_loss: 0.1459
step 29730; step_loss: 0.1919
step 29740; step_loss: 0.1955
step 29750; step_loss: 0.1929
step 29760; step_loss: 0.1918
step 29770; step_loss: 0.2295
step 29780; step_loss: 0.1927
step 29790; step_loss: 0.1677
step 29800; step_loss: 0.2180
step 29810; step_loss: 0.1948
step 29820; step_loss: 0.1855
step 29830; step_loss: 0.1568
step 29840; step_loss: 0.1595
step 29850; step_loss: 0.1744
step 29860; step_loss: 0.1500
step 29870; step_loss: 0.1562
step 29880; step_loss: 0.1896
step 29890; step_loss: 0.1715
step 29900; step_loss: 0.1963
step 29910; step_loss: 0.2005
step 29920; step_loss: 0.1909
step 29930; step_loss: 0.1978
step 29940; step_loss: 0.2715
step 29950; step_loss: 0.1711
step 29960; step_loss: 0.2139
step 29970; step_loss: 0.1951
step 29980; step_loss: 0.2102
step 29990; step_loss: 0.1594

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 1.061 | 1.142 | 1.283 | 1.341 | 1.445 | 1.696 |

============================
Global step:         30000
Learning rate:       0.0043
Step-time (ms):     35.8620
Train loss avg:      0.1952
--------------------------
Val loss:            0.7645
srnn loss:           0.6734
============================

Saving the model...
done in 400.23 ms
step 30000; step_loss: 0.2021
step 30010; step_loss: 0.1858
step 30020; step_loss: 0.2083
step 30030; step_loss: 0.1553
step 30040; step_loss: 0.2438
step 30050; step_loss: 0.1409
step 30060; step_loss: 0.1685
step 30070; step_loss: 0.2270
step 30080; step_loss: 0.1597
step 30090; step_loss: 0.1408
step 30100; step_loss: 0.1732
step 30110; step_loss: 0.1890
step 30120; step_loss: 0.2078
step 30130; step_loss: 0.1634
step 30140; step_loss: 0.2208
step 30150; step_loss: 0.1773
step 30160; step_loss: 0.1548
step 30170; step_loss: 0.2245
step 30180; step_loss: 0.1591
step 30190; step_loss: 0.1964
step 30200; step_loss: 0.1680
step 30210; step_loss: 0.1874
step 30220; step_loss: 0.1632
step 30230; step_loss: 0.1945
step 30240; step_loss: 0.1563
step 30250; step_loss: 0.1845
step 30260; step_loss: 0.1587
step 30270; step_loss: 0.2100
step 30280; step_loss: 0.2317
step 30290; step_loss: 0.1747
step 30300; step_loss: 0.1506
step 30310; step_loss: 0.1793
step 30320; step_loss: 0.2158
step 30330; step_loss: 0.2823
step 30340; step_loss: 0.1564
step 30350; step_loss: 0.1919
step 30360; step_loss: 0.1935
step 30370; step_loss: 0.1841
step 30380; step_loss: 0.2020
step 30390; step_loss: 0.1582
step 30400; step_loss: 0.2746
step 30410; step_loss: 0.1422
step 30420; step_loss: 0.1304
step 30430; step_loss: 0.3187
step 30440; step_loss: 0.1925
step 30450; step_loss: 0.1371
step 30460; step_loss: 0.1691
step 30470; step_loss: 0.2281
step 30480; step_loss: 0.2134
step 30490; step_loss: 0.2605
step 30500; step_loss: 0.1592
step 30510; step_loss: 0.2009
step 30520; step_loss: 0.1873
step 30530; step_loss: 0.2260
step 30540; step_loss: 0.2587
step 30550; step_loss: 0.1683
step 30560; step_loss: 0.1998
step 30570; step_loss: 0.1769
step 30580; step_loss: 0.1296
step 30590; step_loss: 0.1499
step 30600; step_loss: 0.1751
step 30610; step_loss: 0.2813
step 30620; step_loss: 0.1739
step 30630; step_loss: 0.1538
step 30640; step_loss: 0.1935
step 30650; step_loss: 0.3115
step 30660; step_loss: 0.1513
step 30670; step_loss: 0.2149
step 30680; step_loss: 0.2057
step 30690; step_loss: 0.1925
step 30700; step_loss: 0.1893
step 30710; step_loss: 0.1709
step 30720; step_loss: 0.2319
step 30730; step_loss: 0.1872
step 30740; step_loss: 0.1483
step 30750; step_loss: 0.1687
step 30760; step_loss: 0.1588
step 30770; step_loss: 0.1484
step 30780; step_loss: 0.2513
step 30790; step_loss: 0.1874
step 30800; step_loss: 0.1885
step 30810; step_loss: 0.2065
step 30820; step_loss: 0.2586
step 30830; step_loss: 0.1490
step 30840; step_loss: 0.1703
step 30850; step_loss: 0.1794
step 30860; step_loss: 0.1269
step 30870; step_loss: 0.2280
step 30880; step_loss: 0.1486
step 30890; step_loss: 0.1885
step 30900; step_loss: 0.1874
step 30910; step_loss: 0.1577
step 30920; step_loss: 0.1455
step 30930; step_loss: 0.1898
step 30940; step_loss: 0.2152
step 30950; step_loss: 0.2799
step 30960; step_loss: 0.2589
step 30970; step_loss: 0.1807
step 30980; step_loss: 0.1405
step 30990; step_loss: 0.1590

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 1.061 | 1.143 | 1.285 | 1.344 | 1.453 | 1.709 |

============================
Global step:         31000
Learning rate:       0.0043
Step-time (ms):     35.9576
Train loss avg:      0.1922
--------------------------
Val loss:            0.7707
srnn loss:           0.6726
============================

Saving the model...
done in 409.93 ms
step 31000; step_loss: 0.1366
step 31010; step_loss: 0.1674
step 31020; step_loss: 0.1982
step 31030; step_loss: 0.2096
step 31040; step_loss: 0.1320
step 31050; step_loss: 0.2034
step 31060; step_loss: 0.1581
step 31070; step_loss: 0.1793
step 31080; step_loss: 0.1235
step 31090; step_loss: 0.1818
step 31100; step_loss: 0.2239
step 31110; step_loss: 0.2997
step 31120; step_loss: 0.2531
step 31130; step_loss: 0.2131
step 31140; step_loss: 0.1887
step 31150; step_loss: 0.1578
step 31160; step_loss: 0.2183
step 31170; step_loss: 0.1796
step 31180; step_loss: 0.1665
step 31190; step_loss: 0.1674
step 31200; step_loss: 0.1698
step 31210; step_loss: 0.1758
step 31220; step_loss: 0.2117
step 31230; step_loss: 0.3093
step 31240; step_loss: 0.1193
step 31250; step_loss: 0.2035
step 31260; step_loss: 0.1999
step 31270; step_loss: 0.1598
step 31280; step_loss: 0.1282
step 31290; step_loss: 0.1862
step 31300; step_loss: 0.1284
step 31310; step_loss: 0.2180
step 31320; step_loss: 0.2315
step 31330; step_loss: 0.2491
step 31340; step_loss: 0.1313
step 31350; step_loss: 0.2431
step 31360; step_loss: 0.1773
step 31370; step_loss: 0.1647
step 31380; step_loss: 0.1479
step 31390; step_loss: 0.3122
step 31400; step_loss: 0.1879
step 31410; step_loss: 0.2290
step 31420; step_loss: 0.1556
step 31430; step_loss: 0.1675
step 31440; step_loss: 0.2127
step 31450; step_loss: 0.1597
step 31460; step_loss: 0.2233
step 31470; step_loss: 0.1344
step 31480; step_loss: 0.1569
step 31490; step_loss: 0.1850
step 31500; step_loss: 0.1408
step 31510; step_loss: 0.1433
step 31520; step_loss: 0.1932
step 31530; step_loss: 0.2017
step 31540; step_loss: 0.1831
step 31550; step_loss: 0.2326
step 31560; step_loss: 0.1746
step 31570; step_loss: 0.2455
step 31580; step_loss: 0.1817
step 31590; step_loss: 0.1722
step 31600; step_loss: 0.1592
step 31610; step_loss: 0.1804
step 31620; step_loss: 0.1691
step 31630; step_loss: 0.1669
step 31640; step_loss: 0.2432
step 31650; step_loss: 0.1751
step 31660; step_loss: 0.1467
step 31670; step_loss: 0.1232
step 31680; step_loss: 0.2178
step 31690; step_loss: 0.2412
step 31700; step_loss: 0.1382
step 31710; step_loss: 0.1743
step 31720; step_loss: 0.1543
step 31730; step_loss: 0.1757
step 31740; step_loss: 0.2213
step 31750; step_loss: 0.1479
step 31760; step_loss: 0.1758
step 31770; step_loss: 0.2102
step 31780; step_loss: 0.2536
step 31790; step_loss: 0.1574
step 31800; step_loss: 0.1964
step 31810; step_loss: 0.2620
step 31820; step_loss: 0.2172
step 31830; step_loss: 0.1334
step 31840; step_loss: 0.1973
step 31850; step_loss: 0.2026
step 31860; step_loss: 0.2173
step 31870; step_loss: 0.1904
step 31880; step_loss: 0.1439
step 31890; step_loss: 0.1531
step 31900; step_loss: 0.2071
step 31910; step_loss: 0.1636
step 31920; step_loss: 0.1526
step 31930; step_loss: 0.1469
step 31940; step_loss: 0.1983
step 31950; step_loss: 0.1397
step 31960; step_loss: 0.1762
step 31970; step_loss: 0.2160
step 31980; step_loss: 0.1610
step 31990; step_loss: 0.1626

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 1.055 | 1.142 | 1.296 | 1.360 | 1.478 | 1.752 |

============================
Global step:         32000
Learning rate:       0.0043
Step-time (ms):     35.8911
Train loss avg:      0.1876
--------------------------
Val loss:            0.8094
srnn loss:           0.6897
============================

Saving the model...
done in 403.37 ms
step 32000; step_loss: 0.2071
step 32010; step_loss: 0.1674
step 32020; step_loss: 0.2174
step 32030; step_loss: 0.1641
step 32040; step_loss: 0.1902
step 32050; step_loss: 0.1897
step 32060; step_loss: 0.2108
step 32070; step_loss: 0.2025
step 32080; step_loss: 0.1986
step 32090; step_loss: 0.1501
step 32100; step_loss: 0.1615
step 32110; step_loss: 0.1682
step 32120; step_loss: 0.1574
step 32130; step_loss: 0.1520
step 32140; step_loss: 0.2132
step 32150; step_loss: 0.2041
step 32160; step_loss: 0.2152
step 32170; step_loss: 0.2095
step 32180; step_loss: 0.2633
step 32190; step_loss: 0.2121
step 32200; step_loss: 0.1606
step 32210; step_loss: 0.1265
step 32220; step_loss: 0.1828
step 32230; step_loss: 0.1550
step 32240; step_loss: 0.1921
step 32250; step_loss: 0.1584
step 32260; step_loss: 0.1782
step 32270; step_loss: 0.1576
step 32280; step_loss: 0.2180
step 32290; step_loss: 0.2015
step 32300; step_loss: 0.2539
step 32310; step_loss: 0.1622
step 32320; step_loss: 0.1932
step 32330; step_loss: 0.1284
step 32340; step_loss: 0.2140
step 32350; step_loss: 0.1774
step 32360; step_loss: 0.2202
step 32370; step_loss: 0.1612
step 32380; step_loss: 0.2053
step 32390; step_loss: 0.1820
step 32400; step_loss: 0.1320
step 32410; step_loss: 0.1543
step 32420; step_loss: 0.1964
step 32430; step_loss: 0.1687
step 32440; step_loss: 0.1942
step 32450; step_loss: 0.2635
step 32460; step_loss: 0.1833
step 32470; step_loss: 0.2522
step 32480; step_loss: 0.1398
step 32490; step_loss: 0.1497
step 32500; step_loss: 0.1851
step 32510; step_loss: 0.1806
step 32520; step_loss: 0.2266
step 32530; step_loss: 0.1780
step 32540; step_loss: 0.1574
step 32550; step_loss: 0.2007
step 32560; step_loss: 0.1877
step 32570; step_loss: 0.1550
step 32580; step_loss: 0.2238
step 32590; step_loss: 0.2138
step 32600; step_loss: 0.1941
step 32610; step_loss: 0.1944
step 32620; step_loss: 0.1188
step 32630; step_loss: 0.1912
step 32640; step_loss: 0.1991
step 32650; step_loss: 0.1276
step 32660; step_loss: 0.1366
step 32670; step_loss: 0.1925
step 32680; step_loss: 0.1384
step 32690; step_loss: 0.2045
step 32700; step_loss: 0.1312
step 32710; step_loss: 0.2039
step 32720; step_loss: 0.1866
step 32730; step_loss: 0.1223
step 32740; step_loss: 0.2155
step 32750; step_loss: 0.2082
step 32760; step_loss: 0.1731
step 32770; step_loss: 0.2333
step 32780; step_loss: 0.3019
step 32790; step_loss: 0.1376
step 32800; step_loss: 0.1729
step 32810; step_loss: 0.1536
step 32820; step_loss: 0.1356
step 32830; step_loss: 0.1614
step 32840; step_loss: 0.3080
step 32850; step_loss: 0.2398
step 32860; step_loss: 0.1694
step 32870; step_loss: 0.1860
step 32880; step_loss: 0.1100
step 32890; step_loss: 0.1531
step 32900; step_loss: 0.1928
step 32910; step_loss: 0.1985
step 32920; step_loss: 0.1875
step 32930; step_loss: 0.1478
step 32940; step_loss: 0.1680
step 32950; step_loss: 0.1621
step 32960; step_loss: 0.1763
step 32970; step_loss: 0.1705
step 32980; step_loss: 0.1592
step 32990; step_loss: 0.1818

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 1.053 | 1.140 | 1.296 | 1.362 | 1.480 | 1.741 |

============================
Global step:         33000
Learning rate:       0.0043
Step-time (ms):     35.9333
Train loss avg:      0.1864
--------------------------
Val loss:            0.8781
srnn loss:           0.6958
============================

Saving the model...
done in 429.26 ms
step 33000; step_loss: 0.2119
step 33010; step_loss: 0.2220
step 33020; step_loss: 0.1981
step 33030; step_loss: 0.2289
step 33040; step_loss: 0.1619
step 33050; step_loss: 0.1544
step 33060; step_loss: 0.2124
step 33070; step_loss: 0.1741
step 33080; step_loss: 0.1781
step 33090; step_loss: 0.1414
step 33100; step_loss: 0.1700
step 33110; step_loss: 0.1522
step 33120; step_loss: 0.2679
step 33130; step_loss: 0.1233
step 33140; step_loss: 0.2261
step 33150; step_loss: 0.2128
step 33160; step_loss: 0.1845
step 33170; step_loss: 0.1501
step 33180; step_loss: 0.1901
step 33190; step_loss: 0.1615
step 33200; step_loss: 0.1583
step 33210; step_loss: 0.1902
step 33220; step_loss: 0.1990
step 33230; step_loss: 0.1927
step 33240; step_loss: 0.1834
step 33250; step_loss: 0.2145
step 33260; step_loss: 0.2287
step 33270; step_loss: 0.2139
step 33280; step_loss: 0.1272
step 33290; step_loss: 0.2218
step 33300; step_loss: 0.1599
step 33310; step_loss: 0.1434
step 33320; step_loss: 0.1595
step 33330; step_loss: 0.2121
step 33340; step_loss: 0.1882
step 33350; step_loss: 0.1411
step 33360; step_loss: 0.1659
step 33370; step_loss: 0.1954
step 33380; step_loss: 0.2430
step 33390; step_loss: 0.1760
step 33400; step_loss: 0.1653
step 33410; step_loss: 0.2432
step 33420; step_loss: 0.1552
step 33430; step_loss: 0.1496
step 33440; step_loss: 0.2055
step 33450; step_loss: 0.1615
step 33460; step_loss: 0.1603
step 33470; step_loss: 0.2065
step 33480; step_loss: 0.1645
step 33490; step_loss: 0.2273
step 33500; step_loss: 0.1855
step 33510; step_loss: 0.2105
step 33520; step_loss: 0.1668
step 33530; step_loss: 0.1892
step 33540; step_loss: 0.1606
step 33550; step_loss: 0.2415
step 33560; step_loss: 0.2353
step 33570; step_loss: 0.1571
step 33580; step_loss: 0.1916
step 33590; step_loss: 0.1013
step 33600; step_loss: 0.2068
step 33610; step_loss: 0.1760
step 33620; step_loss: 0.1517
step 33630; step_loss: 0.1758
step 33640; step_loss: 0.2231
step 33650; step_loss: 0.1342
step 33660; step_loss: 0.1316
step 33670; step_loss: 0.1735
step 33680; step_loss: 0.1807
step 33690; step_loss: 0.1456
step 33700; step_loss: 0.2004
step 33710; step_loss: 0.1711
step 33720; step_loss: 0.1831
step 33730; step_loss: 0.2129
step 33740; step_loss: 0.2916
step 33750; step_loss: 0.2332
step 33760; step_loss: 0.2007
step 33770; step_loss: 0.1983
step 33780; step_loss: 0.1857
step 33790; step_loss: 0.3106
step 33800; step_loss: 0.1912
step 33810; step_loss: 0.1455
step 33820; step_loss: 0.2443
step 33830; step_loss: 0.1675
step 33840; step_loss: 0.1763
step 33850; step_loss: 0.2189
step 33860; step_loss: 0.1581
step 33870; step_loss: 0.1577
step 33880; step_loss: 0.1540
step 33890; step_loss: 0.1754
step 33900; step_loss: 0.1470
step 33910; step_loss: 0.2204
step 33920; step_loss: 0.2037
step 33930; step_loss: 0.1709
step 33940; step_loss: 0.1692
step 33950; step_loss: 0.1549
step 33960; step_loss: 0.1656
step 33970; step_loss: 0.2410
step 33980; step_loss: 0.2137
step 33990; step_loss: 0.1755

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 1.048 | 1.138 | 1.295 | 1.358 | 1.475 | 1.720 |

============================
Global step:         34000
Learning rate:       0.0043
Step-time (ms):     35.9527
Train loss avg:      0.1861
--------------------------
Val loss:            0.8590
srnn loss:           0.6865
============================

Saving the model...
done in 401.16 ms
step 34000; step_loss: 0.1970
step 34010; step_loss: 0.1764
step 34020; step_loss: 0.1561
step 34030; step_loss: 0.1617
step 34040; step_loss: 0.1693
step 34050; step_loss: 0.1536
step 34060; step_loss: 0.2109
step 34070; step_loss: 0.1800
step 34080; step_loss: 0.2163
step 34090; step_loss: 0.1581
step 34100; step_loss: 0.1526
step 34110; step_loss: 0.1711
step 34120; step_loss: 0.1273
step 34130; step_loss: 0.2320
step 34140; step_loss: 0.1481
step 34150; step_loss: 0.1642
step 34160; step_loss: 0.1537
step 34170; step_loss: 0.2406
step 34180; step_loss: 0.1226
step 34190; step_loss: 0.1803
step 34200; step_loss: 0.1370
step 34210; step_loss: 0.1546
step 34220; step_loss: 0.2192
step 34230; step_loss: 0.2340
step 34240; step_loss: 0.1482
step 34250; step_loss: 0.1618
step 34260; step_loss: 0.3326
step 34270; step_loss: 0.1856
step 34280; step_loss: 0.2147
step 34290; step_loss: 0.2046
step 34300; step_loss: 0.1500
step 34310; step_loss: 0.2884
step 34320; step_loss: 0.1434
step 34330; step_loss: 0.1526
step 34340; step_loss: 0.1437
step 34350; step_loss: 0.1940
step 34360; step_loss: 0.1639
step 34370; step_loss: 0.1335
step 34380; step_loss: 0.1613
step 34390; step_loss: 0.2055
step 34400; step_loss: 0.1333
step 34410; step_loss: 0.2245
step 34420; step_loss: 0.1732
step 34430; step_loss: 0.1868
step 34440; step_loss: 0.1271
step 34450; step_loss: 0.1502
step 34460; step_loss: 0.1864
step 34470; step_loss: 0.1899
step 34480; step_loss: 0.1447
step 34490; step_loss: 0.1805
step 34500; step_loss: 0.1985
step 34510; step_loss: 0.2099
step 34520; step_loss: 0.1658
step 34530; step_loss: 0.1400
step 34540; step_loss: 0.1953
step 34550; step_loss: 0.1734
step 34560; step_loss: 0.1904
step 34570; step_loss: 0.1585
step 34580; step_loss: 0.2876
step 34590; step_loss: 0.1912
step 34600; step_loss: 0.1644
step 34610; step_loss: 0.1403
step 34620; step_loss: 0.1598
step 34630; step_loss: 0.1961
step 34640; step_loss: 0.2244
step 34650; step_loss: 0.1464
step 34660; step_loss: 0.2110
step 34670; step_loss: 0.2407
step 34680; step_loss: 0.1300
step 34690; step_loss: 0.1662
step 34700; step_loss: 0.1680
step 34710; step_loss: 0.1581
step 34720; step_loss: 0.1623
step 34730; step_loss: 0.1837
step 34740; step_loss: 0.2267
step 34750; step_loss: 0.1624
step 34760; step_loss: 0.1641
step 34770; step_loss: 0.1597
step 34780; step_loss: 0.1763
step 34790; step_loss: 0.2046
step 34800; step_loss: 0.1786
step 34810; step_loss: 0.1886
step 34820; step_loss: 0.1719
step 34830; step_loss: 0.1912
step 34840; step_loss: 0.1605
step 34850; step_loss: 0.2482
step 34860; step_loss: 0.1722
step 34870; step_loss: 0.1891
step 34880; step_loss: 0.1617
step 34890; step_loss: 0.1568
step 34900; step_loss: 0.1657
step 34910; step_loss: 0.1678
step 34920; step_loss: 0.1302
step 34930; step_loss: 0.1542
step 34940; step_loss: 0.1540
step 34950; step_loss: 0.2284
step 34960; step_loss: 0.2205
step 34970; step_loss: 0.2193
step 34980; step_loss: 0.1862
step 34990; step_loss: 0.1270

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 1.042 | 1.132 | 1.294 | 1.360 | 1.480 | 1.727 |

============================
Global step:         35000
Learning rate:       0.0043
Step-time (ms):     35.8788
Train loss avg:      0.1817
--------------------------
Val loss:            0.8287
srnn loss:           0.6835
============================

Saving the model...
done in 404.12 ms
step 35000; step_loss: 0.2062
step 35010; step_loss: 0.1785
step 35020; step_loss: 0.1704
step 35030; step_loss: 0.1787
step 35040; step_loss: 0.1847
step 35050; step_loss: 0.1591
step 35060; step_loss: 0.1755
step 35070; step_loss: 0.1950
step 35080; step_loss: 0.1719
step 35090; step_loss: 0.1335
step 35100; step_loss: 0.2280
step 35110; step_loss: 0.1924
step 35120; step_loss: 0.1832
step 35130; step_loss: 0.1890
step 35140; step_loss: 0.2082
step 35150; step_loss: 0.1975
step 35160; step_loss: 0.2519
step 35170; step_loss: 0.1491
step 35180; step_loss: 0.2119
step 35190; step_loss: 0.1582
step 35200; step_loss: 0.1654
step 35210; step_loss: 0.2670
step 35220; step_loss: 0.1829
step 35230; step_loss: 0.1831
step 35240; step_loss: 0.1637
step 35250; step_loss: 0.1324
step 35260; step_loss: 0.1232
step 35270; step_loss: 0.1576
step 35280; step_loss: 0.1953
step 35290; step_loss: 0.2349
step 35300; step_loss: 0.1722
step 35310; step_loss: 0.2007
step 35320; step_loss: 0.1957
step 35330; step_loss: 0.2135
step 35340; step_loss: 0.1996
step 35350; step_loss: 0.1668
step 35360; step_loss: 0.1937
step 35370; step_loss: 0.2461
step 35380; step_loss: 0.1702
step 35390; step_loss: 0.1257
step 35400; step_loss: 0.1671
step 35410; step_loss: 0.1898
step 35420; step_loss: 0.2602
step 35430; step_loss: 0.1690
step 35440; step_loss: 0.1560
step 35450; step_loss: 0.1429
step 35460; step_loss: 0.2110
step 35470; step_loss: 0.2635
step 35480; step_loss: 0.1497
step 35490; step_loss: 0.2373
step 35500; step_loss: 0.1405
step 35510; step_loss: 0.1622
step 35520; step_loss: 0.2291
step 35530; step_loss: 0.1236
step 35540; step_loss: 0.2067
step 35550; step_loss: 0.1373
step 35560; step_loss: 0.1920
step 35570; step_loss: 0.2111
step 35580; step_loss: 0.1646
step 35590; step_loss: 0.1633
step 35600; step_loss: 0.1747
step 35610; step_loss: 0.1771
step 35620; step_loss: 0.1379
step 35630; step_loss: 0.1820
step 35640; step_loss: 0.1860
step 35650; step_loss: 0.1547
step 35660; step_loss: 0.1432
step 35670; step_loss: 0.2085
step 35680; step_loss: 0.1747
step 35690; step_loss: 0.1754
step 35700; step_loss: 0.1726
step 35710; step_loss: 0.1719
step 35720; step_loss: 0.1529
step 35730; step_loss: 0.2116
step 35740; step_loss: 0.2152
step 35750; step_loss: 0.1577
step 35760; step_loss: 0.2173
step 35770; step_loss: 0.1831
step 35780; step_loss: 0.1517
step 35790; step_loss: 0.1898
step 35800; step_loss: 0.1460
step 35810; step_loss: 0.1711
step 35820; step_loss: 0.1913
step 35830; step_loss: 0.2418
step 35840; step_loss: 0.1884
step 35850; step_loss: 0.1934
step 35860; step_loss: 0.1625
step 35870; step_loss: 0.1709
step 35880; step_loss: 0.1833
step 35890; step_loss: 0.1707
step 35900; step_loss: 0.1469
step 35910; step_loss: 0.1722
step 35920; step_loss: 0.1348
step 35930; step_loss: 0.1642
step 35940; step_loss: 0.1723
step 35950; step_loss: 0.1575
step 35960; step_loss: 0.1826
step 35970; step_loss: 0.1909
step 35980; step_loss: 0.1622
step 35990; step_loss: 0.1435

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 1.044 | 1.139 | 1.310 | 1.378 | 1.502 | 1.745 |

============================
Global step:         36000
Learning rate:       0.0043
Step-time (ms):     35.8583
Train loss avg:      0.1816
--------------------------
Val loss:            0.7119
srnn loss:           0.6977
============================

Saving the model...
done in 407.63 ms
step 36000; step_loss: 0.1365
step 36010; step_loss: 0.2266
step 36020; step_loss: 0.1756
step 36030; step_loss: 0.1886
step 36040; step_loss: 0.1480
step 36050; step_loss: 0.1613
step 36060; step_loss: 0.2048
step 36070; step_loss: 0.1576
step 36080; step_loss: 0.0996
step 36090; step_loss: 0.2150
step 36100; step_loss: 0.1610
step 36110; step_loss: 0.2085
step 36120; step_loss: 0.1644
step 36130; step_loss: 0.2756
step 36140; step_loss: 0.1288
step 36150; step_loss: 0.1595
step 36160; step_loss: 0.2121
step 36170; step_loss: 0.2199
step 36180; step_loss: 0.1782
step 36190; step_loss: 0.1975
step 36200; step_loss: 0.2050
step 36210; step_loss: 0.2669
step 36220; step_loss: 0.1658
step 36230; step_loss: 0.1424
step 36240; step_loss: 0.2073
step 36250; step_loss: 0.2081
step 36260; step_loss: 0.2408
step 36270; step_loss: 0.1371
step 36280; step_loss: 0.2389
step 36290; step_loss: 0.1628
step 36300; step_loss: 0.1101
step 36310; step_loss: 0.1691
step 36320; step_loss: 0.1916
step 36330; step_loss: 0.2295
step 36340; step_loss: 0.1827
step 36350; step_loss: 0.1867
step 36360; step_loss: 0.1916
step 36370; step_loss: 0.1289
step 36380; step_loss: 0.1835
step 36390; step_loss: 0.2182
step 36400; step_loss: 0.1747
step 36410; step_loss: 0.1646
step 36420; step_loss: 0.1511
step 36430; step_loss: 0.2532
step 36440; step_loss: 0.1544
step 36450; step_loss: 0.2381
step 36460; step_loss: 0.1585
step 36470; step_loss: 0.2156
step 36480; step_loss: 0.1715
step 36490; step_loss: 0.1630
step 36500; step_loss: 0.1313
step 36510; step_loss: 0.1566
step 36520; step_loss: 0.1249
step 36530; step_loss: 0.1872
step 36540; step_loss: 0.2073
step 36550; step_loss: 0.1450
step 36560; step_loss: 0.1384
step 36570; step_loss: 0.1522
step 36580; step_loss: 0.1197
step 36590; step_loss: 0.1407
step 36600; step_loss: 0.1345
step 36610; step_loss: 0.1432
step 36620; step_loss: 0.2031
step 36630; step_loss: 0.2442
step 36640; step_loss: 0.2365
step 36650; step_loss: 0.2468
step 36660; step_loss: 0.2614
step 36670; step_loss: 0.2626
step 36680; step_loss: 0.2186
step 36690; step_loss: 0.1953
step 36700; step_loss: 0.1960
step 36710; step_loss: 0.1492
step 36720; step_loss: 0.1943
step 36730; step_loss: 0.2188
step 36740; step_loss: 0.1651
step 36750; step_loss: 0.1619
step 36760; step_loss: 0.1568
step 36770; step_loss: 0.1419
step 36780; step_loss: 0.2127
step 36790; step_loss: 0.1582
step 36800; step_loss: 0.1283
step 36810; step_loss: 0.1814
step 36820; step_loss: 0.1547
step 36830; step_loss: 0.1756
step 36840; step_loss: 0.1573
step 36850; step_loss: 0.1841
step 36860; step_loss: 0.2013
step 36870; step_loss: 0.2070
step 36880; step_loss: 0.1595
step 36890; step_loss: 0.1233
step 36900; step_loss: 0.1808
step 36910; step_loss: 0.1461
step 36920; step_loss: 0.2099
step 36930; step_loss: 0.2071
step 36940; step_loss: 0.1523
step 36950; step_loss: 0.1973
step 36960; step_loss: 0.2087
step 36970; step_loss: 0.1501
step 36980; step_loss: 0.1437
step 36990; step_loss: 0.1306

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 1.035 | 1.132 | 1.306 | 1.375 | 1.501 | 1.750 |

============================
Global step:         37000
Learning rate:       0.0043
Step-time (ms):     35.8276
Train loss avg:      0.1771
--------------------------
Val loss:            0.9010
srnn loss:           0.6951
============================

Saving the model...
done in 406.42 ms
step 37000; step_loss: 0.1694
step 37010; step_loss: 0.1774
step 37020; step_loss: 0.1221
step 37030; step_loss: 0.2763
step 37040; step_loss: 0.1753
step 37050; step_loss: 0.1998
step 37060; step_loss: 0.1709
step 37070; step_loss: 0.1874
step 37080; step_loss: 0.2561
step 37090; step_loss: 0.1423
step 37100; step_loss: 0.1213
step 37110; step_loss: 0.1618
step 37120; step_loss: 0.1734
step 37130; step_loss: 0.1265
step 37140; step_loss: 0.1826
step 37150; step_loss: 0.1735
step 37160; step_loss: 0.1560
step 37170; step_loss: 0.2385
step 37180; step_loss: 0.1423
step 37190; step_loss: 0.1859
step 37200; step_loss: 0.1366
step 37210; step_loss: 0.1923
step 37220; step_loss: 0.1572
step 37230; step_loss: 0.1731
step 37240; step_loss: 0.1571
step 37250; step_loss: 0.2009
step 37260; step_loss: 0.2529
step 37270; step_loss: 0.1774
step 37280; step_loss: 0.1803
step 37290; step_loss: 0.1786
step 37300; step_loss: 0.1969
step 37310; step_loss: 0.1185
step 37320; step_loss: 0.1274
step 37330; step_loss: 0.1573
step 37340; step_loss: 0.1466
step 37350; step_loss: 0.1857
step 37360; step_loss: 0.1516
step 37370; step_loss: 0.1476
step 37380; step_loss: 0.1382
step 37390; step_loss: 0.1907
step 37400; step_loss: 0.1195
step 37410; step_loss: 0.1362
step 37420; step_loss: 0.1818
step 37430; step_loss: 0.1968
step 37440; step_loss: 0.1416
step 37450; step_loss: 0.1511
step 37460; step_loss: 0.2277
step 37470; step_loss: 0.1772
step 37480; step_loss: 0.2021
step 37490; step_loss: 0.2650
step 37500; step_loss: 0.1607
step 37510; step_loss: 0.1484
step 37520; step_loss: 0.1341
step 37530; step_loss: 0.1765
step 37540; step_loss: 0.1689
step 37550; step_loss: 0.1885
step 37560; step_loss: 0.1568
step 37570; step_loss: 0.1511
step 37580; step_loss: 0.2434
step 37590; step_loss: 0.2254
step 37600; step_loss: 0.1894
step 37610; step_loss: 0.2370
step 37620; step_loss: 0.1686
step 37630; step_loss: 0.1611
step 37640; step_loss: 0.1827
step 37650; step_loss: 0.1688
step 37660; step_loss: 0.1324
step 37670; step_loss: 0.1559
step 37680; step_loss: 0.1743
step 37690; step_loss: 0.1935
step 37700; step_loss: 0.1360
step 37710; step_loss: 0.1807
step 37720; step_loss: 0.2279
step 37730; step_loss: 0.2067
step 37740; step_loss: 0.2084
step 37750; step_loss: 0.1500
step 37760; step_loss: 0.1931
step 37770; step_loss: 0.1081
step 37780; step_loss: 0.1897
step 37790; step_loss: 0.1821
step 37800; step_loss: 0.2139
step 37810; step_loss: 0.1617
step 37820; step_loss: 0.1300
step 37830; step_loss: 0.2612
step 37840; step_loss: 0.1460
step 37850; step_loss: 0.1459
step 37860; step_loss: 0.1634
step 37870; step_loss: 0.1754
step 37880; step_loss: 0.1473
step 37890; step_loss: 0.1714
step 37900; step_loss: 0.1509
step 37910; step_loss: 0.2109
step 37920; step_loss: 0.1435
step 37930; step_loss: 0.1544
step 37940; step_loss: 0.1280
step 37950; step_loss: 0.1357
step 37960; step_loss: 0.1610
step 37970; step_loss: 0.1733
step 37980; step_loss: 0.1591
step 37990; step_loss: 0.1266

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 1.033 | 1.131 | 1.308 | 1.379 | 1.506 | 1.756 |

============================
Global step:         38000
Learning rate:       0.0043
Step-time (ms):     35.9207
Train loss avg:      0.1768
--------------------------
Val loss:            0.7904
srnn loss:           0.6917
============================

Saving the model...
done in 406.02 ms
step 38000; step_loss: 0.1773
step 38010; step_loss: 0.1564
step 38020; step_loss: 0.1799
step 38030; step_loss: 0.2224
step 38040; step_loss: 0.2008
step 38050; step_loss: 0.1946
step 38060; step_loss: 0.2027
step 38070; step_loss: 0.1079
step 38080; step_loss: 0.2267
step 38090; step_loss: 0.1954
step 38100; step_loss: 0.1478
step 38110; step_loss: 0.1588
step 38120; step_loss: 0.1935
step 38130; step_loss: 0.1270
step 38140; step_loss: 0.1141
step 38150; step_loss: 0.2041
step 38160; step_loss: 0.1810
step 38170; step_loss: 0.1617
step 38180; step_loss: 0.1592
step 38190; step_loss: 0.1452
step 38200; step_loss: 0.1807
step 38210; step_loss: 0.1874
step 38220; step_loss: 0.1836
step 38230; step_loss: 0.1469
step 38240; step_loss: 0.1875
step 38250; step_loss: 0.2068
step 38260; step_loss: 0.2381
step 38270; step_loss: 0.1929
step 38280; step_loss: 0.1746
step 38290; step_loss: 0.1310
step 38300; step_loss: 0.2385
step 38310; step_loss: 0.1633
step 38320; step_loss: 0.1157
step 38330; step_loss: 0.1299
step 38340; step_loss: 0.1676
step 38350; step_loss: 0.2819
step 38360; step_loss: 0.1923
step 38370; step_loss: 0.1897
step 38380; step_loss: 0.1190
step 38390; step_loss: 0.1241
step 38400; step_loss: 0.1578
step 38410; step_loss: 0.1368
step 38420; step_loss: 0.1871
step 38430; step_loss: 0.1918
step 38440; step_loss: 0.1737
step 38450; step_loss: 0.1745
step 38460; step_loss: 0.1479
step 38470; step_loss: 0.2377
step 38480; step_loss: 0.1741
step 38490; step_loss: 0.2526
step 38500; step_loss: 0.1579
step 38510; step_loss: 0.2497
step 38520; step_loss: 0.1138
step 38530; step_loss: 0.1599
step 38540; step_loss: 0.1480
step 38550; step_loss: 0.1158
step 38560; step_loss: 0.1096
step 38570; step_loss: 0.2443
step 38580; step_loss: 0.2310
step 38590; step_loss: 0.1521
step 38600; step_loss: 0.1409
step 38610; step_loss: 0.2283
step 38620; step_loss: 0.1603
step 38630; step_loss: 0.2176
step 38640; step_loss: 0.1815
step 38650; step_loss: 0.1917
step 38660; step_loss: 0.1441
step 38670; step_loss: 0.2165
step 38680; step_loss: 0.1649
step 38690; step_loss: 0.2184
step 38700; step_loss: 0.1720
step 38710; step_loss: 0.1462
step 38720; step_loss: 0.2336
step 38730; step_loss: 0.1657
step 38740; step_loss: 0.1940
step 38750; step_loss: 0.1599
step 38760; step_loss: 0.2070
step 38770; step_loss: 0.1259
step 38780; step_loss: 0.1948
step 38790; step_loss: 0.1797
step 38800; step_loss: 0.1818
step 38810; step_loss: 0.1919
step 38820; step_loss: 0.1434
step 38830; step_loss: 0.2062
step 38840; step_loss: 0.1544
step 38850; step_loss: 0.1629
step 38860; step_loss: 0.1733
step 38870; step_loss: 0.1752
step 38880; step_loss: 0.1782
step 38890; step_loss: 0.2505
step 38900; step_loss: 0.2140
step 38910; step_loss: 0.1721
step 38920; step_loss: 0.2172
step 38930; step_loss: 0.2121
step 38940; step_loss: 0.1477
step 38950; step_loss: 0.2775
step 38960; step_loss: 0.1633
step 38970; step_loss: 0.2057
step 38980; step_loss: 0.1699
step 38990; step_loss: 0.1590

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 1.031 | 1.130 | 1.310 | 1.381 | 1.510 | 1.752 |

============================
Global step:         39000
Learning rate:       0.0043
Step-time (ms):     35.8944
Train loss avg:      0.1752
--------------------------
Val loss:            0.8002
srnn loss:           0.6913
============================

Saving the model...
done in 440.66 ms
step 39000; step_loss: 0.1455
step 39010; step_loss: 0.2067
step 39020; step_loss: 0.1452
step 39030; step_loss: 0.1699
step 39040; step_loss: 0.2052
step 39050; step_loss: 0.1472
step 39060; step_loss: 0.1715
step 39070; step_loss: 0.1316
step 39080; step_loss: 0.1719
step 39090; step_loss: 0.1506
step 39100; step_loss: 0.1501
step 39110; step_loss: 0.1473
step 39120; step_loss: 0.1734
step 39130; step_loss: 0.1754
step 39140; step_loss: 0.1844
step 39150; step_loss: 0.1986
step 39160; step_loss: 0.1624
step 39170; step_loss: 0.2205
step 39180; step_loss: 0.1536
step 39190; step_loss: 0.1796
step 39200; step_loss: 0.1781
step 39210; step_loss: 0.1227
step 39220; step_loss: 0.1588
step 39230; step_loss: 0.1694
step 39240; step_loss: 0.1567
step 39250; step_loss: 0.1901
step 39260; step_loss: 0.1679
step 39270; step_loss: 0.1101
step 39280; step_loss: 0.2515
step 39290; step_loss: 0.1234
step 39300; step_loss: 0.1368
step 39310; step_loss: 0.1846
step 39320; step_loss: 0.1687
step 39330; step_loss: 0.1913
step 39340; step_loss: 0.2077
step 39350; step_loss: 0.1797
step 39360; step_loss: 0.1708
step 39370; step_loss: 0.1507
step 39380; step_loss: 0.2011
step 39390; step_loss: 0.2483
step 39400; step_loss: 0.1300
step 39410; step_loss: 0.1985
step 39420; step_loss: 0.1786
step 39430; step_loss: 0.1509
step 39440; step_loss: 0.1925
step 39450; step_loss: 0.1696
step 39460; step_loss: 0.1864
step 39470; step_loss: 0.1786
step 39480; step_loss: 0.1911
step 39490; step_loss: 0.2274
step 39500; step_loss: 0.1894
step 39510; step_loss: 0.1978
step 39520; step_loss: 0.1740
step 39530; step_loss: 0.1151
step 39540; step_loss: 0.2241
step 39550; step_loss: 0.1921
step 39560; step_loss: 0.1839
step 39570; step_loss: 0.1776
step 39580; step_loss: 0.1390
step 39590; step_loss: 0.2137
step 39600; step_loss: 0.1499
step 39610; step_loss: 0.1596
step 39620; step_loss: 0.1640
step 39630; step_loss: 0.1505
step 39640; step_loss: 0.1796
step 39650; step_loss: 0.2050
step 39660; step_loss: 0.1439
step 39670; step_loss: 0.1819
step 39680; step_loss: 0.1834
step 39690; step_loss: 0.2056
step 39700; step_loss: 0.1571
step 39710; step_loss: 0.1406
step 39720; step_loss: 0.1933
step 39730; step_loss: 0.1853
step 39740; step_loss: 0.1398
step 39750; step_loss: 0.1310
step 39760; step_loss: 0.2183
step 39770; step_loss: 0.1235
step 39780; step_loss: 0.1375
step 39790; step_loss: 0.1501
step 39800; step_loss: 0.1680
step 39810; step_loss: 0.1861
step 39820; step_loss: 0.1428
step 39830; step_loss: 0.2181
step 39840; step_loss: 0.1434
step 39850; step_loss: 0.2095
step 39860; step_loss: 0.1807
step 39870; step_loss: 0.1882
step 39880; step_loss: 0.1738
step 39890; step_loss: 0.1356
step 39900; step_loss: 0.1514
step 39910; step_loss: 0.1819
step 39920; step_loss: 0.2440
step 39930; step_loss: 0.1185
step 39940; step_loss: 0.1613
step 39950; step_loss: 0.1749
step 39960; step_loss: 0.1837
step 39970; step_loss: 0.1659
step 39980; step_loss: 0.1347
step 39990; step_loss: 0.2090

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 1.035 | 1.140 | 1.331 | 1.406 | 1.542 | 1.785 |

============================
Global step:         40000
Learning rate:       0.0041
Step-time (ms):     36.0112
Train loss avg:      0.1724
--------------------------
Val loss:            0.8035
srnn loss:           0.7059
============================

Saving the model...
done in 410.31 ms
step 40000; step_loss: 0.1431
step 40010; step_loss: 0.1625
step 40020; step_loss: 0.1260
step 40030; step_loss: 0.1438
step 40040; step_loss: 0.2963
step 40050; step_loss: 0.1813
step 40060; step_loss: 0.1512
step 40070; step_loss: 0.1307
step 40080; step_loss: 0.1545
step 40090; step_loss: 0.1615
step 40100; step_loss: 0.1896
step 40110; step_loss: 0.1756
step 40120; step_loss: 0.1550
step 40130; step_loss: 0.1996
step 40140; step_loss: 0.2026
step 40150; step_loss: 0.1691
step 40160; step_loss: 0.1364
step 40170; step_loss: 0.1472
step 40180; step_loss: 0.2152
step 40190; step_loss: 0.1865
step 40200; step_loss: 0.1376
step 40210; step_loss: 0.1308
step 40220; step_loss: 0.1916
step 40230; step_loss: 0.2003
step 40240; step_loss: 0.1924
step 40250; step_loss: 0.1130
step 40260; step_loss: 0.1407
step 40270; step_loss: 0.1591
step 40280; step_loss: 0.1804
step 40290; step_loss: 0.2183
step 40300; step_loss: 0.1730
step 40310; step_loss: 0.1833
step 40320; step_loss: 0.1563
step 40330; step_loss: 0.1488
step 40340; step_loss: 0.1765
step 40350; step_loss: 0.1514
step 40360; step_loss: 0.1492
step 40370; step_loss: 0.2454
step 40380; step_loss: 0.1599
step 40390; step_loss: 0.1207
step 40400; step_loss: 0.1807
step 40410; step_loss: 0.1441
step 40420; step_loss: 0.1843
step 40430; step_loss: 0.1639
step 40440; step_loss: 0.2819
step 40450; step_loss: 0.1810
step 40460; step_loss: 0.1548
step 40470; step_loss: 0.1712
step 40480; step_loss: 0.1676
step 40490; step_loss: 0.1798
step 40500; step_loss: 0.1470
step 40510; step_loss: 0.1457
step 40520; step_loss: 0.1613
step 40530; step_loss: 0.1538
step 40540; step_loss: 0.2202
step 40550; step_loss: 0.1832
step 40560; step_loss: 0.2046
step 40570; step_loss: 0.1638
step 40580; step_loss: 0.1449
step 40590; step_loss: 0.1590
step 40600; step_loss: 0.2106
step 40610; step_loss: 0.1585
step 40620; step_loss: 0.1817
step 40630; step_loss: 0.1885
step 40640; step_loss: 0.2094
step 40650; step_loss: 0.1768
step 40660; step_loss: 0.1946
step 40670; step_loss: 0.2406
step 40680; step_loss: 0.1791
step 40690; step_loss: 0.1691
step 40700; step_loss: 0.1526
step 40710; step_loss: 0.1857
step 40720; step_loss: 0.1774
step 40730; step_loss: 0.1492
step 40740; step_loss: 0.1488
step 40750; step_loss: 0.1689
step 40760; step_loss: 0.2648
step 40770; step_loss: 0.1837
step 40780; step_loss: 0.2026
step 40790; step_loss: 0.1733
step 40800; step_loss: 0.1422
step 40810; step_loss: 0.1243
step 40820; step_loss: 0.2325
step 40830; step_loss: 0.0996
step 40840; step_loss: 0.2122
step 40850; step_loss: 0.1275
step 40860; step_loss: 0.1517
step 40870; step_loss: 0.1853
step 40880; step_loss: 0.1525
step 40890; step_loss: 0.2547
step 40900; step_loss: 0.1505
step 40910; step_loss: 0.1811
step 40920; step_loss: 0.1956
step 40930; step_loss: 0.1799
step 40940; step_loss: 0.1816
step 40950; step_loss: 0.1767
step 40960; step_loss: 0.1613
step 40970; step_loss: 0.1907
step 40980; step_loss: 0.1862
step 40990; step_loss: 0.2080

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 1.037 | 1.144 | 1.341 | 1.418 | 1.555 | 1.803 |

============================
Global step:         41000
Learning rate:       0.0041
Step-time (ms):     35.9171
Train loss avg:      0.1707
--------------------------
Val loss:            0.7653
srnn loss:           0.7091
============================

Saving the model...
done in 407.04 ms
step 41000; step_loss: 0.1734
step 41010; step_loss: 0.1581
step 41020; step_loss: 0.1656
step 41030; step_loss: 0.1568
step 41040; step_loss: 0.2145
step 41050; step_loss: 0.1490
step 41060; step_loss: 0.1329
step 41070; step_loss: 0.2497
step 41080; step_loss: 0.2034
step 41090; step_loss: 0.1759
step 41100; step_loss: 0.2424
step 41110; step_loss: 0.1664
step 41120; step_loss: 0.1507
step 41130; step_loss: 0.2073
step 41140; step_loss: 0.1508
step 41150; step_loss: 0.2272
step 41160; step_loss: 0.1552
step 41170; step_loss: 0.1398
step 41180; step_loss: 0.2232
step 41190; step_loss: 0.1307
step 41200; step_loss: 0.2120
step 41210; step_loss: 0.1425
step 41220; step_loss: 0.1794
step 41230; step_loss: 0.1832
step 41240; step_loss: 0.2200
step 41250; step_loss: 0.1334
step 41260; step_loss: 0.1952
step 41270; step_loss: 0.1216
step 41280; step_loss: 0.2097
step 41290; step_loss: 0.1866
step 41300; step_loss: 0.1290
step 41310; step_loss: 0.1044
step 41320; step_loss: 0.1488
step 41330; step_loss: 0.1757
step 41340; step_loss: 0.1869
step 41350; step_loss: 0.1688
step 41360; step_loss: 0.1510
step 41370; step_loss: 0.1897
step 41380; step_loss: 0.1512
step 41390; step_loss: 0.2216
step 41400; step_loss: 0.1103
step 41410; step_loss: 0.1627
step 41420; step_loss: 0.1884
step 41430; step_loss: 0.1952
step 41440; step_loss: 0.1589
step 41450; step_loss: 0.1643
step 41460; step_loss: 0.1834
step 41470; step_loss: 0.1203
step 41480; step_loss: 0.1306
step 41490; step_loss: 0.1581
step 41500; step_loss: 0.1302
step 41510; step_loss: 0.1636
step 41520; step_loss: 0.1287
step 41530; step_loss: 0.1522
step 41540; step_loss: 0.1987
step 41550; step_loss: 0.1612
step 41560; step_loss: 0.2311
step 41570; step_loss: 0.1635
step 41580; step_loss: 0.1910
step 41590; step_loss: 0.1885
step 41600; step_loss: 0.1631
step 41610; step_loss: 0.1522
step 41620; step_loss: 0.2099
step 41630; step_loss: 0.2098
step 41640; step_loss: 0.1339
step 41650; step_loss: 0.1730
step 41660; step_loss: 0.1508
step 41670; step_loss: 0.1704
step 41680; step_loss: 0.2522
step 41690; step_loss: 0.1414
step 41700; step_loss: 0.1518
step 41710; step_loss: 0.1753
step 41720; step_loss: 0.1709
step 41730; step_loss: 0.1652
step 41740; step_loss: 0.1658
step 41750; step_loss: 0.2134
step 41760; step_loss: 0.1595
step 41770; step_loss: 0.1848
step 41780; step_loss: 0.1495
step 41790; step_loss: 0.1496
step 41800; step_loss: 0.1668
step 41810; step_loss: 0.2364
step 41820; step_loss: 0.1701
step 41830; step_loss: 0.2150
step 41840; step_loss: 0.1494
step 41850; step_loss: 0.1803
step 41860; step_loss: 0.1535
step 41870; step_loss: 0.2458
step 41880; step_loss: 0.1352
step 41890; step_loss: 0.1803
step 41900; step_loss: 0.1211
step 41910; step_loss: 0.1525
step 41920; step_loss: 0.1751
step 41930; step_loss: 0.2915
step 41940; step_loss: 0.1343
step 41950; step_loss: 0.1338
step 41960; step_loss: 0.2532
step 41970; step_loss: 0.1530
step 41980; step_loss: 0.1709
step 41990; step_loss: 0.2042

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 1.028 | 1.137 | 1.339 | 1.419 | 1.563 | 1.825 |

============================
Global step:         42000
Learning rate:       0.0041
Step-time (ms):     35.8574
Train loss avg:      0.1691
--------------------------
Val loss:            0.8775
srnn loss:           0.7138
============================

Saving the model...
done in 391.73 ms
step 42000; step_loss: 0.1316
step 42010; step_loss: 0.2018
step 42020; step_loss: 0.1232
step 42030; step_loss: 0.1675
step 42040; step_loss: 0.1442
step 42050; step_loss: 0.1743
step 42060; step_loss: 0.1745
step 42070; step_loss: 0.1064
step 42080; step_loss: 0.1172
step 42090; step_loss: 0.2391
step 42100; step_loss: 0.1834
step 42110; step_loss: 0.2025
step 42120; step_loss: 0.1794
step 42130; step_loss: 0.1345
step 42140; step_loss: 0.1714
step 42150; step_loss: 0.1394
step 42160; step_loss: 0.2064
step 42170; step_loss: 0.1973
step 42180; step_loss: 0.1772
step 42190; step_loss: 0.1579
step 42200; step_loss: 0.1561
step 42210; step_loss: 0.2124
step 42220; step_loss: 0.1034
step 42230; step_loss: 0.1541
step 42240; step_loss: 0.1242
step 42250; step_loss: 0.1637
step 42260; step_loss: 0.1996
step 42270; step_loss: 0.2414
step 42280; step_loss: 0.1771
step 42290; step_loss: 0.1927
step 42300; step_loss: 0.2092
step 42310; step_loss: 0.1328
step 42320; step_loss: 0.2183
step 42330; step_loss: 0.1647
step 42340; step_loss: 0.2229
step 42350; step_loss: 0.1719
step 42360; step_loss: 0.1739
step 42370; step_loss: 0.1457
step 42380; step_loss: 0.1793
step 42390; step_loss: 0.1556
step 42400; step_loss: 0.1327
step 42410; step_loss: 0.1597
step 42420; step_loss: 0.1414
step 42430; step_loss: 0.1773
step 42440; step_loss: 0.1708
step 42450; step_loss: 0.1310
step 42460; step_loss: 0.1633
step 42470; step_loss: 0.1925
step 42480; step_loss: 0.1793
step 42490; step_loss: 0.1172
step 42500; step_loss: 0.1810
step 42510; step_loss: 0.1846
step 42520; step_loss: 0.1264
step 42530; step_loss: 0.1411
step 42540; step_loss: 0.1049
step 42550; step_loss: 0.1208
step 42560; step_loss: 0.1471
step 42570; step_loss: 0.1811
step 42580; step_loss: 0.1637
step 42590; step_loss: 0.1300
step 42600; step_loss: 0.1521
step 42610; step_loss: 0.1470
step 42620; step_loss: 0.2539
step 42630; step_loss: 0.1682
step 42640; step_loss: 0.1674
step 42650; step_loss: 0.1464
step 42660; step_loss: 0.2009
step 42670; step_loss: 0.1233
step 42680; step_loss: 0.1476
step 42690; step_loss: 0.2251
step 42700; step_loss: 0.1549
step 42710; step_loss: 0.1569
step 42720; step_loss: 0.1860
step 42730; step_loss: 0.1913
step 42740; step_loss: 0.1747
step 42750; step_loss: 0.1591
step 42760; step_loss: 0.1828
step 42770; step_loss: 0.1752
step 42780; step_loss: 0.1713
step 42790; step_loss: 0.1847
step 42800; step_loss: 0.1732
step 42810; step_loss: 0.1297
step 42820; step_loss: 0.1482
step 42830; step_loss: 0.1554
step 42840; step_loss: 0.2053
step 42850; step_loss: 0.1620
step 42860; step_loss: 0.1895
step 42870; step_loss: 0.2092
step 42880; step_loss: 0.1589
step 42890; step_loss: 0.2641
step 42900; step_loss: 0.2176
step 42910; step_loss: 0.1095
step 42920; step_loss: 0.1844
step 42930; step_loss: 0.1411
step 42940; step_loss: 0.1345
step 42950; step_loss: 0.2038
step 42960; step_loss: 0.1276
step 42970; step_loss: 0.1497
step 42980; step_loss: 0.1490
step 42990; step_loss: 0.1626

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 1.032 | 1.144 | 1.351 | 1.433 | 1.575 | 1.821 |

============================
Global step:         43000
Learning rate:       0.0041
Step-time (ms):     35.8134
Train loss avg:      0.1666
--------------------------
Val loss:            0.9209
srnn loss:           0.7195
============================

Saving the model...
done in 404.21 ms
step 43000; step_loss: 0.2924
step 43010; step_loss: 0.1483
step 43020; step_loss: 0.1525
step 43030; step_loss: 0.1867
step 43040; step_loss: 0.1903
step 43050; step_loss: 0.1448
step 43060; step_loss: 0.1360
step 43070; step_loss: 0.1485
step 43080; step_loss: 0.1931
step 43090; step_loss: 0.0966
step 43100; step_loss: 0.1192
step 43110; step_loss: 0.1561
step 43120; step_loss: 0.2218
step 43130; step_loss: 0.1519
step 43140; step_loss: 0.1700
step 43150; step_loss: 0.1649
step 43160; step_loss: 0.1691
step 43170; step_loss: 0.1883
step 43180; step_loss: 0.1617
step 43190; step_loss: 0.2077
step 43200; step_loss: 0.1726
step 43210; step_loss: 0.1243
step 43220; step_loss: 0.1677
step 43230; step_loss: 0.1641
step 43240; step_loss: 0.1908
step 43250; step_loss: 0.1476
step 43260; step_loss: 0.2328
step 43270; step_loss: 0.1739
step 43280; step_loss: 0.1256
step 43290; step_loss: 0.1298
step 43300; step_loss: 0.1389
step 43310; step_loss: 0.1375
step 43320; step_loss: 0.2349
step 43330; step_loss: 0.2483
step 43340; step_loss: 0.1433
step 43350; step_loss: 0.1533
step 43360; step_loss: 0.1718
step 43370; step_loss: 0.1901
step 43380; step_loss: 0.1392
step 43390; step_loss: 0.1358
step 43400; step_loss: 0.1388
step 43410; step_loss: 0.0984
step 43420; step_loss: 0.1445
step 43430; step_loss: 0.1778
step 43440; step_loss: 0.1814
step 43450; step_loss: 0.1672
step 43460; step_loss: 0.2046
step 43470; step_loss: 0.1362
step 43480; step_loss: 0.1374
step 43490; step_loss: 0.1590
step 43500; step_loss: 0.1760
step 43510; step_loss: 0.1930
step 43520; step_loss: 0.1556
step 43530; step_loss: 0.2135
step 43540; step_loss: 0.1614
step 43550; step_loss: 0.2073
step 43560; step_loss: 0.1608
step 43570; step_loss: 0.1834
step 43580; step_loss: 0.2272
step 43590; step_loss: 0.1430
step 43600; step_loss: 0.1314
step 43610; step_loss: 0.1155
step 43620; step_loss: 0.1679
step 43630; step_loss: 0.1061
step 43640; step_loss: 0.1931
step 43650; step_loss: 0.1353
step 43660; step_loss: 0.2139
step 43670; step_loss: 0.1293
step 43680; step_loss: 0.1710
step 43690; step_loss: 0.1436
step 43700; step_loss: 0.1440
step 43710; step_loss: 0.1827
step 43720; step_loss: 0.1652
step 43730; step_loss: 0.1903
step 43740; step_loss: 0.1854
step 43750; step_loss: 0.2252
step 43760; step_loss: 0.2088
step 43770; step_loss: 0.1820
step 43780; step_loss: 0.1191
step 43790; step_loss: 0.1507
step 43800; step_loss: 0.1471
step 43810; step_loss: 0.1409
step 43820; step_loss: 0.1779
step 43830; step_loss: 0.1183
step 43840; step_loss: 0.1250
step 43850; step_loss: 0.1961
step 43860; step_loss: 0.2614
step 43870; step_loss: 0.1524
step 43880; step_loss: 0.2168
step 43890; step_loss: 0.1752
step 43900; step_loss: 0.2041
step 43910; step_loss: 0.1474
step 43920; step_loss: 0.1929
step 43930; step_loss: 0.1770
step 43940; step_loss: 0.1164
step 43950; step_loss: 0.1340
step 43960; step_loss: 0.1549
step 43970; step_loss: 0.1047
step 43980; step_loss: 0.1524
step 43990; step_loss: 0.2028

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 1.023 | 1.136 | 1.344 | 1.426 | 1.572 | 1.827 |

============================
Global step:         44000
Learning rate:       0.0041
Step-time (ms):     35.9472
Train loss avg:      0.1644
--------------------------
Val loss:            0.8639
srnn loss:           0.7123
============================

Saving the model...
done in 419.77 ms
step 44000; step_loss: 0.1210
step 44010; step_loss: 0.1939
step 44020; step_loss: 0.1129
step 44030; step_loss: 0.1574
step 44040; step_loss: 0.1581
step 44050; step_loss: 0.1472
step 44060; step_loss: 0.1534
step 44070; step_loss: 0.1985
step 44080; step_loss: 0.1846
step 44090; step_loss: 0.0963
step 44100; step_loss: 0.1214
step 44110; step_loss: 0.1917
step 44120; step_loss: 0.1748
step 44130; step_loss: 0.1251
step 44140; step_loss: 0.2111
step 44150; step_loss: 0.1205
step 44160; step_loss: 0.1708
step 44170; step_loss: 0.1395
step 44180; step_loss: 0.1281
step 44190; step_loss: 0.1905
step 44200; step_loss: 0.1898
step 44210; step_loss: 0.2036
step 44220; step_loss: 0.1686
step 44230; step_loss: 0.1260
step 44240; step_loss: 0.2282
step 44250; step_loss: 0.1358
step 44260; step_loss: 0.1719
step 44270; step_loss: 0.1455
step 44280; step_loss: 0.1472
step 44290; step_loss: 0.1398
step 44300; step_loss: 0.1462
step 44310; step_loss: 0.1664
step 44320; step_loss: 0.1456
step 44330; step_loss: 0.1401
step 44340; step_loss: 0.1288
step 44350; step_loss: 0.1906
step 44360; step_loss: 0.1658
step 44370; step_loss: 0.1712
step 44380; step_loss: 0.1699
step 44390; step_loss: 0.1744
step 44400; step_loss: 0.1497
step 44410; step_loss: 0.1957
step 44420; step_loss: 0.1539
step 44430; step_loss: 0.1458
step 44440; step_loss: 0.1656
step 44450; step_loss: 0.1714
step 44460; step_loss: 0.1240
step 44470; step_loss: 0.2070
step 44480; step_loss: 0.1665
step 44490; step_loss: 0.1428
step 44500; step_loss: 0.1550
step 44510; step_loss: 0.1454
step 44520; step_loss: 0.1521
step 44530; step_loss: 0.1619
step 44540; step_loss: 0.1310
step 44550; step_loss: 0.1581
step 44560; step_loss: 0.2009
step 44570; step_loss: 0.1457
step 44580; step_loss: 0.1682
step 44590; step_loss: 0.1801
step 44600; step_loss: 0.1122
step 44610; step_loss: 0.1062
step 44620; step_loss: 0.1841
step 44630; step_loss: 0.1275
step 44640; step_loss: 0.1323
step 44650; step_loss: 0.3640
step 44660; step_loss: 0.1698
step 44670; step_loss: 0.1155
step 44680; step_loss: 0.1558
step 44690; step_loss: 0.1472
step 44700; step_loss: 0.1510
step 44710; step_loss: 0.1301
step 44720; step_loss: 0.1651
step 44730; step_loss: 0.1128
step 44740; step_loss: 0.1932
step 44750; step_loss: 0.0970
step 44760; step_loss: 0.1816
step 44770; step_loss: 0.1639
step 44780; step_loss: 0.1168
step 44790; step_loss: 0.1619
step 44800; step_loss: 0.1536
step 44810; step_loss: 0.1615
step 44820; step_loss: 0.1638
step 44830; step_loss: 0.1371
step 44840; step_loss: 0.1485
step 44850; step_loss: 0.1700
step 44860; step_loss: 0.1887
step 44870; step_loss: 0.1579
step 44880; step_loss: 0.1227
step 44890; step_loss: 0.1918
step 44900; step_loss: 0.1067
step 44910; step_loss: 0.1447
step 44920; step_loss: 0.1395
step 44930; step_loss: 0.1737
step 44940; step_loss: 0.1134
step 44950; step_loss: 0.1614
step 44960; step_loss: 0.1103
step 44970; step_loss: 0.1807
step 44980; step_loss: 0.1506
step 44990; step_loss: 0.1568

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 1.019 | 1.132 | 1.340 | 1.422 | 1.568 | 1.824 |

============================
Global step:         45000
Learning rate:       0.0041
Step-time (ms):     35.9013
Train loss avg:      0.1620
--------------------------
Val loss:            0.8398
srnn loss:           0.7083
============================

Saving the model...
done in 390.89 ms
step 45000; step_loss: 0.1658
step 45010; step_loss: 0.1538
step 45020; step_loss: 0.1456
step 45030; step_loss: 0.1879
step 45040; step_loss: 0.1750
step 45050; step_loss: 0.1977
step 45060; step_loss: 0.1900
step 45070; step_loss: 0.1955
step 45080; step_loss: 0.1982
step 45090; step_loss: 0.1648
step 45100; step_loss: 0.1552
step 45110; step_loss: 0.2139
step 45120; step_loss: 0.1689
step 45130; step_loss: 0.1512
step 45140; step_loss: 0.1697
step 45150; step_loss: 0.1340
step 45160; step_loss: 0.1475
step 45170; step_loss: 0.2141
step 45180; step_loss: 0.1523
step 45190; step_loss: 0.2230
step 45200; step_loss: 0.1302
step 45210; step_loss: 0.1638
step 45220; step_loss: 0.1322
step 45230; step_loss: 0.1242
step 45240; step_loss: 0.2400
step 45250; step_loss: 0.1714
step 45260; step_loss: 0.1788
step 45270; step_loss: 0.1290
step 45280; step_loss: 0.1770
step 45290; step_loss: 0.1305
step 45300; step_loss: 0.1595
step 45310; step_loss: 0.2706
step 45320; step_loss: 0.1983
step 45330; step_loss: 0.1329
step 45340; step_loss: 0.1429
step 45350; step_loss: 0.1500
step 45360; step_loss: 0.1488
step 45370; step_loss: 0.1641
step 45380; step_loss: 0.1377
step 45390; step_loss: 0.1254
step 45400; step_loss: 0.1095
step 45410; step_loss: 0.1325
step 45420; step_loss: 0.1975
step 45430; step_loss: 0.1382
step 45440; step_loss: 0.2497
step 45450; step_loss: 0.2014
step 45460; step_loss: 0.2546
step 45470; step_loss: 0.1825
step 45480; step_loss: 0.2437
step 45490; step_loss: 0.1340
step 45500; step_loss: 0.1615
step 45510; step_loss: 0.1376
step 45520; step_loss: 0.1627
step 45530; step_loss: 0.1496
step 45540; step_loss: 0.1599
step 45550; step_loss: 0.1816
step 45560; step_loss: 0.1686
step 45570; step_loss: 0.2315
step 45580; step_loss: 0.1224
step 45590; step_loss: 0.1526
step 45600; step_loss: 0.1408
step 45610; step_loss: 0.1612
step 45620; step_loss: 0.1983
step 45630; step_loss: 0.1819
step 45640; step_loss: 0.1715
step 45650; step_loss: 0.1724
step 45660; step_loss: 0.1211
step 45670; step_loss: 0.1883
step 45680; step_loss: 0.1998
step 45690; step_loss: 0.1691
step 45700; step_loss: 0.1461
step 45710; step_loss: 0.1260
step 45720; step_loss: 0.1022
step 45730; step_loss: 0.1921
step 45740; step_loss: 0.1602
step 45750; step_loss: 0.1510
step 45760; step_loss: 0.1020
step 45770; step_loss: 0.1493
step 45780; step_loss: 0.1773
step 45790; step_loss: 0.1652
step 45800; step_loss: 0.1474
step 45810; step_loss: 0.1531
step 45820; step_loss: 0.1711
step 45830; step_loss: 0.1541
step 45840; step_loss: 0.2190
step 45850; step_loss: 0.1781
step 45860; step_loss: 0.1578
step 45870; step_loss: 0.1994
step 45880; step_loss: 0.1733
step 45890; step_loss: 0.2104
step 45900; step_loss: 0.1756
step 45910; step_loss: 0.1965
step 45920; step_loss: 0.1577
step 45930; step_loss: 0.1941
step 45940; step_loss: 0.2042
step 45950; step_loss: 0.1241
step 45960; step_loss: 0.1174
step 45970; step_loss: 0.1318
step 45980; step_loss: 0.1269
step 45990; step_loss: 0.1797

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 1.015 | 1.128 | 1.337 | 1.419 | 1.565 | 1.820 |

============================
Global step:         46000
Learning rate:       0.0041
Step-time (ms):     35.8743
Train loss avg:      0.1622
--------------------------
Val loss:            0.7865
srnn loss:           0.7025
============================

Saving the model...
done in 424.35 ms
step 46000; step_loss: 0.1228
step 46010; step_loss: 0.1085
step 46020; step_loss: 0.1685
step 46030; step_loss: 0.1551
step 46040; step_loss: 0.2216
step 46050; step_loss: 0.1840
step 46060; step_loss: 0.1546
step 46070; step_loss: 0.1350
step 46080; step_loss: 0.1561
step 46090; step_loss: 0.1061
step 46100; step_loss: 0.1626
step 46110; step_loss: 0.1306
step 46120; step_loss: 0.1540
step 46130; step_loss: 0.1289
step 46140; step_loss: 0.1347
step 46150; step_loss: 0.1429
step 46160; step_loss: 0.1431
step 46170; step_loss: 0.1392
step 46180; step_loss: 0.1689
step 46190; step_loss: 0.1596
step 46200; step_loss: 0.1554
step 46210; step_loss: 0.1941
step 46220; step_loss: 0.1845
step 46230; step_loss: 0.1762
step 46240; step_loss: 0.1266
step 46250; step_loss: 0.1399
step 46260; step_loss: 0.1672
step 46270; step_loss: 0.1271
step 46280; step_loss: 0.1698
step 46290; step_loss: 0.1337
step 46300; step_loss: 0.1952
step 46310; step_loss: 0.1826
step 46320; step_loss: 0.1192
step 46330; step_loss: 0.1503
step 46340; step_loss: 0.2230
step 46350; step_loss: 0.1418
step 46360; step_loss: 0.1505
step 46370; step_loss: 0.1690
step 46380; step_loss: 0.1714
step 46390; step_loss: 0.1762
step 46400; step_loss: 0.1700
step 46410; step_loss: 0.1364
step 46420; step_loss: 0.1851
step 46430; step_loss: 0.1397
step 46440; step_loss: 0.1391
step 46450; step_loss: 0.1920
step 46460; step_loss: 0.1496
step 46470; step_loss: 0.1165
step 46480; step_loss: 0.1508
step 46490; step_loss: 0.1814
step 46500; step_loss: 0.1798
step 46510; step_loss: 0.1253
step 46520; step_loss: 0.2280
step 46530; step_loss: 0.1716
step 46540; step_loss: 0.1610
step 46550; step_loss: 0.1381
step 46560; step_loss: 0.1909
step 46570; step_loss: 0.1570
step 46580; step_loss: 0.2111
step 46590; step_loss: 0.1669
step 46600; step_loss: 0.1624
step 46610; step_loss: 0.1065
step 46620; step_loss: 0.2580
step 46630; step_loss: 0.1770
step 46640; step_loss: 0.1742
step 46650; step_loss: 0.1807
step 46660; step_loss: 0.1453
step 46670; step_loss: 0.2236
step 46680; step_loss: 0.1671
step 46690; step_loss: 0.2800
step 46700; step_loss: 0.1306
step 46710; step_loss: 0.1550
step 46720; step_loss: 0.1652
step 46730; step_loss: 0.1417
step 46740; step_loss: 0.1637
step 46750; step_loss: 0.1497
step 46760; step_loss: 0.1586
step 46770; step_loss: 0.1636
step 46780; step_loss: 0.1691
step 46790; step_loss: 0.2120
step 46800; step_loss: 0.1504
step 46810; step_loss: 0.1471
step 46820; step_loss: 0.1472
step 46830; step_loss: 0.1671
step 46840; step_loss: 0.1476
step 46850; step_loss: 0.1342
step 46860; step_loss: 0.1573
step 46870; step_loss: 0.1636
step 46880; step_loss: 0.1630
step 46890; step_loss: 0.1424
step 46900; step_loss: 0.1779
step 46910; step_loss: 0.1354
step 46920; step_loss: 0.1706
step 46930; step_loss: 0.1579
step 46940; step_loss: 0.1487
step 46950; step_loss: 0.1445
step 46960; step_loss: 0.1988
step 46970; step_loss: 0.1504
step 46980; step_loss: 0.1161
step 46990; step_loss: 0.1286

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 1.017 | 1.135 | 1.351 | 1.435 | 1.580 | 1.814 |

============================
Global step:         47000
Learning rate:       0.0041
Step-time (ms):     35.9675
Train loss avg:      0.1599
--------------------------
Val loss:            0.8041
srnn loss:           0.7143
============================

Saving the model...
done in 409.74 ms
step 47000; step_loss: 0.1298
step 47010; step_loss: 0.1600
step 47020; step_loss: 0.1471
step 47030; step_loss: 0.1576
step 47040; step_loss: 0.1845
step 47050; step_loss: 0.2166
step 47060; step_loss: 0.1516
step 47070; step_loss: 0.1734
step 47080; step_loss: 0.1494
step 47090; step_loss: 0.1438
step 47100; step_loss: 0.1748
step 47110; step_loss: 0.1062
step 47120; step_loss: 0.2338
step 47130; step_loss: 0.1252
step 47140; step_loss: 0.1635
step 47150; step_loss: 0.1285
step 47160; step_loss: 0.1292
step 47170; step_loss: 0.1306
step 47180; step_loss: 0.1510
step 47190; step_loss: 0.1186
step 47200; step_loss: 0.1426
step 47210; step_loss: 0.1747
step 47220; step_loss: 0.1372
step 47230; step_loss: 0.1418
step 47240; step_loss: 0.1557
step 47250; step_loss: 0.1411
step 47260; step_loss: 0.1622
step 47270; step_loss: 0.1888
step 47280; step_loss: 0.1927
step 47290; step_loss: 0.1310
step 47300; step_loss: 0.1347
step 47310; step_loss: 0.1604
step 47320; step_loss: 0.1473
step 47330; step_loss: 0.1092
step 47340; step_loss: 0.2047
step 47350; step_loss: 0.1635
step 47360; step_loss: 0.1986
step 47370; step_loss: 0.1607
step 47380; step_loss: 0.1508
step 47390; step_loss: 0.1183
step 47400; step_loss: 0.1703
step 47410; step_loss: 0.1298
step 47420; step_loss: 0.2154
step 47430; step_loss: 0.2124
step 47440; step_loss: 0.2375
step 47450; step_loss: 0.1232
step 47460; step_loss: 0.1181
step 47470; step_loss: 0.1637
step 47480; step_loss: 0.1325
step 47490; step_loss: 0.1669
step 47500; step_loss: 0.1597
step 47510; step_loss: 0.2340
step 47520; step_loss: 0.1558
step 47530; step_loss: 0.1919
step 47540; step_loss: 0.1759
step 47550; step_loss: 0.1387
step 47560; step_loss: 0.1730
step 47570; step_loss: 0.1761
step 47580; step_loss: 0.1597
step 47590; step_loss: 0.1367
step 47600; step_loss: 0.2160
step 47610; step_loss: 0.1452
step 47620; step_loss: 0.1680
step 47630; step_loss: 0.1755
step 47640; step_loss: 0.1420
step 47650; step_loss: 0.1739
step 47660; step_loss: 0.1612
step 47670; step_loss: 0.1553
step 47680; step_loss: 0.1269
step 47690; step_loss: 0.1650
step 47700; step_loss: 0.1332
step 47710; step_loss: 0.2164
step 47720; step_loss: 0.1546
step 47730; step_loss: 0.1450
step 47740; step_loss: 0.1869
step 47750; step_loss: 0.1508
step 47760; step_loss: 0.1355
step 47770; step_loss: 0.1879
step 47780; step_loss: 0.1819
step 47790; step_loss: 0.1506
step 47800; step_loss: 0.1920
step 47810; step_loss: 0.1450
step 47820; step_loss: 0.1391
step 47830; step_loss: 0.1848
step 47840; step_loss: 0.1067
step 47850; step_loss: 0.2094
step 47860; step_loss: 0.1358
step 47870; step_loss: 0.1892
step 47880; step_loss: 0.1119
step 47890; step_loss: 0.1531
step 47900; step_loss: 0.1526
step 47910; step_loss: 0.1952
step 47920; step_loss: 0.1572
step 47930; step_loss: 0.1439
step 47940; step_loss: 0.1584
step 47950; step_loss: 0.3065
step 47960; step_loss: 0.1298
step 47970; step_loss: 0.2134
step 47980; step_loss: 0.1506
step 47990; step_loss: 0.1327

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 1.022 | 1.143 | 1.369 | 1.457 | 1.609 | 1.857 |

============================
Global step:         48000
Learning rate:       0.0041
Step-time (ms):     35.9765
Train loss avg:      0.1582
--------------------------
Val loss:            0.8076
srnn loss:           0.7209
============================

Saving the model...
done in 400.64 ms
step 48000; step_loss: 0.2546
step 48010; step_loss: 0.2139
step 48020; step_loss: 0.1417
step 48030; step_loss: 0.1401
step 48040; step_loss: 0.1401
step 48050; step_loss: 0.1515
step 48060; step_loss: 0.1167
step 48070; step_loss: 0.1859
step 48080; step_loss: 0.1550
step 48090; step_loss: 0.1622
step 48100; step_loss: 0.1824
step 48110; step_loss: 0.1118
step 48120; step_loss: 0.1659
step 48130; step_loss: 0.1871
step 48140; step_loss: 0.1498
step 48150; step_loss: 0.1599
step 48160; step_loss: 0.0976
step 48170; step_loss: 0.1827
step 48180; step_loss: 0.1525
step 48190; step_loss: 0.1978
step 48200; step_loss: 0.1442
step 48210; step_loss: 0.1285
step 48220; step_loss: 0.1361
step 48230; step_loss: 0.1410
step 48240; step_loss: 0.1263
step 48250; step_loss: 0.1793
step 48260; step_loss: 0.1240
step 48270; step_loss: 0.1290
step 48280; step_loss: 0.1561
step 48290; step_loss: 0.1855
step 48300; step_loss: 0.1277
step 48310; step_loss: 0.1480
step 48320; step_loss: 0.1050
step 48330; step_loss: 0.1485
step 48340; step_loss: 0.1257
step 48350; step_loss: 0.1459
step 48360; step_loss: 0.1844
step 48370; step_loss: 0.1546
step 48380; step_loss: 0.1117
step 48390; step_loss: 0.1346
step 48400; step_loss: 0.1731
step 48410; step_loss: 0.1535
step 48420; step_loss: 0.1665
step 48430; step_loss: 0.1473
step 48440; step_loss: 0.1644
step 48450; step_loss: 0.1580
step 48460; step_loss: 0.1588
step 48470; step_loss: 0.1540
step 48480; step_loss: 0.1488
step 48490; step_loss: 0.1542
step 48500; step_loss: 0.1327
step 48510; step_loss: 0.1007
step 48520; step_loss: 0.1811
step 48530; step_loss: 0.1356
step 48540; step_loss: 0.1561
step 48550; step_loss: 0.2096
step 48560; step_loss: 0.1285
step 48570; step_loss: 0.1644
step 48580; step_loss: 0.1565
step 48590; step_loss: 0.1323
step 48600; step_loss: 0.1379
step 48610; step_loss: 0.1881
step 48620; step_loss: 0.1332
step 48630; step_loss: 0.1506
step 48640; step_loss: 0.1140
step 48650; step_loss: 0.1242
step 48660; step_loss: 0.1442
step 48670; step_loss: 0.2404
step 48680; step_loss: 0.1702
step 48690; step_loss: 0.1405
step 48700; step_loss: 0.1586
step 48710; step_loss: 0.1406
step 48720; step_loss: 0.1601
step 48730; step_loss: 0.1448
step 48740; step_loss: 0.2528
step 48750; step_loss: 0.1511
step 48760; step_loss: 0.2236
step 48770; step_loss: 0.1829
step 48780; step_loss: 0.1489
step 48790; step_loss: 0.1471
step 48800; step_loss: 0.1917
step 48810; step_loss: 0.1356
step 48820; step_loss: 0.2607
step 48830; step_loss: 0.1943
step 48840; step_loss: 0.1635
step 48850; step_loss: 0.1393
step 48860; step_loss: 0.1880
step 48870; step_loss: 0.1543
step 48880; step_loss: 0.1487
step 48890; step_loss: 0.1954
step 48900; step_loss: 0.1737
step 48910; step_loss: 0.2097
step 48920; step_loss: 0.1205
step 48930; step_loss: 0.1440
step 48940; step_loss: 0.1271
step 48950; step_loss: 0.1310
step 48960; step_loss: 0.1328
step 48970; step_loss: 0.1435
step 48980; step_loss: 0.1272
step 48990; step_loss: 0.1337

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 1.011 | 1.132 | 1.358 | 1.447 | 1.598 | 1.863 |

============================
Global step:         49000
Learning rate:       0.0041
Step-time (ms):     35.9681
Train loss avg:      0.1583
--------------------------
Val loss:            0.7916
srnn loss:           0.7214
============================

Saving the model...
done in 421.46 ms
step 49000; step_loss: 0.1667
step 49010; step_loss: 0.1328
step 49020; step_loss: 0.1587
step 49030; step_loss: 0.1392
step 49040; step_loss: 0.1590
step 49050; step_loss: 0.2155
step 49060; step_loss: 0.1229
step 49070; step_loss: 0.1045
step 49080; step_loss: 0.1369
step 49090; step_loss: 0.1516
step 49100; step_loss: 0.1863
step 49110; step_loss: 0.1799
step 49120; step_loss: 0.1114
step 49130; step_loss: 0.1266
step 49140; step_loss: 0.1302
step 49150; step_loss: 0.1588
step 49160; step_loss: 0.1445
step 49170; step_loss: 0.1595
step 49180; step_loss: 0.1864
step 49190; step_loss: 0.1596
step 49200; step_loss: 0.1635
step 49210; step_loss: 0.1665
step 49220; step_loss: 0.1464
step 49230; step_loss: 0.1612
step 49240; step_loss: 0.1577
step 49250; step_loss: 0.1802
step 49260; step_loss: 0.1390
step 49270; step_loss: 0.1922
step 49280; step_loss: 0.1392
step 49290; step_loss: 0.1210
step 49300; step_loss: 0.1336
step 49310; step_loss: 0.1602
step 49320; step_loss: 0.1932
step 49330; step_loss: 0.2318
step 49340; step_loss: 0.1885
step 49350; step_loss: 0.1659
step 49360; step_loss: 0.2201
step 49370; step_loss: 0.2012
step 49380; step_loss: 0.1183
step 49390; step_loss: 0.1397
step 49400; step_loss: 0.0994
step 49410; step_loss: 0.1425
step 49420; step_loss: 0.1583
step 49430; step_loss: 0.1261
step 49440; step_loss: 0.1108
step 49450; step_loss: 0.1240
step 49460; step_loss: 0.1139
step 49470; step_loss: 0.1597
step 49480; step_loss: 0.1387
step 49490; step_loss: 0.1626
step 49500; step_loss: 0.1093
step 49510; step_loss: 0.1959
step 49520; step_loss: 0.1553
step 49530; step_loss: 0.1341
step 49540; step_loss: 0.0988
step 49550; step_loss: 0.1865
step 49560; step_loss: 0.1865
step 49570; step_loss: 0.1595
step 49580; step_loss: 0.1818
step 49590; step_loss: 0.1383
step 49600; step_loss: 0.1397
step 49610; step_loss: 0.2316
step 49620; step_loss: 0.1357
step 49630; step_loss: 0.1559
step 49640; step_loss: 0.1617
step 49650; step_loss: 0.1491
step 49660; step_loss: 0.1437
step 49670; step_loss: 0.1192
step 49680; step_loss: 0.1235
step 49690; step_loss: 0.1385
step 49700; step_loss: 0.2010
step 49710; step_loss: 0.1693
step 49720; step_loss: 0.1917
step 49730; step_loss: 0.2089
step 49740; step_loss: 0.1843
step 49750; step_loss: 0.2639
step 49760; step_loss: 0.1302
step 49770; step_loss: 0.2444
step 49780; step_loss: 0.1585
step 49790; step_loss: 0.1734
step 49800; step_loss: 0.1353
step 49810; step_loss: 0.2193
step 49820; step_loss: 0.1962
step 49830; step_loss: 0.1323
step 49840; step_loss: 0.1578
step 49850; step_loss: 0.1494
step 49860; step_loss: 0.1281
step 49870; step_loss: 0.2053
step 49880; step_loss: 0.1321
step 49890; step_loss: 0.1412
step 49900; step_loss: 0.2051
step 49910; step_loss: 0.1689
step 49920; step_loss: 0.1905
step 49930; step_loss: 0.1067
step 49940; step_loss: 0.1449
step 49950; step_loss: 0.1780
step 49960; step_loss: 0.1446
step 49970; step_loss: 0.1261
step 49980; step_loss: 0.1594
step 49990; step_loss: 0.1808

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 1.015 | 1.141 | 1.380 | 1.475 | 1.636 | 1.894 |

============================
Global step:         50000
Learning rate:       0.0039
Step-time (ms):     35.8620
Train loss avg:      0.1563
--------------------------
Val loss:            0.7787
srnn loss:           0.7364
============================

Saving the model...
done in 411.59 ms
step 50000; step_loss: 0.1671
step 50010; step_loss: 0.2171
step 50020; step_loss: 0.1495
step 50030; step_loss: 0.1372
step 50040; step_loss: 0.1161
step 50050; step_loss: 0.1752
step 50060; step_loss: 0.1380
step 50070; step_loss: 0.1385
step 50080; step_loss: 0.1549
step 50090; step_loss: 0.2057
step 50100; step_loss: 0.1568
step 50110; step_loss: 0.1551
step 50120; step_loss: 0.1603
step 50130; step_loss: 0.1208
step 50140; step_loss: 0.1410
step 50150; step_loss: 0.1533
step 50160; step_loss: 0.1267
step 50170; step_loss: 0.1866
step 50180; step_loss: 0.1922
step 50190; step_loss: 0.1516
step 50200; step_loss: 0.1301
step 50210; step_loss: 0.1138
step 50220; step_loss: 0.2025
step 50230; step_loss: 0.1338
step 50240; step_loss: 0.1454
step 50250; step_loss: 0.1694
step 50260; step_loss: 0.1522
step 50270; step_loss: 0.1199
step 50280; step_loss: 0.1308
step 50290; step_loss: 0.1360
step 50300; step_loss: 0.1564
step 50310; step_loss: 0.1398
step 50320; step_loss: 0.1609
step 50330; step_loss: 0.1497
step 50340; step_loss: 0.1987
step 50350; step_loss: 0.1760
step 50360; step_loss: 0.1231
step 50370; step_loss: 0.1331
step 50380; step_loss: 0.1375
step 50390; step_loss: 0.1722
step 50400; step_loss: 0.1680
step 50410; step_loss: 0.1579
step 50420; step_loss: 0.1627
step 50430; step_loss: 0.1505
step 50440; step_loss: 0.1495
step 50450; step_loss: 0.1467
step 50460; step_loss: 0.1482
step 50470; step_loss: 0.1497
step 50480; step_loss: 0.1440
step 50490; step_loss: 0.1566
step 50500; step_loss: 0.1561
step 50510; step_loss: 0.1491
step 50520; step_loss: 0.1567
step 50530; step_loss: 0.1748
step 50540; step_loss: 0.1818
step 50550; step_loss: 0.1334
step 50560; step_loss: 0.2196
step 50570; step_loss: 0.1652
step 50580; step_loss: 0.1530
step 50590; step_loss: 0.1358
step 50600; step_loss: 0.1169
step 50610; step_loss: 0.1567
step 50620; step_loss: 0.1655
step 50630; step_loss: 0.1946
step 50640; step_loss: 0.1959
step 50650; step_loss: 0.1271
step 50660; step_loss: 0.1532
step 50670; step_loss: 0.1063
step 50680; step_loss: 0.1406
step 50690; step_loss: 0.1383
step 50700; step_loss: 0.1077
step 50710; step_loss: 0.1622
step 50720; step_loss: 0.1701
step 50730; step_loss: 0.1807
step 50740; step_loss: 0.1362
step 50750; step_loss: 0.0980
step 50760; step_loss: 0.1454
step 50770; step_loss: 0.1261
step 50780; step_loss: 0.1845
step 50790; step_loss: 0.1725
step 50800; step_loss: 0.2023
step 50810; step_loss: 0.1287
step 50820; step_loss: 0.1690
step 50830; step_loss: 0.1399
step 50840; step_loss: 0.1590
step 50850; step_loss: 0.1856
step 50860; step_loss: 0.1417
step 50870; step_loss: 0.1749
step 50880; step_loss: 0.1151
step 50890; step_loss: 0.1532
step 50900; step_loss: 0.1750
step 50910; step_loss: 0.1348
step 50920; step_loss: 0.1607
step 50930; step_loss: 0.1357
step 50940; step_loss: 0.1696
step 50950; step_loss: 0.1836
step 50960; step_loss: 0.1743
step 50970; step_loss: 0.1340
step 50980; step_loss: 0.1211
step 50990; step_loss: 0.1390

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 1.010 | 1.134 | 1.371 | 1.465 | 1.624 | 1.877 |

============================
Global step:         51000
Learning rate:       0.0039
Step-time (ms):     35.8650
Train loss avg:      0.1541
--------------------------
Val loss:            0.8244
srnn loss:           0.7275
============================

Saving the model...
done in 410.06 ms
step 51000; step_loss: 0.1578
step 51010; step_loss: 0.1189
step 51020; step_loss: 0.1343
step 51030; step_loss: 0.1531
step 51040; step_loss: 0.1243
step 51050; step_loss: 0.1334
step 51060; step_loss: 0.1521
step 51070; step_loss: 0.1149
step 51080; step_loss: 0.1826
step 51090; step_loss: 0.1352
step 51100; step_loss: 0.1411
step 51110; step_loss: 0.1682
step 51120; step_loss: 0.1270
step 51130; step_loss: 0.1755
step 51140; step_loss: 0.1619
step 51150; step_loss: 0.1725
step 51160; step_loss: 0.1806
step 51170; step_loss: 0.1571
step 51180; step_loss: 0.1178
step 51190; step_loss: 0.1067
step 51200; step_loss: 0.0927
step 51210; step_loss: 0.1710
step 51220; step_loss: 0.1654
step 51230; step_loss: 0.1713
step 51240; step_loss: 0.1756
step 51250; step_loss: 0.1246
step 51260; step_loss: 0.1370
step 51270; step_loss: 0.1625
step 51280; step_loss: 0.1431
step 51290; step_loss: 0.1327
step 51300; step_loss: 0.1565
step 51310; step_loss: 0.1425
step 51320; step_loss: 0.1033
step 51330; step_loss: 0.1900
step 51340; step_loss: 0.2086
step 51350; step_loss: 0.1139
step 51360; step_loss: 0.1666
step 51370; step_loss: 0.1599
step 51380; step_loss: 0.1592
step 51390; step_loss: 0.1547
step 51400; step_loss: 0.1915
step 51410; step_loss: 0.1693
step 51420; step_loss: 0.1150
step 51430; step_loss: 0.1562
step 51440; step_loss: 0.1462
step 51450; step_loss: 0.2020
step 51460; step_loss: 0.1185
step 51470; step_loss: 0.1208
step 51480; step_loss: 0.1406
step 51490; step_loss: 0.2235
step 51500; step_loss: 0.1423
step 51510; step_loss: 0.1383
step 51520; step_loss: 0.1559
step 51530; step_loss: 0.1704
step 51540; step_loss: 0.1902
step 51550; step_loss: 0.1682
step 51560; step_loss: 0.2527
step 51570; step_loss: 0.1597
step 51580; step_loss: 0.1371
step 51590; step_loss: 0.1207
step 51600; step_loss: 0.1996
step 51610; step_loss: 0.1644
step 51620; step_loss: 0.1254
step 51630; step_loss: 0.1247
step 51640; step_loss: 0.1632
step 51650; step_loss: 0.1151
step 51660; step_loss: 0.1523
step 51670; step_loss: 0.2866
step 51680; step_loss: 0.1130
step 51690; step_loss: 0.1685
step 51700; step_loss: 0.2195
step 51710; step_loss: 0.1153
step 51720; step_loss: 0.1035
step 51730; step_loss: 0.1790
step 51740; step_loss: 0.1761
step 51750; step_loss: 0.1378
step 51760; step_loss: 0.1291
step 51770; step_loss: 0.1427
step 51780; step_loss: 0.1754
step 51790; step_loss: 0.1591
step 51800; step_loss: 0.1263
step 51810; step_loss: 0.1470
step 51820; step_loss: 0.1665
step 51830; step_loss: 0.1278
step 51840; step_loss: 0.1425
step 51850; step_loss: 0.2076
step 51860; step_loss: 0.1385
step 51870; step_loss: 0.2113
step 51880; step_loss: 0.1985
step 51890; step_loss: 0.1480
step 51900; step_loss: 0.1258
step 51910; step_loss: 0.1844
step 51920; step_loss: 0.2063
step 51930; step_loss: 0.1851
step 51940; step_loss: 0.1615
step 51950; step_loss: 0.1241
step 51960; step_loss: 0.1172
step 51970; step_loss: 0.1317
step 51980; step_loss: 0.1280
step 51990; step_loss: 0.1699

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 1.009 | 1.134 | 1.366 | 1.456 | 1.613 | 1.864 |

============================
Global step:         52000
Learning rate:       0.0039
Step-time (ms):     35.9392
Train loss avg:      0.1527
--------------------------
Val loss:            0.9570
srnn loss:           0.7265
============================

Saving the model...
done in 401.20 ms
step 52000; step_loss: 0.2344
step 52010; step_loss: 0.1795
step 52020; step_loss: 0.1431
step 52030; step_loss: 0.1598
step 52040; step_loss: 0.1242
step 52050; step_loss: 0.1446
step 52060; step_loss: 0.1582
step 52070; step_loss: 0.1161
step 52080; step_loss: 0.1732
step 52090; step_loss: 0.1343
step 52100; step_loss: 0.1757
step 52110; step_loss: 0.1658
step 52120; step_loss: 0.1599
step 52130; step_loss: 0.1510
step 52140; step_loss: 0.1423
step 52150; step_loss: 0.1491
step 52160; step_loss: 0.1514
step 52170; step_loss: 0.1656
step 52180; step_loss: 0.1096
step 52190; step_loss: 0.1330
step 52200; step_loss: 0.1552
step 52210; step_loss: 0.1187
step 52220; step_loss: 0.1959
step 52230; step_loss: 0.1555
step 52240; step_loss: 0.1401
step 52250; step_loss: 0.1523
step 52260; step_loss: 0.1787
step 52270; step_loss: 0.1564
step 52280; step_loss: 0.1528
step 52290; step_loss: 0.1308
step 52300; step_loss: 0.1706
step 52310; step_loss: 0.1270
step 52320; step_loss: 0.1324
step 52330; step_loss: 0.1456
step 52340; step_loss: 0.1311
step 52350; step_loss: 0.1491
step 52360; step_loss: 0.1509
step 52370; step_loss: 0.1310
step 52380; step_loss: 0.1318
step 52390; step_loss: 0.1870
step 52400; step_loss: 0.1508
step 52410; step_loss: 0.1205
step 52420; step_loss: 0.1215
step 52430; step_loss: 0.1408
step 52440; step_loss: 0.1962
step 52450; step_loss: 0.1619
step 52460; step_loss: 0.1355
step 52470; step_loss: 0.1318
step 52480; step_loss: 0.2711
step 52490; step_loss: 0.1327
step 52500; step_loss: 0.1380
step 52510; step_loss: 0.1602
step 52520; step_loss: 0.1565
step 52530; step_loss: 0.1689
step 52540; step_loss: 0.1308
step 52550; step_loss: 0.1644
step 52560; step_loss: 0.1614
step 52570; step_loss: 0.1662
step 52580; step_loss: 0.1755
step 52590; step_loss: 0.1360
step 52600; step_loss: 0.1030
step 52610; step_loss: 0.2020
step 52620; step_loss: 0.1177
step 52630; step_loss: 0.1151
step 52640; step_loss: 0.1315
step 52650; step_loss: 0.1948
step 52660; step_loss: 0.1021
step 52670; step_loss: 0.1227
step 52680; step_loss: 0.1518
step 52690; step_loss: 0.1469
step 52700; step_loss: 0.1704
step 52710; step_loss: 0.1623
step 52720; step_loss: 0.1587
step 52730; step_loss: 0.1916
step 52740; step_loss: 0.1413
step 52750; step_loss: 0.1477
step 52760; step_loss: 0.1494
step 52770; step_loss: 0.1996
step 52780; step_loss: 0.1974
step 52790; step_loss: 0.1951
step 52800; step_loss: 0.1543
step 52810; step_loss: 0.1373
step 52820; step_loss: 0.0981
step 52830; step_loss: 0.2157
step 52840; step_loss: 0.1894
step 52850; step_loss: 0.1211
step 52860; step_loss: 0.1868
step 52870; step_loss: 0.1901
step 52880; step_loss: 0.1380
step 52890; step_loss: 0.1446
step 52900; step_loss: 0.1442
step 52910; step_loss: 0.1380
step 52920; step_loss: 0.1341
step 52930; step_loss: 0.1117
step 52940; step_loss: 0.1719
step 52950; step_loss: 0.1671
step 52960; step_loss: 0.1768
step 52970; step_loss: 0.1167
step 52980; step_loss: 0.1482
step 52990; step_loss: 0.1463

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 1.012 | 1.143 | 1.388 | 1.485 | 1.651 | 1.916 |

============================
Global step:         53000
Learning rate:       0.0039
Step-time (ms):     35.9028
Train loss avg:      0.1499
--------------------------
Val loss:            0.7909
srnn loss:           0.7430
============================

Saving the model...
done in 410.23 ms
step 53000; step_loss: 0.1591
step 53010; step_loss: 0.0776
step 53020; step_loss: 0.1283
step 53030; step_loss: 0.1230
step 53040; step_loss: 0.1283
step 53050; step_loss: 0.1186
step 53060; step_loss: 0.1689
step 53070; step_loss: 0.1507
step 53080; step_loss: 0.1876
step 53090; step_loss: 0.1170
step 53100; step_loss: 0.1911
step 53110; step_loss: 0.1080
step 53120; step_loss: 0.1253
step 53130; step_loss: 0.1703
step 53140; step_loss: 0.1451
step 53150; step_loss: 0.2204
step 53160; step_loss: 0.1597
step 53170; step_loss: 0.1615
step 53180; step_loss: 0.1287
step 53190; step_loss: 0.1564
step 53200; step_loss: 0.1430
step 53210; step_loss: 0.1437
step 53220; step_loss: 0.1329
step 53230; step_loss: 0.1224
step 53240; step_loss: 0.1560
step 53250; step_loss: 0.1396
step 53260; step_loss: 0.1259
step 53270; step_loss: 0.1689
step 53280; step_loss: 0.1723
step 53290; step_loss: 0.1089
step 53300; step_loss: 0.1562
step 53310; step_loss: 0.1795
step 53320; step_loss: 0.1510
step 53330; step_loss: 0.1738
step 53340; step_loss: 0.1238
step 53350; step_loss: 0.1123
step 53360; step_loss: 0.1662
step 53370; step_loss: 0.1677
step 53380; step_loss: 0.1474
step 53390; step_loss: 0.2265
step 53400; step_loss: 0.1456
step 53410; step_loss: 0.1245
step 53420; step_loss: 0.1916
step 53430; step_loss: 0.1746
step 53440; step_loss: 0.1676
step 53450; step_loss: 0.1458
step 53460; step_loss: 0.1875
step 53470; step_loss: 0.1497
step 53480; step_loss: 0.1172
step 53490; step_loss: 0.1525
step 53500; step_loss: 0.1485
step 53510; step_loss: 0.1126
step 53520; step_loss: 0.1956
step 53530; step_loss: 0.1201
step 53540; step_loss: 0.1671
step 53550; step_loss: 0.1047
step 53560; step_loss: 0.1618
step 53570; step_loss: 0.1585
step 53580; step_loss: 0.1274
step 53590; step_loss: 0.1281
step 53600; step_loss: 0.1455
step 53610; step_loss: 0.1593
step 53620; step_loss: 0.1582
step 53630; step_loss: 0.1421
step 53640; step_loss: 0.1263
step 53650; step_loss: 0.1529
step 53660; step_loss: 0.2152
step 53670; step_loss: 0.2030
step 53680; step_loss: 0.1800
step 53690; step_loss: 0.1311
step 53700; step_loss: 0.1775
step 53710; step_loss: 0.1752
step 53720; step_loss: 0.1586
step 53730; step_loss: 0.1388
step 53740; step_loss: 0.1245
step 53750; step_loss: 0.1661
step 53760; step_loss: 0.1768
step 53770; step_loss: 0.1702
step 53780; step_loss: 0.0980
step 53790; step_loss: 0.1196
step 53800; step_loss: 0.1440
step 53810; step_loss: 0.1006
step 53820; step_loss: 0.1058
step 53830; step_loss: 0.1318
step 53840; step_loss: 0.1067
step 53850; step_loss: 0.1783
step 53860; step_loss: 0.1185
step 53870; step_loss: 0.1196
step 53880; step_loss: 0.1489
step 53890; step_loss: 0.1948
step 53900; step_loss: 0.1284
step 53910; step_loss: 0.0863
step 53920; step_loss: 0.1144
step 53930; step_loss: 0.1732
step 53940; step_loss: 0.1500
step 53950; step_loss: 0.1444
step 53960; step_loss: 0.1075
step 53970; step_loss: 0.1168
step 53980; step_loss: 0.1951
step 53990; step_loss: 0.1735

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 1.010 | 1.141 | 1.387 | 1.483 | 1.645 | 1.901 |

============================
Global step:         54000
Learning rate:       0.0039
Step-time (ms):     35.9579
Train loss avg:      0.1487
--------------------------
Val loss:            0.8732
srnn loss:           0.7393
============================

Saving the model...
done in 464.76 ms
step 54000; step_loss: 0.2262
step 54010; step_loss: 0.0917
step 54020; step_loss: 0.1324
step 54030; step_loss: 0.1949
step 54040; step_loss: 0.1364
step 54050; step_loss: 0.1420
step 54060; step_loss: 0.1415
step 54070; step_loss: 0.1390
step 54080; step_loss: 0.1182
step 54090; step_loss: 0.1688
step 54100; step_loss: 0.1657
step 54110; step_loss: 0.1325
step 54120; step_loss: 0.1226
step 54130; step_loss: 0.1499
step 54140; step_loss: 0.1343
step 54150; step_loss: 0.1684
step 54160; step_loss: 0.1346
step 54170; step_loss: 0.1491
step 54180; step_loss: 0.1619
step 54190; step_loss: 0.1427
step 54200; step_loss: 0.1598
step 54210; step_loss: 0.2155
step 54220; step_loss: 0.1614
step 54230; step_loss: 0.1252
step 54240; step_loss: 0.1513
step 54250; step_loss: 0.1539
step 54260; step_loss: 0.1510
step 54270; step_loss: 0.1533
step 54280; step_loss: 0.1174
step 54290; step_loss: 0.1734
step 54300; step_loss: 0.1341
step 54310; step_loss: 0.1171
step 54320; step_loss: 0.2032
step 54330; step_loss: 0.1755
step 54340; step_loss: 0.1542
step 54350; step_loss: 0.1222
step 54360; step_loss: 0.1601
step 54370; step_loss: 0.1424
step 54380; step_loss: 0.1879
step 54390; step_loss: 0.1786
step 54400; step_loss: 0.1794
step 54410; step_loss: 0.1292
step 54420; step_loss: 0.1106
step 54430; step_loss: 0.1309
step 54440; step_loss: 0.0959
step 54450; step_loss: 0.1527
step 54460; step_loss: 0.2033
step 54470; step_loss: 0.1143
step 54480; step_loss: 0.0971
step 54490; step_loss: 0.1288
step 54500; step_loss: 0.1294
step 54510; step_loss: 0.1451
step 54520; step_loss: 0.0940
step 54530; step_loss: 0.1480
step 54540; step_loss: 0.1532
step 54550; step_loss: 0.1456
step 54560; step_loss: 0.1618
step 54570; step_loss: 0.1315
step 54580; step_loss: 0.1523
step 54590; step_loss: 0.1270
step 54600; step_loss: 0.1236
step 54610; step_loss: 0.0764
step 54620; step_loss: 0.1911
step 54630; step_loss: 0.1959
step 54640; step_loss: 0.1350
step 54650; step_loss: 0.1395
step 54660; step_loss: 0.1612
step 54670; step_loss: 0.1633
step 54680; step_loss: 0.1264
step 54690; step_loss: 0.1333
step 54700; step_loss: 0.1467
step 54710; step_loss: 0.1273
step 54720; step_loss: 0.1457
step 54730; step_loss: 0.1419
step 54740; step_loss: 0.1936
step 54750; step_loss: 0.2084
step 54760; step_loss: 0.2154
step 54770; step_loss: 0.1440
step 54780; step_loss: 0.1437
step 54790; step_loss: 0.1299
step 54800; step_loss: 0.1829
step 54810; step_loss: 0.1868
step 54820; step_loss: 0.2240
step 54830; step_loss: 0.1412
step 54840; step_loss: 0.0985
step 54850; step_loss: 0.1108
step 54860; step_loss: 0.1045
step 54870; step_loss: 0.1613
step 54880; step_loss: 0.1898
step 54890; step_loss: 0.1361
step 54900; step_loss: 0.1411
step 54910; step_loss: 0.1889
step 54920; step_loss: 0.1620
step 54930; step_loss: 0.1993
step 54940; step_loss: 0.1777
step 54950; step_loss: 0.1547
step 54960; step_loss: 0.2045
step 54970; step_loss: 0.1343
step 54980; step_loss: 0.1561
step 54990; step_loss: 0.1402

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 1.014 | 1.148 | 1.398 | 1.496 | 1.657 | 1.905 |

============================
Global step:         55000
Learning rate:       0.0039
Step-time (ms):     35.9718
Train loss avg:      0.1488
--------------------------
Val loss:            0.8339
srnn loss:           0.7413
============================

Saving the model...
done in 407.09 ms
step 55000; step_loss: 0.1594
step 55010; step_loss: 0.1148
step 55020; step_loss: 0.1421
step 55030; step_loss: 0.1141
step 55040; step_loss: 0.1647
step 55050; step_loss: 0.1295
step 55060; step_loss: 0.1570
step 55070; step_loss: 0.0970
step 55080; step_loss: 0.1292
step 55090; step_loss: 0.1128
step 55100; step_loss: 0.1247
step 55110; step_loss: 0.1252
step 55120; step_loss: 0.1540
step 55130; step_loss: 0.1484
step 55140; step_loss: 0.1380
step 55150; step_loss: 0.1693
step 55160; step_loss: 0.1592
step 55170; step_loss: 0.1638
step 55180; step_loss: 0.1293
step 55190; step_loss: 0.1647
step 55200; step_loss: 0.1834
step 55210; step_loss: 0.1007
step 55220; step_loss: 0.1504
step 55230; step_loss: 0.1555
step 55240; step_loss: 0.1172
step 55250; step_loss: 0.1327
step 55260; step_loss: 0.1921
step 55270; step_loss: 0.1071
step 55280; step_loss: 0.1064
step 55290; step_loss: 0.1373
step 55300; step_loss: 0.1396
step 55310; step_loss: 0.1101
step 55320; step_loss: 0.1345
step 55330; step_loss: 0.1369
step 55340; step_loss: 0.1586
step 55350; step_loss: 0.1159
step 55360; step_loss: 0.1851
step 55370; step_loss: 0.1737
step 55380; step_loss: 0.1507
step 55390; step_loss: 0.1388
step 55400; step_loss: 0.1581
step 55410; step_loss: 0.1056
step 55420; step_loss: 0.1483
step 55430; step_loss: 0.2156
step 55440; step_loss: 0.1762
step 55450; step_loss: 0.1103
step 55460; step_loss: 0.2224
step 55470; step_loss: 0.1570
step 55480; step_loss: 0.1143
step 55490; step_loss: 0.1984
step 55500; step_loss: 0.1752
step 55510; step_loss: 0.1634
step 55520; step_loss: 0.1603
step 55530; step_loss: 0.1343
step 55540; step_loss: 0.1209
step 55550; step_loss: 0.1863
step 55560; step_loss: 0.1737
step 55570; step_loss: 0.2133
step 55580; step_loss: 0.1605
step 55590; step_loss: 0.1090
step 55600; step_loss: 0.1792
step 55610; step_loss: 0.1305
step 55620; step_loss: 0.1332
step 55630; step_loss: 0.1259
step 55640; step_loss: 0.1211
step 55650; step_loss: 0.1465
step 55660; step_loss: 0.1920
step 55670; step_loss: 0.1618
step 55680; step_loss: 0.1437
step 55690; step_loss: 0.1012
step 55700; step_loss: 0.1729
step 55710; step_loss: 0.1337
step 55720; step_loss: 0.1349
step 55730; step_loss: 0.1395
step 55740; step_loss: 0.1242
step 55750; step_loss: 0.1235
step 55760; step_loss: 0.1256
step 55770; step_loss: 0.1201
step 55780; step_loss: 0.1178
step 55790; step_loss: 0.1298
step 55800; step_loss: 0.1481
step 55810; step_loss: 0.1947
step 55820; step_loss: 0.1527
step 55830; step_loss: 0.1564
step 55840; step_loss: 0.1787
step 55850; step_loss: 0.1852
step 55860; step_loss: 0.1480
step 55870; step_loss: 0.1251
step 55880; step_loss: 0.1202
step 55890; step_loss: 0.1345
step 55900; step_loss: 0.1690
step 55910; step_loss: 0.1460
step 55920; step_loss: 0.1093
step 55930; step_loss: 0.1205
step 55940; step_loss: 0.1861
step 55950; step_loss: 0.1096
step 55960; step_loss: 0.1355
step 55970; step_loss: 0.1573
step 55980; step_loss: 0.1536
step 55990; step_loss: 0.1683

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 1.004 | 1.136 | 1.382 | 1.478 | 1.637 | 1.882 |

============================
Global step:         56000
Learning rate:       0.0039
Step-time (ms):     35.9382
Train loss avg:      0.1467
--------------------------
Val loss:            0.8891
srnn loss:           0.7294
============================

Saving the model...
done in 404.34 ms
step 56000; step_loss: 0.1030
step 56010; step_loss: 0.1699
step 56020; step_loss: 0.1069
step 56030; step_loss: 0.1703
step 56040; step_loss: 0.1249
step 56050; step_loss: 0.1660
step 56060; step_loss: 0.1434
step 56070; step_loss: 0.1055
step 56080; step_loss: 0.1261
step 56090; step_loss: 0.1264
step 56100; step_loss: 0.1555
step 56110; step_loss: 0.1453
step 56120; step_loss: 0.1456
step 56130; step_loss: 0.1055
step 56140; step_loss: 0.1595
step 56150; step_loss: 0.1197
step 56160; step_loss: 0.1905
step 56170; step_loss: 0.1741
step 56180; step_loss: 0.1883
step 56190; step_loss: 0.1736
step 56200; step_loss: 0.1392
step 56210; step_loss: 0.1356
step 56220; step_loss: 0.1840
step 56230; step_loss: 0.1293
step 56240; step_loss: 0.0933
step 56250; step_loss: 0.1621
step 56260; step_loss: 0.1156
step 56270; step_loss: 0.1456
step 56280; step_loss: 0.1635
step 56290; step_loss: 0.1998
step 56300; step_loss: 0.1303
step 56310; step_loss: 0.1490
step 56320; step_loss: 0.1643
step 56330; step_loss: 0.1130
step 56340; step_loss: 0.1328
step 56350; step_loss: 0.1007
step 56360; step_loss: 0.1422
step 56370; step_loss: 0.1000
step 56380; step_loss: 0.1603
step 56390; step_loss: 0.1460
step 56400; step_loss: 0.1152
step 56410; step_loss: 0.1576
step 56420; step_loss: 0.1478
step 56430; step_loss: 0.1705
step 56440; step_loss: 0.1548
step 56450; step_loss: 0.1630
step 56460; step_loss: 0.1023
step 56470; step_loss: 0.1336
step 56480; step_loss: 0.1799
step 56490; step_loss: 0.2218
step 56500; step_loss: 0.1579
step 56510; step_loss: 0.1519
step 56520; step_loss: 0.1508
step 56530; step_loss: 0.1329
step 56540; step_loss: 0.1578
step 56550; step_loss: 0.1753
step 56560; step_loss: 0.1575
step 56570; step_loss: 0.1478
step 56580; step_loss: 0.1941
step 56590; step_loss: 0.1187
step 56600; step_loss: 0.1616
step 56610; step_loss: 0.1668
step 56620; step_loss: 0.1240
step 56630; step_loss: 0.0881
step 56640; step_loss: 0.1599
step 56650; step_loss: 0.1345
step 56660; step_loss: 0.1568
step 56670; step_loss: 0.1512
step 56680; step_loss: 0.1097
step 56690; step_loss: 0.1336
step 56700; step_loss: 0.1626
step 56710; step_loss: 0.1761
step 56720; step_loss: 0.1461
step 56730; step_loss: 0.1345
step 56740; step_loss: 0.1377
step 56750; step_loss: 0.1332
step 56760; step_loss: 0.1511
step 56770; step_loss: 0.1713
step 56780; step_loss: 0.1701
step 56790; step_loss: 0.0915
step 56800; step_loss: 0.1631
step 56810; step_loss: 0.2065
step 56820; step_loss: 0.1153
step 56830; step_loss: 0.1470
step 56840; step_loss: 0.1839
step 56850; step_loss: 0.1252
step 56860; step_loss: 0.1837
step 56870; step_loss: 0.1367
step 56880; step_loss: 0.1299
step 56890; step_loss: 0.1761
step 56900; step_loss: 0.1510
step 56910; step_loss: 0.1264
step 56920; step_loss: 0.2029
step 56930; step_loss: 0.2000
step 56940; step_loss: 0.1719
step 56950; step_loss: 0.2303
step 56960; step_loss: 0.1079
step 56970; step_loss: 0.1718
step 56980; step_loss: 0.2203
step 56990; step_loss: 0.1152

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 0.998 | 1.130 | 1.379 | 1.477 | 1.637 | 1.896 |

============================
Global step:         57000
Learning rate:       0.0039
Step-time (ms):     35.8253
Train loss avg:      0.1456
--------------------------
Val loss:            0.9399
srnn loss:           0.7364
============================

Saving the model...
done in 408.56 ms
step 57000; step_loss: 0.1152
step 57010; step_loss: 0.1874
step 57020; step_loss: 0.0837
step 57030; step_loss: 0.1772
step 57040; step_loss: 0.1097
step 57050; step_loss: 0.1746
step 57060; step_loss: 0.1310
step 57070; step_loss: 0.1290
step 57080; step_loss: 0.1177
step 57090; step_loss: 0.1784
step 57100; step_loss: 0.1631
step 57110; step_loss: 0.1152
step 57120; step_loss: 0.1387
step 57130; step_loss: 0.1121
step 57140; step_loss: 0.1182
step 57150; step_loss: 0.1486
step 57160; step_loss: 0.1417
step 57170; step_loss: 0.1634
step 57180; step_loss: 0.1312
step 57190; step_loss: 0.1606
step 57200; step_loss: 0.2100
step 57210; step_loss: 0.1683
step 57220; step_loss: 0.1281
step 57230; step_loss: 0.1230
step 57240; step_loss: 0.1413
step 57250; step_loss: 0.1793
step 57260; step_loss: 0.1848
step 57270; step_loss: 0.1957
step 57280; step_loss: 0.1382
step 57290; step_loss: 0.1436
step 57300; step_loss: 0.1463
step 57310; step_loss: 0.1399
step 57320; step_loss: 0.1234
step 57330; step_loss: 0.1371
step 57340; step_loss: 0.1786
step 57350; step_loss: 0.0897
step 57360; step_loss: 0.1712
step 57370; step_loss: 0.1414
step 57380; step_loss: 0.1798
step 57390; step_loss: 0.1129
step 57400; step_loss: 0.1636
step 57410; step_loss: 0.1679
step 57420; step_loss: 0.1571
step 57430; step_loss: 0.1081
step 57440; step_loss: 0.1781
step 57450; step_loss: 0.1661
step 57460; step_loss: 0.1322
step 57470; step_loss: 0.1527
step 57480; step_loss: 0.1450
step 57490; step_loss: 0.1973
step 57500; step_loss: 0.1392
step 57510; step_loss: 0.1508
step 57520; step_loss: 0.1735
step 57530; step_loss: 0.1648
step 57540; step_loss: 0.1417
step 57550; step_loss: 0.1197
step 57560; step_loss: 0.1433
step 57570; step_loss: 0.1004
step 57580; step_loss: 0.1484
step 57590; step_loss: 0.1447
step 57600; step_loss: 0.1662
step 57610; step_loss: 0.1123
step 57620; step_loss: 0.1149
step 57630; step_loss: 0.1370
step 57640; step_loss: 0.1775
step 57650; step_loss: 0.1311
step 57660; step_loss: 0.1311
step 57670; step_loss: 0.1567
step 57680; step_loss: 0.1062
step 57690; step_loss: 0.1182
step 57700; step_loss: 0.1666
step 57710; step_loss: 0.1412
step 57720; step_loss: 0.1720
step 57730; step_loss: 0.1222
step 57740; step_loss: 0.1895
step 57750; step_loss: 0.0995
step 57760; step_loss: 0.1211
step 57770; step_loss: 0.1706
step 57780; step_loss: 0.1550
step 57790; step_loss: 0.1279
step 57800; step_loss: 0.1395
step 57810; step_loss: 0.1212
step 57820; step_loss: 0.1477
step 57830; step_loss: 0.1969
step 57840; step_loss: 0.1580
step 57850; step_loss: 0.1189
step 57860; step_loss: 0.1181
step 57870; step_loss: 0.1193
step 57880; step_loss: 0.1441
step 57890; step_loss: 0.1023
step 57900; step_loss: 0.1809
step 57910; step_loss: 0.1356
step 57920; step_loss: 0.1519
step 57930; step_loss: 0.1541
step 57940; step_loss: 0.1246
step 57950; step_loss: 0.1289
step 57960; step_loss: 0.0920
step 57970; step_loss: 0.1465
step 57980; step_loss: 0.1758
step 57990; step_loss: 0.1366

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 0.998 | 1.131 | 1.379 | 1.476 | 1.636 | 1.888 |

============================
Global step:         58000
Learning rate:       0.0039
Step-time (ms):     35.8967
Train loss avg:      0.1445
--------------------------
Val loss:            0.9505
srnn loss:           0.7355
============================

Saving the model...
done in 413.64 ms
step 58000; step_loss: 0.1446
step 58010; step_loss: 0.1613
step 58020; step_loss: 0.1141
step 58030; step_loss: 0.1163
step 58040; step_loss: 0.0962
step 58050; step_loss: 0.1449
step 58060; step_loss: 0.1204
step 58070; step_loss: 0.1175
step 58080; step_loss: 0.1495
step 58090; step_loss: 0.1792
step 58100; step_loss: 0.1416
step 58110; step_loss: 0.1626
step 58120; step_loss: 0.1330
step 58130; step_loss: 0.1082
step 58140; step_loss: 0.1459
step 58150; step_loss: 0.1419
step 58160; step_loss: 0.1473
step 58170; step_loss: 0.1315
step 58180; step_loss: 0.1353
step 58190; step_loss: 0.1478
step 58200; step_loss: 0.1652
step 58210; step_loss: 0.1807
step 58220; step_loss: 0.1509
step 58230; step_loss: 0.1139
step 58240; step_loss: 0.1500
step 58250; step_loss: 0.1214
step 58260; step_loss: 0.1595
step 58270; step_loss: 0.2180
step 58280; step_loss: 0.1174
step 58290; step_loss: 0.1362
step 58300; step_loss: 0.1368
step 58310; step_loss: 0.1672
step 58320; step_loss: 0.1656
step 58330; step_loss: 0.1306
step 58340; step_loss: 0.1181
step 58350; step_loss: 0.1584
step 58360; step_loss: 0.1092
step 58370; step_loss: 0.1163
step 58380; step_loss: 0.1735
step 58390; step_loss: 0.1145
step 58400; step_loss: 0.2004
step 58410; step_loss: 0.1579
step 58420; step_loss: 0.0910
step 58430; step_loss: 0.1710
step 58440; step_loss: 0.1381
step 58450; step_loss: 0.1260
step 58460; step_loss: 0.1950
step 58470; step_loss: 0.1758
step 58480; step_loss: 0.1377
step 58490; step_loss: 0.1166
step 58500; step_loss: 0.1411
step 58510; step_loss: 0.1414
step 58520; step_loss: 0.1393
step 58530; step_loss: 0.0948
step 58540; step_loss: 0.1157
step 58550; step_loss: 0.1454
step 58560; step_loss: 0.1255
step 58570; step_loss: 0.1799
step 58580; step_loss: 0.1545
step 58590; step_loss: 0.1310
step 58600; step_loss: 0.1297
step 58610; step_loss: 0.1220
step 58620; step_loss: 0.1585
step 58630; step_loss: 0.1284
step 58640; step_loss: 0.1425
step 58650; step_loss: 0.1257
step 58660; step_loss: 0.1307
step 58670; step_loss: 0.1562
step 58680; step_loss: 0.1384
step 58690; step_loss: 0.1625
step 58700; step_loss: 0.1011
step 58710; step_loss: 0.2217
step 58720; step_loss: 0.1217
step 58730; step_loss: 0.1456
step 58740; step_loss: 0.1512
step 58750; step_loss: 0.1742
step 58760; step_loss: 0.1442
step 58770; step_loss: 0.1542
step 58780; step_loss: 0.1746
step 58790; step_loss: 0.1718
step 58800; step_loss: 0.1327
step 58810; step_loss: 0.1481
step 58820; step_loss: 0.1388
step 58830; step_loss: 0.1553
step 58840; step_loss: 0.1467
step 58850; step_loss: 0.1030
step 58860; step_loss: 0.1803
step 58870; step_loss: 0.1209
step 58880; step_loss: 0.1608
step 58890; step_loss: 0.1646
step 58900; step_loss: 0.1617
step 58910; step_loss: 0.1873
step 58920; step_loss: 0.1077
step 58930; step_loss: 0.0968
step 58940; step_loss: 0.1074
step 58950; step_loss: 0.1130
step 58960; step_loss: 0.1516
step 58970; step_loss: 0.1483
step 58980; step_loss: 0.1315
step 58990; step_loss: 0.1565

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 0.995 | 1.130 | 1.385 | 1.485 | 1.649 | 1.902 |

============================
Global step:         59000
Learning rate:       0.0039
Step-time (ms):     35.8252
Train loss avg:      0.1419
--------------------------
Val loss:            0.8926
srnn loss:           0.7441
============================

Saving the model...
done in 389.47 ms
step 59000; step_loss: 0.1613
step 59010; step_loss: 0.1085
step 59020; step_loss: 0.1431
step 59030; step_loss: 0.1113
step 59040; step_loss: 0.1538
step 59050; step_loss: 0.1723
step 59060; step_loss: 0.1820
step 59070; step_loss: 0.1553
step 59080; step_loss: 0.1899
step 59090; step_loss: 0.1255
step 59100; step_loss: 0.1619
step 59110; step_loss: 0.1334
step 59120; step_loss: 0.1323
step 59130; step_loss: 0.1349
step 59140; step_loss: 0.1678
step 59150; step_loss: 0.1356
step 59160; step_loss: 0.1374
step 59170; step_loss: 0.1898
step 59180; step_loss: 0.1505
step 59190; step_loss: 0.1484
step 59200; step_loss: 0.1548
step 59210; step_loss: 0.1377
step 59220; step_loss: 0.1509
step 59230; step_loss: 0.1325
step 59240; step_loss: 0.1385
step 59250; step_loss: 0.1422
step 59260; step_loss: 0.1390
step 59270; step_loss: 0.0992
step 59280; step_loss: 0.1530
step 59290; step_loss: 0.1203
step 59300; step_loss: 0.0891
step 59310; step_loss: 0.1550
step 59320; step_loss: 0.1449
step 59330; step_loss: 0.0970
step 59340; step_loss: 0.2374
step 59350; step_loss: 0.0971
step 59360; step_loss: 0.1406
step 59370; step_loss: 0.1587
step 59380; step_loss: 0.1910
step 59390; step_loss: 0.1224
step 59400; step_loss: 0.1170
step 59410; step_loss: 0.1357
step 59420; step_loss: 0.1263
step 59430; step_loss: 0.1584
step 59440; step_loss: 0.1572
step 59450; step_loss: 0.1312
step 59460; step_loss: 0.1561
step 59470; step_loss: 0.1132
step 59480; step_loss: 0.1350
step 59490; step_loss: 0.1333
step 59500; step_loss: 0.1603
step 59510; step_loss: 0.1601
step 59520; step_loss: 0.1953
step 59530; step_loss: 0.1515
step 59540; step_loss: 0.1159
step 59550; step_loss: 0.1096
step 59560; step_loss: 0.1147
step 59570; step_loss: 0.1526
step 59580; step_loss: 0.1034
step 59590; step_loss: 0.1047
step 59600; step_loss: 0.1411
step 59610; step_loss: 0.2100
step 59620; step_loss: 0.1221
step 59630; step_loss: 0.1354
step 59640; step_loss: 0.1269
step 59650; step_loss: 0.0949
step 59660; step_loss: 0.1283
step 59670; step_loss: 0.1268
step 59680; step_loss: 0.1667
step 59690; step_loss: 0.1493
step 59700; step_loss: 0.1283
step 59710; step_loss: 0.1764
step 59720; step_loss: 0.1140
step 59730; step_loss: 0.1520
step 59740; step_loss: 0.1274
step 59750; step_loss: 0.1750
step 59760; step_loss: 0.1327
step 59770; step_loss: 0.1192
step 59780; step_loss: 0.1492
step 59790; step_loss: 0.1548
step 59800; step_loss: 0.1611
step 59810; step_loss: 0.0979
step 59820; step_loss: 0.1147
step 59830; step_loss: 0.1358
step 59840; step_loss: 0.1338
step 59850; step_loss: 0.1195
step 59860; step_loss: 0.1537
step 59870; step_loss: 0.1519
step 59880; step_loss: 0.1169
step 59890; step_loss: 0.1669
step 59900; step_loss: 0.1600
step 59910; step_loss: 0.0855
step 59920; step_loss: 0.0845
step 59930; step_loss: 0.1510
step 59940; step_loss: 0.1128
step 59950; step_loss: 0.1446
step 59960; step_loss: 0.1582
step 59970; step_loss: 0.1358
step 59980; step_loss: 0.1233
step 59990; step_loss: 0.1556

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 0.994 | 1.130 | 1.385 | 1.483 | 1.646 | 1.903 |

============================
Global step:         60000
Learning rate:       0.0037
Step-time (ms):     35.9229
Train loss avg:      0.1411
--------------------------
Val loss:            0.8410
srnn loss:           0.7363
============================

Saving the model...
done in 402.47 ms
step 60000; step_loss: 0.1534
step 60010; step_loss: 0.1166
step 60020; step_loss: 0.1670
step 60030; step_loss: 0.1745
step 60040; step_loss: 0.1343
step 60050; step_loss: 0.1351
step 60060; step_loss: 0.1760
step 60070; step_loss: 0.1160
step 60080; step_loss: 0.1451
step 60090; step_loss: 0.2004
step 60100; step_loss: 0.1200
step 60110; step_loss: 0.1256
step 60120; step_loss: 0.1247
step 60130; step_loss: 0.1103
step 60140; step_loss: 0.1522
step 60150; step_loss: 0.1786
step 60160; step_loss: 0.1254
step 60170; step_loss: 0.1378
step 60180; step_loss: 0.1386
step 60190; step_loss: 0.1452
step 60200; step_loss: 0.1088
step 60210; step_loss: 0.1122
step 60220; step_loss: 0.1249
step 60230; step_loss: 0.1176
step 60240; step_loss: 0.1567
step 60250; step_loss: 0.1601
step 60260; step_loss: 0.1070
step 60270; step_loss: 0.1681
step 60280; step_loss: 0.1207
step 60290; step_loss: 0.1583
step 60300; step_loss: 0.1201
step 60310; step_loss: 0.1251
step 60320; step_loss: 0.1447
step 60330; step_loss: 0.2251
step 60340; step_loss: 0.1420
step 60350; step_loss: 0.1198
step 60360; step_loss: 0.1557
step 60370; step_loss: 0.1526
step 60380; step_loss: 0.1000
step 60390; step_loss: 0.1472
step 60400; step_loss: 0.1189
step 60410; step_loss: 0.1235
step 60420; step_loss: 0.2063
step 60430; step_loss: 0.1940
step 60440; step_loss: 0.1531
step 60450; step_loss: 0.1427
step 60460; step_loss: 0.1826
step 60470; step_loss: 0.1160
step 60480; step_loss: 0.1062
step 60490; step_loss: 0.1147
step 60500; step_loss: 0.0939
step 60510; step_loss: 0.1424
step 60520; step_loss: 0.0946
step 60530; step_loss: 0.2114
step 60540; step_loss: 0.1569
step 60550; step_loss: 0.1589
step 60560; step_loss: 0.1775
step 60570; step_loss: 0.1173
step 60580; step_loss: 0.1906
step 60590; step_loss: 0.1406
step 60600; step_loss: 0.1853
step 60610; step_loss: 0.1113
step 60620; step_loss: 0.1202
step 60630; step_loss: 0.1092
step 60640; step_loss: 0.1214
step 60650; step_loss: 0.1576
step 60660; step_loss: 0.1259
step 60670; step_loss: 0.1066
step 60680; step_loss: 0.1138
step 60690; step_loss: 0.1346
step 60700; step_loss: 0.1099
step 60710; step_loss: 0.1221
step 60720; step_loss: 0.1167
step 60730; step_loss: 0.1422
step 60740; step_loss: 0.1034
step 60750; step_loss: 0.1394
step 60760; step_loss: 0.1158
step 60770; step_loss: 0.1374
step 60780; step_loss: 0.2228
step 60790; step_loss: 0.1284
step 60800; step_loss: 0.1544
step 60810; step_loss: 0.1508
step 60820; step_loss: 0.1528
step 60830; step_loss: 0.1715
step 60840; step_loss: 0.1633
step 60850; step_loss: 0.1630
step 60860; step_loss: 0.1618
step 60870; step_loss: 0.1552
step 60880; step_loss: 0.1844
step 60890; step_loss: 0.1832
step 60900; step_loss: 0.1109
step 60910; step_loss: 0.1330
step 60920; step_loss: 0.1137
step 60930; step_loss: 0.1560
step 60940; step_loss: 0.1340
step 60950; step_loss: 0.1061
step 60960; step_loss: 0.1234
step 60970; step_loss: 0.1793
step 60980; step_loss: 0.1541
step 60990; step_loss: 0.1636

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 0.989 | 1.125 | 1.380 | 1.481 | 1.646 | 1.906 |

============================
Global step:         61000
Learning rate:       0.0037
Step-time (ms):     35.9282
Train loss avg:      0.1402
--------------------------
Val loss:            0.8894
srnn loss:           0.7429
============================

Saving the model...
done in 413.26 ms
step 61000; step_loss: 0.1818
step 61010; step_loss: 0.1468
step 61020; step_loss: 0.1401
step 61030; step_loss: 0.1326
step 61040; step_loss: 0.1650
step 61050; step_loss: 0.1648
step 61060; step_loss: 0.1632
step 61070; step_loss: 0.1108
step 61080; step_loss: 0.1169
step 61090; step_loss: 0.0917
step 61100; step_loss: 0.1304
step 61110; step_loss: 0.1134
step 61120; step_loss: 0.1170
step 61130; step_loss: 0.1100
step 61140; step_loss: 0.1734
step 61150; step_loss: 0.1447
step 61160; step_loss: 0.1181
step 61170; step_loss: 0.1188
step 61180; step_loss: 0.1947
step 61190; step_loss: 0.1626
step 61200; step_loss: 0.1329
step 61210; step_loss: 0.1456
step 61220; step_loss: 0.2085
step 61230; step_loss: 0.1549
step 61240; step_loss: 0.1691
step 61250; step_loss: 0.2624
step 61260; step_loss: 0.1096
step 61270; step_loss: 0.0980
step 61280; step_loss: 0.1336
step 61290; step_loss: 0.1729
step 61300; step_loss: 0.1740
step 61310; step_loss: 0.1503
step 61320; step_loss: 0.2125
step 61330; step_loss: 0.1536
step 61340; step_loss: 0.1601
step 61350; step_loss: 0.1422
step 61360; step_loss: 0.1455
step 61370; step_loss: 0.1264
step 61380; step_loss: 0.1210
step 61390; step_loss: 0.1787
step 61400; step_loss: 0.1186
step 61410; step_loss: 0.1438
step 61420; step_loss: 0.1517
step 61430; step_loss: 0.1993
step 61440; step_loss: 0.1515
step 61450; step_loss: 0.1033
step 61460; step_loss: 0.1419
step 61470; step_loss: 0.1207
step 61480; step_loss: 0.1381
step 61490; step_loss: 0.1368
step 61500; step_loss: 0.1089
step 61510; step_loss: 0.1479
step 61520; step_loss: 0.1515
step 61530; step_loss: 0.1270
step 61540; step_loss: 0.1862
step 61550; step_loss: 0.1263
step 61560; step_loss: 0.1764
step 61570; step_loss: 0.1623
step 61580; step_loss: 0.1471
step 61590; step_loss: 0.0966
step 61600; step_loss: 0.1391
step 61610; step_loss: 0.1448
step 61620; step_loss: 0.1218
step 61630; step_loss: 0.1565
step 61640; step_loss: 0.1814
step 61650; step_loss: 0.1323
step 61660; step_loss: 0.0962
step 61670; step_loss: 0.1059
step 61680; step_loss: 0.1248
step 61690; step_loss: 0.1229
step 61700; step_loss: 0.1468
step 61710; step_loss: 0.1335
step 61720; step_loss: 0.1618
step 61730; step_loss: 0.1364
step 61740; step_loss: 0.1807
step 61750; step_loss: 0.1402
step 61760; step_loss: 0.2181
step 61770; step_loss: 0.1650
step 61780; step_loss: 0.1131
step 61790; step_loss: 0.2005
step 61800; step_loss: 0.1827
step 61810; step_loss: 0.1295
step 61820; step_loss: 0.1442
step 61830; step_loss: 0.1130
step 61840; step_loss: 0.1616
step 61850; step_loss: 0.0991
step 61860; step_loss: 0.1499
step 61870; step_loss: 0.1324
step 61880; step_loss: 0.1293
step 61890; step_loss: 0.1812
step 61900; step_loss: 0.1451
step 61910; step_loss: 0.1172
step 61920; step_loss: 0.1376
step 61930; step_loss: 0.1627
step 61940; step_loss: 0.1798
step 61950; step_loss: 0.1640
step 61960; step_loss: 0.1417
step 61970; step_loss: 0.0998
step 61980; step_loss: 0.1659
step 61990; step_loss: 0.1310

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 0.990 | 1.129 | 1.390 | 1.492 | 1.656 | 1.907 |

============================
Global step:         62000
Learning rate:       0.0037
Step-time (ms):     35.9871
Train loss avg:      0.1387
--------------------------
Val loss:            0.7954
srnn loss:           0.7481
============================

Saving the model...
done in 442.96 ms
step 62000; step_loss: 0.1123
step 62010; step_loss: 0.1280
step 62020; step_loss: 0.1336
step 62030; step_loss: 0.1203
step 62040; step_loss: 0.1689
step 62050; step_loss: 0.0896
step 62060; step_loss: 0.1004
step 62070; step_loss: 0.1226
step 62080; step_loss: 0.1137
step 62090; step_loss: 0.1353
step 62100; step_loss: 0.1723
step 62110; step_loss: 0.1565
step 62120; step_loss: 0.1129
step 62130; step_loss: 0.1109
step 62140; step_loss: 0.1297
step 62150; step_loss: 0.1689
step 62160; step_loss: 0.1182
step 62170; step_loss: 0.1227
step 62180; step_loss: 0.1561
step 62190; step_loss: 0.1366
step 62200; step_loss: 0.1630
step 62210; step_loss: 0.1574
step 62220; step_loss: 0.0924
step 62230; step_loss: 0.1073
step 62240; step_loss: 0.1544
step 62250; step_loss: 0.1907
step 62260; step_loss: 0.1783
step 62270; step_loss: 0.1466
step 62280; step_loss: 0.1506
step 62290; step_loss: 0.0863
step 62300; step_loss: 0.1270
step 62310; step_loss: 0.1393
step 62320; step_loss: 0.2179
step 62330; step_loss: 0.1330
step 62340; step_loss: 0.1163
step 62350; step_loss: 0.1187
step 62360; step_loss: 0.1244
step 62370; step_loss: 0.1494
step 62380; step_loss: 0.2109
step 62390; step_loss: 0.1746
step 62400; step_loss: 0.1421
step 62410; step_loss: 0.1358
step 62420; step_loss: 0.1097
step 62430; step_loss: 0.1477
step 62440; step_loss: 0.1479
step 62450; step_loss: 0.1544
step 62460; step_loss: 0.1053
step 62470; step_loss: 0.1652
step 62480; step_loss: 0.1690
step 62490; step_loss: 0.1631
step 62500; step_loss: 0.1227
step 62510; step_loss: 0.1407
step 62520; step_loss: 0.1629
step 62530; step_loss: 0.0994
step 62540; step_loss: 0.1311
step 62550; step_loss: 0.0869
step 62560; step_loss: 0.1796
step 62570; step_loss: 0.1343
step 62580; step_loss: 0.0913
step 62590; step_loss: 0.1752
step 62600; step_loss: 0.1683
step 62610; step_loss: 0.1007
step 62620; step_loss: 0.1348
step 62630; step_loss: 0.1493
step 62640; step_loss: 0.1279
step 62650; step_loss: 0.1374
step 62660; step_loss: 0.1623
step 62670; step_loss: 0.1902
step 62680; step_loss: 0.1620
step 62690; step_loss: 0.1155
step 62700; step_loss: 0.1662
step 62710; step_loss: 0.1260
step 62720; step_loss: 0.1146
step 62730; step_loss: 0.1387
step 62740; step_loss: 0.1666
step 62750; step_loss: 0.0896
step 62760; step_loss: 0.1091
step 62770; step_loss: 0.1644
step 62780; step_loss: 0.1222
step 62790; step_loss: 0.1198
step 62800; step_loss: 0.1275
step 62810; step_loss: 0.1910
step 62820; step_loss: 0.1265
step 62830; step_loss: 0.1253
step 62840; step_loss: 0.1323
step 62850; step_loss: 0.0960
step 62860; step_loss: 0.1430
step 62870; step_loss: 0.1047
step 62880; step_loss: 0.1566
step 62890; step_loss: 0.1635
step 62900; step_loss: 0.1352
step 62910; step_loss: 0.1269
step 62920; step_loss: 0.1497
step 62930; step_loss: 0.1688
step 62940; step_loss: 0.1292
step 62950; step_loss: 0.1289
step 62960; step_loss: 0.1339
step 62970; step_loss: 0.1330
step 62980; step_loss: 0.1233
step 62990; step_loss: 0.1612

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 0.988 | 1.126 | 1.386 | 1.488 | 1.651 | 1.897 |

============================
Global step:         63000
Learning rate:       0.0037
Step-time (ms):     35.8833
Train loss avg:      0.1400
--------------------------
Val loss:            0.8963
srnn loss:           0.7482
============================

Saving the model...
done in 413.63 ms
step 63000; step_loss: 0.1132
step 63010; step_loss: 0.1122
step 63020; step_loss: 0.1230
step 63030; step_loss: 0.1351
step 63040; step_loss: 0.1125
step 63050; step_loss: 0.1963
step 63060; step_loss: 0.1216
step 63070; step_loss: 0.0841
step 63080; step_loss: 0.1324
step 63090; step_loss: 0.1345
step 63100; step_loss: 0.1446
step 63110; step_loss: 0.1430
step 63120; step_loss: 0.1185
step 63130; step_loss: 0.1605
step 63140; step_loss: 0.1149
step 63150; step_loss: 0.1203
step 63160; step_loss: 0.1882
step 63170; step_loss: 0.1700
step 63180; step_loss: 0.1440
step 63190; step_loss: 0.1952
step 63200; step_loss: 0.1218
step 63210; step_loss: 0.1157
step 63220; step_loss: 0.0941
step 63230; step_loss: 0.1240
step 63240; step_loss: 0.1227
step 63250; step_loss: 0.1855
step 63260; step_loss: 0.1169
step 63270; step_loss: 0.1036
step 63280; step_loss: 0.0954
step 63290; step_loss: 0.1207
step 63300; step_loss: 0.1159
step 63310; step_loss: 0.1112
step 63320; step_loss: 0.1921
step 63330; step_loss: 0.1061
step 63340; step_loss: 0.1481
step 63350; step_loss: 0.1137
step 63360; step_loss: 0.1442
step 63370; step_loss: 0.1517
step 63380; step_loss: 0.1562
step 63390; step_loss: 0.1961
step 63400; step_loss: 0.1089
step 63410; step_loss: 0.1654
step 63420; step_loss: 0.1432
step 63430; step_loss: 0.1643
step 63440; step_loss: 0.1069
step 63450; step_loss: 0.1386
step 63460; step_loss: 0.1533
step 63470; step_loss: 0.1273
step 63480; step_loss: 0.1392
step 63490; step_loss: 0.1306
step 63500; step_loss: 0.1007
step 63510; step_loss: 0.1422
step 63520; step_loss: 0.1371
step 63530; step_loss: 0.1485
step 63540; step_loss: 0.1293
step 63550; step_loss: 0.1714
step 63560; step_loss: 0.1950
step 63570; step_loss: 0.1860
step 63580; step_loss: 0.1106
step 63590; step_loss: 0.1497
step 63600; step_loss: 0.1732
step 63610; step_loss: 0.1334
step 63620; step_loss: 0.1284
step 63630; step_loss: 0.1570
step 63640; step_loss: 0.1765
step 63650; step_loss: 0.1041
step 63660; step_loss: 0.1114
step 63670; step_loss: 0.1321
step 63680; step_loss: 0.1249
step 63690; step_loss: 0.1383
step 63700; step_loss: 0.1447
step 63710; step_loss: 0.1211
step 63720; step_loss: 0.1163
step 63730; step_loss: 0.1280
step 63740; step_loss: 0.1261
step 63750; step_loss: 0.1379
step 63760; step_loss: 0.1121
step 63770; step_loss: 0.1212
step 63780; step_loss: 0.1074
step 63790; step_loss: 0.1672
step 63800; step_loss: 0.1295
step 63810; step_loss: 0.1820
step 63820; step_loss: 0.1168
step 63830; step_loss: 0.1430
step 63840; step_loss: 0.1215
step 63850; step_loss: 0.1604
step 63860; step_loss: 0.1027
step 63870; step_loss: 0.1220
step 63880; step_loss: 0.1083
step 63890; step_loss: 0.1560
step 63900; step_loss: 0.1660
step 63910; step_loss: 0.1704
step 63920; step_loss: 0.1310
step 63930; step_loss: 0.1455
step 63940; step_loss: 0.1368
step 63950; step_loss: 0.1819
step 63960; step_loss: 0.1884
step 63970; step_loss: 0.0853
step 63980; step_loss: 0.1267
step 63990; step_loss: 0.0953

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 0.990 | 1.131 | 1.395 | 1.498 | 1.668 | 1.923 |

============================
Global step:         64000
Learning rate:       0.0037
Step-time (ms):     35.9809
Train loss avg:      0.1369
--------------------------
Val loss:            0.8522
srnn loss:           0.7527
============================

Saving the model...
done in 408.06 ms
step 64000; step_loss: 0.2036
step 64010; step_loss: 0.0975
step 64020; step_loss: 0.1144
step 64030; step_loss: 0.0896
step 64040; step_loss: 0.1193
step 64050; step_loss: 0.1574
step 64060; step_loss: 0.1023
step 64070; step_loss: 0.1427
step 64080; step_loss: 0.1626
step 64090; step_loss: 0.1343
step 64100; step_loss: 0.1440
step 64110; step_loss: 0.1376
step 64120; step_loss: 0.0955
step 64130; step_loss: 0.1362
step 64140; step_loss: 0.1266
step 64150; step_loss: 0.1488
step 64160; step_loss: 0.1010
step 64170; step_loss: 0.1365
step 64180; step_loss: 0.0704
step 64190; step_loss: 0.1183
step 64200; step_loss: 0.1402
step 64210; step_loss: 0.1183
step 64220; step_loss: 0.1575
step 64230; step_loss: 0.1506
step 64240; step_loss: 0.1099
step 64250; step_loss: 0.1326
step 64260; step_loss: 0.1566
step 64270; step_loss: 0.1129
step 64280; step_loss: 0.1155
step 64290; step_loss: 0.1197
step 64300; step_loss: 0.1128
step 64310; step_loss: 0.0925
step 64320; step_loss: 0.1757
step 64330; step_loss: 0.1138
step 64340; step_loss: 0.1285
step 64350; step_loss: 0.0938
step 64360; step_loss: 0.1630
step 64370; step_loss: 0.1672
step 64380; step_loss: 0.1204
step 64390; step_loss: 0.1109
step 64400; step_loss: 0.1390
step 64410; step_loss: 0.1727
step 64420; step_loss: 0.1587
step 64430; step_loss: 0.1586
step 64440; step_loss: 0.2435
step 64450; step_loss: 0.1332
step 64460; step_loss: 0.1410
step 64470; step_loss: 0.1070
step 64480; step_loss: 0.1640
step 64490; step_loss: 0.1334
step 64500; step_loss: 0.1322
step 64510; step_loss: 0.1449
step 64520; step_loss: 0.1280
step 64530; step_loss: 0.1068
step 64540; step_loss: 0.1356
step 64550; step_loss: 0.1347
step 64560; step_loss: 0.1951
step 64570; step_loss: 0.1223
step 64580; step_loss: 0.1074
step 64590; step_loss: 0.1147
step 64600; step_loss: 0.1112
step 64610; step_loss: 0.1056
step 64620; step_loss: 0.1447
step 64630; step_loss: 0.1238
step 64640; step_loss: 0.1350
step 64650; step_loss: 0.1368
step 64660; step_loss: 0.1676
step 64670; step_loss: 0.1132
step 64680; step_loss: 0.1155
step 64690; step_loss: 0.1184
step 64700; step_loss: 0.1464
step 64710; step_loss: 0.1210
step 64720; step_loss: 0.1480
step 64730; step_loss: 0.1272
step 64740; step_loss: 0.1312
step 64750; step_loss: 0.1160
step 64760; step_loss: 0.1252
step 64770; step_loss: 0.1283
step 64780; step_loss: 0.1030
step 64790; step_loss: 0.1576
step 64800; step_loss: 0.1097
step 64810; step_loss: 0.1226
step 64820; step_loss: 0.1514
step 64830; step_loss: 0.1548
step 64840; step_loss: 0.1458
step 64850; step_loss: 0.1139
step 64860; step_loss: 0.1500
step 64870; step_loss: 0.1247
step 64880; step_loss: 0.1292
step 64890; step_loss: 0.1489
step 64900; step_loss: 0.1475
step 64910; step_loss: 0.1259
step 64920; step_loss: 0.1497
step 64930; step_loss: 0.1641
step 64940; step_loss: 0.1278
step 64950; step_loss: 0.1517
step 64960; step_loss: 0.1294
step 64970; step_loss: 0.1125
step 64980; step_loss: 0.1099
step 64990; step_loss: 0.1367

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 0.988 | 1.131 | 1.402 | 1.509 | 1.679 | 1.918 |

============================
Global step:         65000
Learning rate:       0.0037
Step-time (ms):     35.7927
Train loss avg:      0.1353
--------------------------
Val loss:            0.9050
srnn loss:           0.7621
============================

Saving the model...
done in 404.62 ms
step 65000; step_loss: 0.0706
step 65010; step_loss: 0.1126
step 65020; step_loss: 0.1359
step 65030; step_loss: 0.1695
step 65040; step_loss: 0.1487
step 65050; step_loss: 0.1179
step 65060; step_loss: 0.2334
step 65070; step_loss: 0.1342
step 65080; step_loss: 0.1147
step 65090; step_loss: 0.1295
step 65100; step_loss: 0.1087
step 65110; step_loss: 0.1364
step 65120; step_loss: 0.1268
step 65130; step_loss: 0.1748
step 65140; step_loss: 0.1270
step 65150; step_loss: 0.1104
step 65160; step_loss: 0.1478
step 65170; step_loss: 0.1427
step 65180; step_loss: 0.1097
step 65190; step_loss: 0.1523
step 65200; step_loss: 0.1135
step 65210; step_loss: 0.1272
step 65220; step_loss: 0.1317
step 65230; step_loss: 0.1142
step 65240; step_loss: 0.1687
step 65250; step_loss: 0.1722
step 65260; step_loss: 0.1928
step 65270; step_loss: 0.0941
step 65280; step_loss: 0.1136
step 65290; step_loss: 0.1210
step 65300; step_loss: 0.1347
step 65310; step_loss: 0.1274
step 65320; step_loss: 0.1135
step 65330; step_loss: 0.0958
step 65340; step_loss: 0.1223
step 65350; step_loss: 0.1201
step 65360; step_loss: 0.1615
step 65370; step_loss: 0.1267
step 65380; step_loss: 0.1046
step 65390; step_loss: 0.1317
step 65400; step_loss: 0.1178
step 65410; step_loss: 0.1351
step 65420; step_loss: 0.1553
step 65430; step_loss: 0.1545
step 65440; step_loss: 0.1783
step 65450; step_loss: 0.1047
step 65460; step_loss: 0.1638
step 65470; step_loss: 0.1801
step 65480; step_loss: 0.0998
step 65490; step_loss: 0.1036
step 65500; step_loss: 0.1039
step 65510; step_loss: 0.1210
step 65520; step_loss: 0.1365
step 65530; step_loss: 0.1198
step 65540; step_loss: 0.1865
step 65550; step_loss: 0.1081
step 65560; step_loss: 0.1413
step 65570; step_loss: 0.1241
step 65580; step_loss: 0.1434
step 65590; step_loss: 0.0967
step 65600; step_loss: 0.2128
step 65610; step_loss: 0.1270
step 65620; step_loss: 0.1351
step 65630; step_loss: 0.1203
step 65640; step_loss: 0.1468
step 65650; step_loss: 0.1746
step 65660; step_loss: 0.1336
step 65670; step_loss: 0.1058
step 65680; step_loss: 0.1514
step 65690; step_loss: 0.1202
step 65700; step_loss: 0.0852
step 65710; step_loss: 0.1702
step 65720; step_loss: 0.1611
step 65730; step_loss: 0.1278
step 65740; step_loss: 0.1472
step 65750; step_loss: 0.1310
step 65760; step_loss: 0.1938
step 65770; step_loss: 0.1094
step 65780; step_loss: 0.1382
step 65790; step_loss: 0.0936
step 65800; step_loss: 0.1414
step 65810; step_loss: 0.1493
step 65820; step_loss: 0.1297
step 65830; step_loss: 0.1493
step 65840; step_loss: 0.1463
step 65850; step_loss: 0.1637
step 65860; step_loss: 0.1512
step 65870; step_loss: 0.1053
step 65880; step_loss: 0.1642
step 65890; step_loss: 0.1350
step 65900; step_loss: 0.1706
step 65910; step_loss: 0.0852
step 65920; step_loss: 0.1322
step 65930; step_loss: 0.1136
step 65940; step_loss: 0.0872
step 65950; step_loss: 0.1052
step 65960; step_loss: 0.1725
step 65970; step_loss: 0.1097
step 65980; step_loss: 0.1433
step 65990; step_loss: 0.2094

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 0.977 | 1.116 | 1.379 | 1.482 | 1.646 | 1.895 |

============================
Global step:         66000
Learning rate:       0.0037
Step-time (ms):     35.8297
Train loss avg:      0.1348
--------------------------
Val loss:            0.8498
srnn loss:           0.7428
============================

Saving the model...
done in 405.30 ms
step 66000; step_loss: 0.2190
step 66010; step_loss: 0.1011
step 66020; step_loss: 0.1826
step 66030; step_loss: 0.1399
step 66040; step_loss: 0.1453
step 66050; step_loss: 0.1943
step 66060; step_loss: 0.1894
step 66070; step_loss: 0.1352
step 66080; step_loss: 0.1734
step 66090; step_loss: 0.1068
step 66100; step_loss: 0.1122
step 66110; step_loss: 0.1697
step 66120; step_loss: 0.0914
step 66130; step_loss: 0.1296
step 66140; step_loss: 0.1544
step 66150; step_loss: 0.1838
step 66160; step_loss: 0.0991
step 66170; step_loss: 0.2160
step 66180; step_loss: 0.1971
step 66190; step_loss: 0.1040
step 66200; step_loss: 0.1140
step 66210; step_loss: 0.1337
step 66220; step_loss: 0.1412
step 66230; step_loss: 0.1353
step 66240; step_loss: 0.1601
step 66250; step_loss: 0.1345
step 66260; step_loss: 0.1588
step 66270; step_loss: 0.1270
step 66280; step_loss: 0.1039
step 66290; step_loss: 0.1228
step 66300; step_loss: 0.1014
step 66310; step_loss: 0.1720
step 66320; step_loss: 0.1672
step 66330; step_loss: 0.0892
step 66340; step_loss: 0.1234
step 66350; step_loss: 0.1404
step 66360; step_loss: 0.1263
step 66370; step_loss: 0.1244
step 66380; step_loss: 0.1948
step 66390; step_loss: 0.0923
step 66400; step_loss: 0.0856
step 66410; step_loss: 0.1022
step 66420; step_loss: 0.1657
step 66430; step_loss: 0.1219
step 66440; step_loss: 0.1144
step 66450; step_loss: 0.1050
step 66460; step_loss: 0.1937
step 66470; step_loss: 0.1952
step 66480; step_loss: 0.1481
step 66490; step_loss: 0.1089
step 66500; step_loss: 0.2054
step 66510; step_loss: 0.1389
step 66520; step_loss: 0.1585
step 66530; step_loss: 0.1256
step 66540; step_loss: 0.1664
step 66550; step_loss: 0.1181
step 66560; step_loss: 0.1142
step 66570; step_loss: 0.1183
step 66580; step_loss: 0.1356
step 66590; step_loss: 0.1179
step 66600; step_loss: 0.0952
step 66610; step_loss: 0.1364
step 66620; step_loss: 0.1288
step 66630; step_loss: 0.1235
step 66640; step_loss: 0.1187
step 66650; step_loss: 0.1486
step 66660; step_loss: 0.1434
step 66670; step_loss: 0.1250
step 66680; step_loss: 0.1173
step 66690; step_loss: 0.1278
step 66700; step_loss: 0.1485
step 66710; step_loss: 0.1462
step 66720; step_loss: 0.1383
step 66730; step_loss: 0.1222
step 66740; step_loss: 0.1115
step 66750; step_loss: 0.1466
step 66760; step_loss: 0.2137
step 66770; step_loss: 0.1274
step 66780; step_loss: 0.1000
step 66790; step_loss: 0.1283
step 66800; step_loss: 0.1312
step 66810; step_loss: 0.1332
step 66820; step_loss: 0.1504
step 66830; step_loss: 0.1356
step 66840; step_loss: 0.1543
step 66850; step_loss: 0.1661
step 66860; step_loss: 0.1216
step 66870; step_loss: 0.1318
step 66880; step_loss: 0.1797
step 66890; step_loss: 0.1173
step 66900; step_loss: 0.1154
step 66910; step_loss: 0.1226
step 66920; step_loss: 0.1737
step 66930; step_loss: 0.1459
step 66940; step_loss: 0.0748
step 66950; step_loss: 0.1081
step 66960; step_loss: 0.0958
step 66970; step_loss: 0.1582
step 66980; step_loss: 0.1353
step 66990; step_loss: 0.1067

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 0.982 | 1.127 | 1.398 | 1.505 | 1.676 | 1.929 |

============================
Global step:         67000
Learning rate:       0.0037
Step-time (ms):     35.8557
Train loss avg:      0.1340
--------------------------
Val loss:            0.8033
srnn loss:           0.7537
============================

Saving the model...
done in 391.26 ms
step 67000; step_loss: 0.1246
step 67010; step_loss: 0.1525
step 67020; step_loss: 0.1170
step 67030; step_loss: 0.1479
step 67040; step_loss: 0.1464
step 67050; step_loss: 0.1335
step 67060; step_loss: 0.1267
step 67070; step_loss: 0.1277
step 67080; step_loss: 0.1799
step 67090; step_loss: 0.1446
step 67100; step_loss: 0.1092
step 67110; step_loss: 0.1192
step 67120; step_loss: 0.1150
step 67130; step_loss: 0.1237
step 67140; step_loss: 0.1587
step 67150; step_loss: 0.0853
step 67160; step_loss: 0.1116
step 67170; step_loss: 0.1339
step 67180; step_loss: 0.1134
step 67190; step_loss: 0.1177
step 67200; step_loss: 0.1429
step 67210; step_loss: 0.1209
step 67220; step_loss: 0.1009
step 67230; step_loss: 0.1454
step 67240; step_loss: 0.1786
step 67250; step_loss: 0.1367
step 67260; step_loss: 0.1230
step 67270; step_loss: 0.1238
step 67280; step_loss: 0.1460
step 67290; step_loss: 0.1047
step 67300; step_loss: 0.1116
step 67310; step_loss: 0.1333
step 67320; step_loss: 0.1346
step 67330; step_loss: 0.1420
step 67340; step_loss: 0.1536
step 67350; step_loss: 0.1185
step 67360; step_loss: 0.1269
step 67370; step_loss: 0.1081
step 67380; step_loss: 0.1510
step 67390; step_loss: 0.1187
step 67400; step_loss: 0.1016
step 67410; step_loss: 0.1431
step 67420; step_loss: 0.1334
step 67430; step_loss: 0.2099
step 67440; step_loss: 0.2584
step 67450; step_loss: 0.1693
step 67460; step_loss: 0.1394
step 67470; step_loss: 0.1346
step 67480; step_loss: 0.1405
step 67490; step_loss: 0.1372
step 67500; step_loss: 0.1263
step 67510; step_loss: 0.1169
step 67520; step_loss: 0.1237
step 67530; step_loss: 0.1425
step 67540; step_loss: 0.1520
step 67550; step_loss: 0.1463
step 67560; step_loss: 0.1367
step 67570; step_loss: 0.1319
step 67580; step_loss: 0.1523
step 67590; step_loss: 0.1368
step 67600; step_loss: 0.1382
step 67610; step_loss: 0.1509
step 67620; step_loss: 0.1334
step 67630; step_loss: 0.1008
step 67640; step_loss: 0.1281
step 67650; step_loss: 0.1346
step 67660; step_loss: 0.1474
step 67670; step_loss: 0.1236
step 67680; step_loss: 0.1067
step 67690; step_loss: 0.1307
step 67700; step_loss: 0.1261
step 67710; step_loss: 0.1078
step 67720; step_loss: 0.1227
step 67730; step_loss: 0.1072
step 67740; step_loss: 0.1063
step 67750; step_loss: 0.1190
step 67760; step_loss: 0.1335
step 67770; step_loss: 0.1178
step 67780; step_loss: 0.1203
step 67790; step_loss: 0.1140
step 67800; step_loss: 0.0916
step 67810; step_loss: 0.1311
step 67820; step_loss: 0.1133
step 67830; step_loss: 0.1132
step 67840; step_loss: 0.1515
step 67850; step_loss: 0.1262
step 67860; step_loss: 0.1432
step 67870; step_loss: 0.0976
step 67880; step_loss: 0.1123
step 67890; step_loss: 0.1421
step 67900; step_loss: 0.1234
step 67910; step_loss: 0.1388
step 67920; step_loss: 0.1089
step 67930; step_loss: 0.1180
step 67940; step_loss: 0.1910
step 67950; step_loss: 0.1567
step 67960; step_loss: 0.1259
step 67970; step_loss: 0.1686
step 67980; step_loss: 0.1354
step 67990; step_loss: 0.1302

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 0.978 | 1.124 | 1.398 | 1.507 | 1.676 | 1.940 |

============================
Global step:         68000
Learning rate:       0.0037
Step-time (ms):     35.9233
Train loss avg:      0.1326
--------------------------
Val loss:            0.9081
srnn loss:           0.7605
============================

Saving the model...
done in 392.82 ms
step 68000; step_loss: 0.1151
step 68010; step_loss: 0.0905
step 68020; step_loss: 0.1315
step 68030; step_loss: 0.1276
step 68040; step_loss: 0.1261
step 68050; step_loss: 0.1115
step 68060; step_loss: 0.1180
step 68070; step_loss: 0.1131
step 68080; step_loss: 0.1015
step 68090; step_loss: 0.1333
step 68100; step_loss: 0.1569
step 68110; step_loss: 0.1254
step 68120; step_loss: 0.1390
step 68130; step_loss: 0.1115
step 68140; step_loss: 0.1312
step 68150; step_loss: 0.1036
step 68160; step_loss: 0.1636
step 68170; step_loss: 0.1517
step 68180; step_loss: 0.1357
step 68190; step_loss: 0.1768
step 68200; step_loss: 0.1347
step 68210; step_loss: 0.1262
step 68220; step_loss: 0.1219
step 68230; step_loss: 0.1191
step 68240; step_loss: 0.1057
step 68250; step_loss: 0.1295
step 68260; step_loss: 0.1439
step 68270; step_loss: 0.0968
step 68280; step_loss: 0.1039
step 68290; step_loss: 0.1398
step 68300; step_loss: 0.1581
step 68310; step_loss: 0.1361
step 68320; step_loss: 0.0987
step 68330; step_loss: 0.1539
step 68340; step_loss: 0.1364
step 68350; step_loss: 0.1268
step 68360; step_loss: 0.0903
step 68370; step_loss: 0.1961
step 68380; step_loss: 0.1515
step 68390; step_loss: 0.1101
step 68400; step_loss: 0.1066
step 68410; step_loss: 0.1156
step 68420; step_loss: 0.1140
step 68430; step_loss: 0.1490
step 68440; step_loss: 0.0984
step 68450; step_loss: 0.1417
step 68460; step_loss: 0.1198
step 68470; step_loss: 0.1474
step 68480; step_loss: 0.1153
step 68490; step_loss: 0.2243
step 68500; step_loss: 0.1556
step 68510; step_loss: 0.1508
step 68520; step_loss: 0.1481
step 68530; step_loss: 0.0998
step 68540; step_loss: 0.1060
step 68550; step_loss: 0.0874
step 68560; step_loss: 0.0984
step 68570; step_loss: 0.1228
step 68580; step_loss: 0.2025
step 68590; step_loss: 0.1254
step 68600; step_loss: 0.1079
step 68610; step_loss: 0.1122
step 68620; step_loss: 0.1194
step 68630; step_loss: 0.1350
step 68640; step_loss: 0.1940
step 68650; step_loss: 0.1115
step 68660; step_loss: 0.1183
step 68670; step_loss: 0.1419
step 68680; step_loss: 0.2088
step 68690; step_loss: 0.1391
step 68700; step_loss: 0.1260
step 68710; step_loss: 0.1188
step 68720; step_loss: 0.0919
step 68730; step_loss: 0.0888
step 68740; step_loss: 0.1515
step 68750; step_loss: 0.1242
step 68760; step_loss: 0.1231
step 68770; step_loss: 0.1513
step 68780; step_loss: 0.1143
step 68790; step_loss: 0.1153
step 68800; step_loss: 0.1096
step 68810; step_loss: 0.1514
step 68820; step_loss: 0.1273
step 68830; step_loss: 0.1322
step 68840; step_loss: 0.0948
step 68850; step_loss: 0.1151
step 68860; step_loss: 0.1443
step 68870; step_loss: 0.1366
step 68880; step_loss: 0.1233
step 68890; step_loss: 0.1214
step 68900; step_loss: 0.0993
step 68910; step_loss: 0.1376
step 68920; step_loss: 0.1411
step 68930; step_loss: 0.1345
step 68940; step_loss: 0.1092
step 68950; step_loss: 0.1155
step 68960; step_loss: 0.1493
step 68970; step_loss: 0.1001
step 68980; step_loss: 0.1871
step 68990; step_loss: 0.1690

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 0.978 | 1.124 | 1.401 | 1.511 | 1.687 | 1.943 |

============================
Global step:         69000
Learning rate:       0.0037
Step-time (ms):     35.8977
Train loss avg:      0.1327
--------------------------
Val loss:            0.8870
srnn loss:           0.7666
============================

Saving the model...
done in 395.27 ms
step 69000; step_loss: 0.1343
step 69010; step_loss: 0.1682
step 69020; step_loss: 0.1375
step 69030; step_loss: 0.1298
step 69040; step_loss: 0.1004
step 69050; step_loss: 0.1137
step 69060; step_loss: 0.1090
step 69070; step_loss: 0.0924
step 69080; step_loss: 0.1361
step 69090; step_loss: 0.1297
step 69100; step_loss: 0.1100
step 69110; step_loss: 0.1152
step 69120; step_loss: 0.1111
step 69130; step_loss: 0.1514
step 69140; step_loss: 0.1627
step 69150; step_loss: 0.1184
step 69160; step_loss: 0.1099
step 69170; step_loss: 0.1465
step 69180; step_loss: 0.1495
step 69190; step_loss: 0.1186
step 69200; step_loss: 0.1197
step 69210; step_loss: 0.1609
step 69220; step_loss: 0.1490
step 69230; step_loss: 0.1172
step 69240; step_loss: 0.0852
step 69250; step_loss: 0.1071
step 69260; step_loss: 0.1255
step 69270; step_loss: 0.1836
step 69280; step_loss: 0.1342
step 69290; step_loss: 0.1729
step 69300; step_loss: 0.1296
step 69310; step_loss: 0.1660
step 69320; step_loss: 0.1503
step 69330; step_loss: 0.1524
step 69340; step_loss: 0.1027
step 69350; step_loss: 0.1130
step 69360; step_loss: 0.1941
step 69370; step_loss: 0.1708
step 69380; step_loss: 0.1414
step 69390; step_loss: 0.1126
step 69400; step_loss: 0.1581
step 69410; step_loss: 0.1329
step 69420; step_loss: 0.1287
step 69430; step_loss: 0.1242
step 69440; step_loss: 0.1895
step 69450; step_loss: 0.1069
step 69460; step_loss: 0.1340
step 69470; step_loss: 0.1433
step 69480; step_loss: 0.1391
step 69490; step_loss: 0.1526
step 69500; step_loss: 0.1166
step 69510; step_loss: 0.1349
step 69520; step_loss: 0.1215
step 69530; step_loss: 0.1228
step 69540; step_loss: 0.1067
step 69550; step_loss: 0.1494
step 69560; step_loss: 0.1342
step 69570; step_loss: 0.1149
step 69580; step_loss: 0.1132
step 69590; step_loss: 0.1187
step 69600; step_loss: 0.1313
step 69610; step_loss: 0.1505
step 69620; step_loss: 0.1222
step 69630; step_loss: 0.1124
step 69640; step_loss: 0.1320
step 69650; step_loss: 0.1172
step 69660; step_loss: 0.1077
step 69670; step_loss: 0.1224
step 69680; step_loss: 0.1377
step 69690; step_loss: 0.1027
step 69700; step_loss: 0.1240
step 69710; step_loss: 0.1020
step 69720; step_loss: 0.0867
step 69730; step_loss: 0.0836
step 69740; step_loss: 0.1155
step 69750; step_loss: 0.1046
step 69760; step_loss: 0.1292
step 69770; step_loss: 0.1333
step 69780; step_loss: 0.1136
step 69790; step_loss: 0.1454
step 69800; step_loss: 0.1407
step 69810; step_loss: 0.1036
step 69820; step_loss: 0.1091
step 69830; step_loss: 0.0828
step 69840; step_loss: 0.1459
step 69850; step_loss: 0.1668
step 69860; step_loss: 0.1058
step 69870; step_loss: 0.0974
step 69880; step_loss: 0.1342
step 69890; step_loss: 0.1574
step 69900; step_loss: 0.1726
step 69910; step_loss: 0.1313
step 69920; step_loss: 0.1149
step 69930; step_loss: 0.1085
step 69940; step_loss: 0.1351
step 69950; step_loss: 0.1173
step 69960; step_loss: 0.1006
step 69970; step_loss: 0.1147
step 69980; step_loss: 0.1161
step 69990; step_loss: 0.1338

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 0.974 | 1.121 | 1.395 | 1.502 | 1.671 | 1.913 |

============================
Global step:         70000
Learning rate:       0.0035
Step-time (ms):     35.9917
Train loss avg:      0.1285
--------------------------
Val loss:            0.7786
srnn loss:           0.7576
============================

Saving the model...
done in 403.39 ms
step 70000; step_loss: 0.0920
step 70010; step_loss: 0.1190
step 70020; step_loss: 0.1263
step 70030; step_loss: 0.1188
step 70040; step_loss: 0.1154
step 70050; step_loss: 0.1297
step 70060; step_loss: 0.1090
step 70070; step_loss: 0.1478
step 70080; step_loss: 0.1036
step 70090; step_loss: 0.1261
step 70100; step_loss: 0.1497
step 70110; step_loss: 0.1281
step 70120; step_loss: 0.1169
step 70130; step_loss: 0.1045
step 70140; step_loss: 0.1425
step 70150; step_loss: 0.1267
step 70160; step_loss: 0.1007
step 70170; step_loss: 0.1342
step 70180; step_loss: 0.1533
step 70190; step_loss: 0.0971
step 70200; step_loss: 0.1399
step 70210; step_loss: 0.1174
step 70220; step_loss: 0.1450
step 70230; step_loss: 0.0971
step 70240; step_loss: 0.1149
step 70250; step_loss: 0.1082
step 70260; step_loss: 0.0940
step 70270; step_loss: 0.1482
step 70280; step_loss: 0.1216
step 70290; step_loss: 0.1208
step 70300; step_loss: 0.1301
step 70310; step_loss: 0.1454
step 70320; step_loss: 0.1045
step 70330; step_loss: 0.1164
step 70340; step_loss: 0.1077
step 70350; step_loss: 0.1417
step 70360; step_loss: 0.1391
step 70370; step_loss: 0.1432
step 70380; step_loss: 0.1211
step 70390; step_loss: 0.1098
step 70400; step_loss: 0.1505
step 70410; step_loss: 0.1423
step 70420; step_loss: 0.1558
step 70430; step_loss: 0.1606
step 70440; step_loss: 0.1451
step 70450; step_loss: 0.1041
step 70460; step_loss: 0.1495
step 70470; step_loss: 0.1087
step 70480; step_loss: 0.1227
step 70490; step_loss: 0.1504
step 70500; step_loss: 0.1305
step 70510; step_loss: 0.1659
step 70520; step_loss: 0.1277
step 70530; step_loss: 0.0841
step 70540; step_loss: 0.1467
step 70550; step_loss: 0.1002
step 70560; step_loss: 0.2090
step 70570; step_loss: 0.1577
step 70580; step_loss: 0.1326
step 70590; step_loss: 0.0959
step 70600; step_loss: 0.1444
step 70610; step_loss: 0.1250
step 70620; step_loss: 0.1401
step 70630; step_loss: 0.1165
step 70640; step_loss: 0.1601
step 70650; step_loss: 0.1113
step 70660; step_loss: 0.0889
step 70670; step_loss: 0.1264
step 70680; step_loss: 0.1170
step 70690; step_loss: 0.1092
step 70700; step_loss: 0.1038
step 70710; step_loss: 0.1136
step 70720; step_loss: 0.1317
step 70730; step_loss: 0.1275
step 70740; step_loss: 0.1678
step 70750; step_loss: 0.1280
step 70760; step_loss: 0.1044
step 70770; step_loss: 0.1372
step 70780; step_loss: 0.1425
step 70790; step_loss: 0.0987
step 70800; step_loss: 0.1278
step 70810; step_loss: 0.0937
step 70820; step_loss: 0.1664
step 70830; step_loss: 0.1409
step 70840; step_loss: 0.1818
step 70850; step_loss: 0.0989
step 70860; step_loss: 0.1349
step 70870; step_loss: 0.1586
step 70880; step_loss: 0.1071
step 70890; step_loss: 0.1313
step 70900; step_loss: 0.1421
step 70910; step_loss: 0.1321
step 70920; step_loss: 0.0993
step 70930; step_loss: 0.1797
step 70940; step_loss: 0.1486
step 70950; step_loss: 0.1489
step 70960; step_loss: 0.0876
step 70970; step_loss: 0.1016
step 70980; step_loss: 0.1201
step 70990; step_loss: 0.0972

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 0.971 | 1.116 | 1.391 | 1.500 | 1.669 | 1.918 |

============================
Global step:         71000
Learning rate:       0.0035
Step-time (ms):     35.8319
Train loss avg:      0.1280
--------------------------
Val loss:            0.8798
srnn loss:           0.7572
============================

Saving the model...
done in 409.77 ms
step 71000; step_loss: 0.1310
step 71010; step_loss: 0.1281
step 71020; step_loss: 0.1103
step 71030; step_loss: 0.1527
step 71040; step_loss: 0.1247
step 71050; step_loss: 0.1431
step 71060; step_loss: 0.1372
step 71070; step_loss: 0.1150
step 71080; step_loss: 0.1376
step 71090; step_loss: 0.0855
step 71100; step_loss: 0.1034
step 71110; step_loss: 0.1887
step 71120; step_loss: 0.0836
step 71130; step_loss: 0.1129
step 71140; step_loss: 0.1320
step 71150; step_loss: 0.1300
step 71160; step_loss: 0.1050
step 71170; step_loss: 0.1682
step 71180; step_loss: 0.1533
step 71190; step_loss: 0.1565
step 71200; step_loss: 0.1493
step 71210; step_loss: 0.1571
step 71220; step_loss: 0.1278
step 71230; step_loss: 0.1006
step 71240; step_loss: 0.1658
step 71250; step_loss: 0.1031
step 71260; step_loss: 0.0960
step 71270; step_loss: 0.1653
step 71280; step_loss: 0.1130
step 71290; step_loss: 0.1445
step 71300; step_loss: 0.1686
step 71310; step_loss: 0.1106
step 71320; step_loss: 0.0993
step 71330; step_loss: 0.1633
step 71340; step_loss: 0.1657
step 71350; step_loss: 0.1865
step 71360; step_loss: 0.1544
step 71370; step_loss: 0.1200
step 71380; step_loss: 0.1153
step 71390; step_loss: 0.1219
step 71400; step_loss: 0.0873
step 71410; step_loss: 0.1929
step 71420; step_loss: 0.1032
step 71430; step_loss: 0.1246
step 71440; step_loss: 0.1045
step 71450; step_loss: 0.1391
step 71460; step_loss: 0.1356
step 71470; step_loss: 0.0847
step 71480; step_loss: 0.1513
step 71490; step_loss: 0.1236
step 71500; step_loss: 0.1465
step 71510; step_loss: 0.1213
step 71520; step_loss: 0.1100
step 71530; step_loss: 0.1284
step 71540; step_loss: 0.1333
step 71550; step_loss: 0.1667
step 71560; step_loss: 0.1213
step 71570; step_loss: 0.1381
step 71580; step_loss: 0.1468
step 71590; step_loss: 0.1360
step 71600; step_loss: 0.1454
step 71610; step_loss: 0.1145
step 71620; step_loss: 0.1216
step 71630; step_loss: 0.1346
step 71640; step_loss: 0.1645
step 71650; step_loss: 0.1574
step 71660; step_loss: 0.1217
step 71670; step_loss: 0.1438
step 71680; step_loss: 0.1414
step 71690; step_loss: 0.1306
step 71700; step_loss: 0.1164
step 71710; step_loss: 0.1167
step 71720; step_loss: 0.1169
step 71730; step_loss: 0.1350
step 71740; step_loss: 0.1277
step 71750; step_loss: 0.1182
step 71760; step_loss: 0.1189
step 71770; step_loss: 0.0924
step 71780; step_loss: 0.1217
step 71790; step_loss: 0.1213
step 71800; step_loss: 0.0859
step 71810; step_loss: 0.1008
step 71820; step_loss: 0.1187
step 71830; step_loss: 0.1148
step 71840; step_loss: 0.1275
step 71850; step_loss: 0.0974
step 71860; step_loss: 0.1433
step 71870; step_loss: 0.0997
step 71880; step_loss: 0.1283
step 71890; step_loss: 0.1340
step 71900; step_loss: 0.1574
step 71910; step_loss: 0.1497
step 71920; step_loss: 0.1033
step 71930; step_loss: 0.1170
step 71940; step_loss: 0.1329
step 71950; step_loss: 0.1281
step 71960; step_loss: 0.1227
step 71970; step_loss: 0.1239
step 71980; step_loss: 0.1717
step 71990; step_loss: 0.1243

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 0.975 | 1.125 | 1.409 | 1.520 | 1.695 | 1.942 |

============================
Global step:         72000
Learning rate:       0.0035
Step-time (ms):     35.8002
Train loss avg:      0.1296
--------------------------
Val loss:            0.7855
srnn loss:           0.7708
============================

Saving the model...
done in 387.37 ms
step 72000; step_loss: 0.1006
step 72010; step_loss: 0.0883
step 72020; step_loss: 0.0988
step 72030; step_loss: 0.1406
step 72040; step_loss: 0.1872
step 72050; step_loss: 0.0914
step 72060; step_loss: 0.1195
step 72070; step_loss: 0.1243
step 72080; step_loss: 0.1389
step 72090; step_loss: 0.1416
step 72100; step_loss: 0.1582
step 72110; step_loss: 0.1162
step 72120; step_loss: 0.1544
step 72130; step_loss: 0.1160
step 72140; step_loss: 0.0882
step 72150; step_loss: 0.1903
step 72160; step_loss: 0.1311
step 72170; step_loss: 0.1214
step 72180; step_loss: 0.1087
step 72190; step_loss: 0.0914
step 72200; step_loss: 0.1134
step 72210; step_loss: 0.0982
step 72220; step_loss: 0.1117
step 72230; step_loss: 0.1208
step 72240; step_loss: 0.0989
step 72250; step_loss: 0.1208
step 72260; step_loss: 0.1665
step 72270; step_loss: 0.1507
step 72280; step_loss: 0.1735
step 72290; step_loss: 0.1407
step 72300; step_loss: 0.1081
step 72310; step_loss: 0.1423
step 72320; step_loss: 0.1083
step 72330; step_loss: 0.1379
step 72340; step_loss: 0.1156
step 72350; step_loss: 0.1323
step 72360; step_loss: 0.1272
step 72370; step_loss: 0.1279
step 72380; step_loss: 0.1378
step 72390; step_loss: 0.1296
step 72400; step_loss: 0.0967
step 72410; step_loss: 0.0938
step 72420; step_loss: 0.1666
step 72430; step_loss: 0.0743
step 72440; step_loss: 0.1515
step 72450; step_loss: 0.0949
step 72460; step_loss: 0.1577
step 72470; step_loss: 0.1237
step 72480; step_loss: 0.1339
step 72490; step_loss: 0.1626
step 72500; step_loss: 0.1493
step 72510; step_loss: 0.1374
step 72520; step_loss: 0.0918
step 72530; step_loss: 0.1514
step 72540; step_loss: 0.1731
step 72550; step_loss: 0.1243
step 72560; step_loss: 0.1501
step 72570; step_loss: 0.1304
step 72580; step_loss: 0.1800
step 72590; step_loss: 0.1238
step 72600; step_loss: 0.0913
step 72610; step_loss: 0.0959
step 72620; step_loss: 0.1654
step 72630; step_loss: 0.1500
step 72640; step_loss: 0.1096
step 72650; step_loss: 0.1399
step 72660; step_loss: 0.1240
step 72670; step_loss: 0.1133
step 72680; step_loss: 0.1007
step 72690; step_loss: 0.1208
step 72700; step_loss: 0.1429
step 72710; step_loss: 0.1131
step 72720; step_loss: 0.1164
step 72730; step_loss: 0.1529
step 72740; step_loss: 0.1241
step 72750; step_loss: 0.1333
step 72760; step_loss: 0.1534
step 72770; step_loss: 0.1794
step 72780; step_loss: 0.1305
step 72790; step_loss: 0.1464
step 72800; step_loss: 0.1669
step 72810; step_loss: 0.1073
step 72820; step_loss: 0.1326
step 72830; step_loss: 0.1852
step 72840; step_loss: 0.1029
step 72850; step_loss: 0.1327
step 72860; step_loss: 0.1622
step 72870; step_loss: 0.1372
step 72880; step_loss: 0.1307
step 72890; step_loss: 0.1074
step 72900; step_loss: 0.1405
step 72910; step_loss: 0.1211
step 72920; step_loss: 0.1361
step 72930; step_loss: 0.1230
step 72940; step_loss: 0.1575
step 72950; step_loss: 0.0999
step 72960; step_loss: 0.1525
step 72970; step_loss: 0.1173
step 72980; step_loss: 0.0978
step 72990; step_loss: 0.0998

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 0.978 | 1.132 | 1.419 | 1.533 | 1.712 | 1.976 |

============================
Global step:         73000
Learning rate:       0.0035
Step-time (ms):     35.7428
Train loss avg:      0.1280
--------------------------
Val loss:            0.8786
srnn loss:           0.7825
============================

Saving the model...
done in 396.67 ms
step 73000; step_loss: 0.1249
step 73010; step_loss: 0.1258
step 73020; step_loss: 0.1159
step 73030; step_loss: 0.1784
step 73040; step_loss: 0.1206
step 73050; step_loss: 0.1495
step 73060; step_loss: 0.1106
step 73070; step_loss: 0.1606
step 73080; step_loss: 0.0868
step 73090; step_loss: 0.1093
step 73100; step_loss: 0.1033
step 73110; step_loss: 0.1206
step 73120; step_loss: 0.1227
step 73130; step_loss: 0.1061
step 73140; step_loss: 0.1493
step 73150; step_loss: 0.1609
step 73160; step_loss: 0.1230
step 73170; step_loss: 0.1403
step 73180; step_loss: 0.1096
step 73190; step_loss: 0.1756
step 73200; step_loss: 0.1403
step 73210; step_loss: 0.1339
step 73220; step_loss: 0.1101
step 73230; step_loss: 0.1597
step 73240; step_loss: 0.1366
step 73250; step_loss: 0.1629
step 73260; step_loss: 0.1038
step 73270; step_loss: 0.1271
step 73280; step_loss: 0.0773
step 73290; step_loss: 0.1165
step 73300; step_loss: 0.1671
step 73310; step_loss: 0.1042
step 73320; step_loss: 0.1200
step 73330; step_loss: 0.2098
step 73340; step_loss: 0.0897
step 73350; step_loss: 0.1284
step 73360; step_loss: 0.1744
step 73370; step_loss: 0.1176
step 73380; step_loss: 0.1040
step 73390; step_loss: 0.1247
step 73400; step_loss: 0.1099
step 73410; step_loss: 0.1093
step 73420; step_loss: 0.1199
step 73430; step_loss: 0.1019
step 73440; step_loss: 0.1299
step 73450; step_loss: 0.1277
step 73460; step_loss: 0.1292
step 73470; step_loss: 0.1167
step 73480; step_loss: 0.0880
step 73490; step_loss: 0.0988
step 73500; step_loss: 0.0961
step 73510; step_loss: 0.1379
step 73520; step_loss: 0.1686
step 73530; step_loss: 0.1548
step 73540; step_loss: 0.1509
step 73550; step_loss: 0.1355
step 73560; step_loss: 0.1242
step 73570; step_loss: 0.1244
step 73580; step_loss: 0.1095
step 73590; step_loss: 0.1015
step 73600; step_loss: 0.1328
step 73610; step_loss: 0.1285
step 73620; step_loss: 0.0956
step 73630; step_loss: 0.1405
step 73640; step_loss: 0.1106
step 73650; step_loss: 0.1576
step 73660; step_loss: 0.1133
step 73670; step_loss: 0.1356
step 73680; step_loss: 0.1355
step 73690; step_loss: 0.1297
step 73700; step_loss: 0.1750
step 73710; step_loss: 0.1321
step 73720; step_loss: 0.1043
step 73730; step_loss: 0.1229
step 73740; step_loss: 0.1065
step 73750; step_loss: 0.1255
step 73760; step_loss: 0.1261
step 73770; step_loss: 0.1360
step 73780; step_loss: 0.1094
step 73790; step_loss: 0.1181
step 73800; step_loss: 0.1068
step 73810; step_loss: 0.1419
step 73820; step_loss: 0.1112
step 73830; step_loss: 0.0843
step 73840; step_loss: 0.1627
step 73850; step_loss: 0.1165
step 73860; step_loss: 0.1177
step 73870; step_loss: 0.1057
step 73880; step_loss: 0.0838
step 73890; step_loss: 0.1208
step 73900; step_loss: 0.1343
step 73910; step_loss: 0.1598
step 73920; step_loss: 0.1487
step 73930; step_loss: 0.1431
step 73940; step_loss: 0.1253
step 73950; step_loss: 0.1567
step 73960; step_loss: 0.1147
step 73970; step_loss: 0.1086
step 73980; step_loss: 0.1472
step 73990; step_loss: 0.1657

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 0.970 | 1.120 | 1.401 | 1.512 | 1.684 | 1.940 |

============================
Global step:         74000
Learning rate:       0.0035
Step-time (ms):     35.8534
Train loss avg:      0.1270
--------------------------
Val loss:            0.9307
srnn loss:           0.7725
============================

Saving the model...
done in 451.18 ms
step 74000; step_loss: 0.0896
step 74010; step_loss: 0.1021
step 74020; step_loss: 0.1345
step 74030; step_loss: 0.0792
step 74040; step_loss: 0.1042
step 74050; step_loss: 0.1561
step 74060; step_loss: 0.1283
step 74070; step_loss: 0.1202
step 74080; step_loss: 0.1379
step 74090; step_loss: 0.1359
step 74100; step_loss: 0.1868
step 74110; step_loss: 0.1083
step 74120; step_loss: 0.1157
step 74130; step_loss: 0.1203
step 74140; step_loss: 0.1220
step 74150; step_loss: 0.1134
step 74160; step_loss: 0.1081
step 74170; step_loss: 0.2040
step 74180; step_loss: 0.1336
step 74190; step_loss: 0.1201
step 74200; step_loss: 0.2057
step 74210; step_loss: 0.1528
step 74220; step_loss: 0.1312
step 74230; step_loss: 0.1754
step 74240; step_loss: 0.1233
step 74250; step_loss: 0.1033
step 74260; step_loss: 0.1243
step 74270; step_loss: 0.1125
step 74280; step_loss: 0.1160
step 74290; step_loss: 0.1435
step 74300; step_loss: 0.1111
step 74310; step_loss: 0.1184
step 74320; step_loss: 0.0959
step 74330; step_loss: 0.1356
step 74340; step_loss: 0.1179
step 74350; step_loss: 0.0990
step 74360; step_loss: 0.1351
step 74370; step_loss: 0.1423
step 74380; step_loss: 0.1623
step 74390; step_loss: 0.1516
step 74400; step_loss: 0.1813
step 74410; step_loss: 0.1930
step 74420; step_loss: 0.0888
step 74430; step_loss: 0.1818
step 74440; step_loss: 0.0941
step 74450; step_loss: 0.1036
step 74460; step_loss: 0.1336
step 74470; step_loss: 0.0975
step 74480; step_loss: 0.0968
step 74490; step_loss: 0.1174
step 74500; step_loss: 0.1148
step 74510; step_loss: 0.1555
step 74520; step_loss: 0.1103
step 74530; step_loss: 0.1161
step 74540; step_loss: 0.0850
step 74550; step_loss: 0.1187
step 74560; step_loss: 0.0936
step 74570; step_loss: 0.1124
step 74580; step_loss: 0.1439
step 74590; step_loss: 0.1166
step 74600; step_loss: 0.1764
step 74610; step_loss: 0.1342
step 74620; step_loss: 0.1528
step 74630; step_loss: 0.1084
step 74640; step_loss: 0.1202
step 74650; step_loss: 0.1375
step 74660; step_loss: 0.1474
step 74670; step_loss: 0.1852
step 74680; step_loss: 0.1481
step 74690; step_loss: 0.1217
step 74700; step_loss: 0.1377
step 74710; step_loss: 0.1488
step 74720; step_loss: 0.0986
step 74730; step_loss: 0.1760
step 74740; step_loss: 0.1389
step 74750; step_loss: 0.1683
step 74760; step_loss: 0.1479
step 74770; step_loss: 0.1472
step 74780; step_loss: 0.1202
step 74790; step_loss: 0.1678
step 74800; step_loss: 0.1117
step 74810; step_loss: 0.1337
step 74820; step_loss: 0.1476
step 74830; step_loss: 0.1317
step 74840; step_loss: 0.1387
step 74850; step_loss: 0.1424
step 74860; step_loss: 0.1069
step 74870; step_loss: 0.1291
step 74880; step_loss: 0.1266
step 74890; step_loss: 0.1902
step 74900; step_loss: 0.1006
step 74910; step_loss: 0.1280
step 74920; step_loss: 0.1760
step 74930; step_loss: 0.1039
step 74940; step_loss: 0.1652
step 74950; step_loss: 0.1391
step 74960; step_loss: 0.1555
step 74970; step_loss: 0.1197
step 74980; step_loss: 0.1003
step 74990; step_loss: 0.1834

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 0.977 | 1.130 | 1.417 | 1.530 | 1.701 | 1.942 |

============================
Global step:         75000
Learning rate:       0.0035
Step-time (ms):     35.8101
Train loss avg:      0.1275
--------------------------
Val loss:            0.9412
srnn loss:           0.7782
============================

Saving the model...
done in 399.03 ms
step 75000; step_loss: 0.1134
step 75010; step_loss: 0.1219
step 75020; step_loss: 0.1489
step 75030; step_loss: 0.0882
step 75040; step_loss: 0.1315
step 75050; step_loss: 0.0871
step 75060; step_loss: 0.1127
step 75070; step_loss: 0.1969
step 75080; step_loss: 0.1164
step 75090; step_loss: 0.1089
step 75100; step_loss: 0.1425
step 75110; step_loss: 0.1532
step 75120; step_loss: 0.1289
step 75130; step_loss: 0.1126
step 75140; step_loss: 0.0982
step 75150; step_loss: 0.1298
step 75160; step_loss: 0.1389
step 75170; step_loss: 0.1325
step 75180; step_loss: 0.1245
step 75190; step_loss: 0.1247
step 75200; step_loss: 0.1215
step 75210; step_loss: 0.1133
step 75220; step_loss: 0.0942
step 75230; step_loss: 0.1203
step 75240; step_loss: 0.1148
step 75250; step_loss: 0.1028
step 75260; step_loss: 0.1780
step 75270; step_loss: 0.0965
step 75280; step_loss: 0.1181
step 75290; step_loss: 0.0743
step 75300; step_loss: 0.1053
step 75310; step_loss: 0.1569
step 75320; step_loss: 0.1068
step 75330; step_loss: 0.1042
step 75340; step_loss: 0.1878
step 75350; step_loss: 0.1144
step 75360; step_loss: 0.0864
step 75370; step_loss: 0.1451
step 75380; step_loss: 0.1686
step 75390; step_loss: 0.0927
step 75400; step_loss: 0.1116
step 75410; step_loss: 0.1265
step 75420; step_loss: 0.0797
step 75430; step_loss: 0.1437
step 75440; step_loss: 0.1197
step 75450; step_loss: 0.1356
step 75460; step_loss: 0.1343
step 75470; step_loss: 0.1769
step 75480; step_loss: 0.1851
step 75490; step_loss: 0.1362
step 75500; step_loss: 0.0950
step 75510; step_loss: 0.1562
step 75520; step_loss: 0.1228
step 75530; step_loss: 0.1196
step 75540; step_loss: 0.0884
step 75550; step_loss: 0.1222
step 75560; step_loss: 0.0783
step 75570; step_loss: 0.1072
step 75580; step_loss: 0.1600
step 75590; step_loss: 0.2124
step 75600; step_loss: 0.0823
step 75610; step_loss: 0.1332
step 75620; step_loss: 0.1060
step 75630; step_loss: 0.1105
step 75640; step_loss: 0.0951
step 75650; step_loss: 0.1688
step 75660; step_loss: 0.1536
step 75670; step_loss: 0.1172
step 75680; step_loss: 0.1499
step 75690; step_loss: 0.1529
step 75700; step_loss: 0.1125
step 75710; step_loss: 0.1231
step 75720; step_loss: 0.1106
step 75730; step_loss: 0.1274
step 75740; step_loss: 0.1497
step 75750; step_loss: 0.1167
step 75760; step_loss: 0.1310
step 75770; step_loss: 0.1130
step 75780; step_loss: 0.1040
step 75790; step_loss: 0.1281
step 75800; step_loss: 0.0995
step 75810; step_loss: 0.1261
step 75820; step_loss: 0.1495
step 75830; step_loss: 0.1398
step 75840; step_loss: 0.1080
step 75850; step_loss: 0.1267
step 75860; step_loss: 0.1284
step 75870; step_loss: 0.0933
step 75880; step_loss: 0.1139
step 75890; step_loss: 0.1065
step 75900; step_loss: 0.0819
step 75910; step_loss: 0.1191
step 75920; step_loss: 0.1031
step 75930; step_loss: 0.1058
step 75940; step_loss: 0.0689
step 75950; step_loss: 0.1336
step 75960; step_loss: 0.1509
step 75970; step_loss: 0.0933
step 75980; step_loss: 0.1169
step 75990; step_loss: 0.1093

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 0.968 | 1.120 | 1.405 | 1.518 | 1.690 | 1.928 |

============================
Global step:         76000
Learning rate:       0.0035
Step-time (ms):     35.8677
Train loss avg:      0.1250
--------------------------
Val loss:            1.0185
srnn loss:           0.7720
============================

Saving the model...
done in 397.35 ms
step 76000; step_loss: 0.0951
step 76010; step_loss: 0.1176
step 76020; step_loss: 0.1404
step 76030; step_loss: 0.1166
step 76040; step_loss: 0.1459
step 76050; step_loss: 0.1215
step 76060; step_loss: 0.1740
step 76070; step_loss: 0.1680
step 76080; step_loss: 0.1141
step 76090; step_loss: 0.1299
step 76100; step_loss: 0.0999
step 76110; step_loss: 0.1329
step 76120; step_loss: 0.1029
step 76130; step_loss: 0.1289
step 76140; step_loss: 0.1505
step 76150; step_loss: 0.0938
step 76160; step_loss: 0.1331
step 76170; step_loss: 0.1316
step 76180; step_loss: 0.1145
step 76190; step_loss: 0.1190
step 76200; step_loss: 0.1458
step 76210; step_loss: 0.1262
step 76220; step_loss: 0.0960
step 76230; step_loss: 0.1490
step 76240; step_loss: 0.1196
step 76250; step_loss: 0.1044
step 76260; step_loss: 0.1137
step 76270; step_loss: 0.1537
step 76280; step_loss: 0.1233
step 76290; step_loss: 0.1662
step 76300; step_loss: 0.1033
step 76310; step_loss: 0.1048
step 76320; step_loss: 0.1724
step 76330; step_loss: 0.1198
step 76340; step_loss: 0.1380
step 76350; step_loss: 0.1011
step 76360; step_loss: 0.1053
step 76370; step_loss: 0.1155
step 76380; step_loss: 0.1057
step 76390; step_loss: 0.1040
step 76400; step_loss: 0.1299
step 76410; step_loss: 0.1419
step 76420; step_loss: 0.1223
step 76430; step_loss: 0.1024
step 76440; step_loss: 0.0997
step 76450; step_loss: 0.1407
step 76460; step_loss: 0.0962
step 76470; step_loss: 0.1096
step 76480; step_loss: 0.1326
step 76490; step_loss: 0.1104
step 76500; step_loss: 0.1303
step 76510; step_loss: 0.1192
step 76520; step_loss: 0.1238
step 76530; step_loss: 0.1476
step 76540; step_loss: 0.1075
step 76550; step_loss: 0.1374
step 76560; step_loss: 0.1080
step 76570; step_loss: 0.1252
step 76580; step_loss: 0.1105
step 76590; step_loss: 0.1139
step 76600; step_loss: 0.1147
step 76610; step_loss: 0.1469
step 76620; step_loss: 0.1068
step 76630; step_loss: 0.1266
step 76640; step_loss: 0.1256
step 76650; step_loss: 0.1282
step 76660; step_loss: 0.1035
step 76670; step_loss: 0.2263
step 76680; step_loss: 0.1133
step 76690; step_loss: 0.1181
step 76700; step_loss: 0.0886
step 76710; step_loss: 0.1360
step 76720; step_loss: 0.1098
step 76730; step_loss: 0.1433
step 76740; step_loss: 0.1631
step 76750; step_loss: 0.1202
step 76760; step_loss: 0.0856
step 76770; step_loss: 0.1450
step 76780; step_loss: 0.1086
step 76790; step_loss: 0.1079
step 76800; step_loss: 0.1202
step 76810; step_loss: 0.1608
step 76820; step_loss: 0.0962
step 76830; step_loss: 0.1285
step 76840; step_loss: 0.1190
step 76850; step_loss: 0.0916
step 76860; step_loss: 0.0864
step 76870; step_loss: 0.1271
step 76880; step_loss: 0.1620
step 76890; step_loss: 0.1300
step 76900; step_loss: 0.1170
step 76910; step_loss: 0.0930
step 76920; step_loss: 0.1423
step 76930; step_loss: 0.0980
step 76940; step_loss: 0.1068
step 76950; step_loss: 0.1298
step 76960; step_loss: 0.0722
step 76970; step_loss: 0.1046
step 76980; step_loss: 0.0869
step 76990; step_loss: 0.0909

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 0.973 | 1.128 | 1.418 | 1.534 | 1.713 | 1.974 |

============================
Global step:         77000
Learning rate:       0.0035
Step-time (ms):     35.8824
Train loss avg:      0.1239
--------------------------
Val loss:            0.9188
srnn loss:           0.7828
============================

Saving the model...
done in 409.21 ms
step 77000; step_loss: 0.1263
step 77010; step_loss: 0.0851
step 77020; step_loss: 0.1470
step 77030; step_loss: 0.1759
step 77040; step_loss: 0.0916
step 77050; step_loss: 0.1432
step 77060; step_loss: 0.1015
step 77070; step_loss: 0.1122
step 77080; step_loss: 0.1574
step 77090; step_loss: 0.0891
step 77100; step_loss: 0.1343
step 77110; step_loss: 0.1232
step 77120; step_loss: 0.1033
step 77130; step_loss: 0.0997
step 77140; step_loss: 0.1458
step 77150; step_loss: 0.1142
step 77160; step_loss: 0.0946
step 77170; step_loss: 0.1571
step 77180; step_loss: 0.0996
step 77190; step_loss: 0.1719
step 77200; step_loss: 0.1585
step 77210; step_loss: 0.1398
step 77220; step_loss: 0.1513
step 77230; step_loss: 0.1177
step 77240; step_loss: 0.1591
step 77250; step_loss: 0.1620
step 77260; step_loss: 0.0971
step 77270; step_loss: 0.1210
step 77280; step_loss: 0.0964
step 77290; step_loss: 0.1311
step 77300; step_loss: 0.1321
step 77310; step_loss: 0.1153
step 77320; step_loss: 0.1361
step 77330; step_loss: 0.0928
step 77340; step_loss: 0.1426
step 77350; step_loss: 0.1121
step 77360; step_loss: 0.1448
step 77370; step_loss: 0.0935
step 77380; step_loss: 0.0928
step 77390; step_loss: 0.1066
step 77400; step_loss: 0.0817
step 77410; step_loss: 0.1129
step 77420; step_loss: 0.1185
step 77430; step_loss: 0.1294
step 77440; step_loss: 0.1457
step 77450; step_loss: 0.0932
step 77460; step_loss: 0.2016
step 77470; step_loss: 0.1278
step 77480; step_loss: 0.1122
step 77490; step_loss: 0.1437
step 77500; step_loss: 0.1200
step 77510; step_loss: 0.1183
step 77520; step_loss: 0.1421
step 77530; step_loss: 0.1297
step 77540; step_loss: 0.1280
step 77550; step_loss: 0.0871
step 77560; step_loss: 0.0949
step 77570; step_loss: 0.1327
step 77580; step_loss: 0.1318
step 77590; step_loss: 0.1314
step 77600; step_loss: 0.1274
step 77610; step_loss: 0.1463
step 77620; step_loss: 0.1707
step 77630; step_loss: 0.1696
step 77640; step_loss: 0.1029
step 77650; step_loss: 0.0964
step 77660; step_loss: 0.0977
step 77670; step_loss: 0.1430
step 77680; step_loss: 0.1121
step 77690; step_loss: 0.1094
step 77700; step_loss: 0.1103
step 77710; step_loss: 0.1231
step 77720; step_loss: 0.1746
step 77730; step_loss: 0.1263
step 77740; step_loss: 0.1111
step 77750; step_loss: 0.1072
step 77760; step_loss: 0.1242
step 77770; step_loss: 0.0866
step 77780; step_loss: 0.1234
step 77790; step_loss: 0.0925
step 77800; step_loss: 0.1063
step 77810; step_loss: 0.0887
step 77820; step_loss: 0.1279
step 77830; step_loss: 0.0984
step 77840; step_loss: 0.0853
step 77850; step_loss: 0.1397
step 77860; step_loss: 0.1079
step 77870; step_loss: 0.2042
step 77880; step_loss: 0.1239
step 77890; step_loss: 0.1251
step 77900; step_loss: 0.0979
step 77910; step_loss: 0.0896
step 77920; step_loss: 0.1381
step 77930; step_loss: 0.1107
step 77940; step_loss: 0.1421
step 77950; step_loss: 0.1142
step 77960; step_loss: 0.1208
step 77970; step_loss: 0.1252
step 77980; step_loss: 0.0723
step 77990; step_loss: 0.1080

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 0.967 | 1.121 | 1.409 | 1.522 | 1.693 | 1.930 |

============================
Global step:         78000
Learning rate:       0.0035
Step-time (ms):     35.9247
Train loss avg:      0.1245
--------------------------
Val loss:            0.9430
srnn loss:           0.7809
============================

Saving the model...
done in 400.19 ms
step 78000; step_loss: 0.1506
step 78010; step_loss: 0.1032
step 78020; step_loss: 0.0770
step 78030; step_loss: 0.1400
step 78040; step_loss: 0.1646
step 78050; step_loss: 0.1332
step 78060; step_loss: 0.1133
step 78070; step_loss: 0.1250
step 78080; step_loss: 0.0900
step 78090; step_loss: 0.1756
step 78100; step_loss: 0.0964
step 78110; step_loss: 0.1238
step 78120; step_loss: 0.1181
step 78130; step_loss: 0.1200
step 78140; step_loss: 0.0987
step 78150; step_loss: 0.1331
step 78160; step_loss: 0.1120
step 78170; step_loss: 0.1116
step 78180; step_loss: 0.1073
step 78190; step_loss: 0.1327
step 78200; step_loss: 0.1207
step 78210; step_loss: 0.1506
step 78220; step_loss: 0.1406
step 78230; step_loss: 0.0845
step 78240; step_loss: 0.0924
step 78250; step_loss: 0.1589
step 78260; step_loss: 0.1081
step 78270; step_loss: 0.1193
step 78280; step_loss: 0.1184
step 78290; step_loss: 0.1052
step 78300; step_loss: 0.1054
step 78310; step_loss: 0.1180
step 78320; step_loss: 0.1551
step 78330; step_loss: 0.1030
step 78340; step_loss: 0.1173
step 78350; step_loss: 0.1468
step 78360; step_loss: 0.1273
step 78370; step_loss: 0.1219
step 78380; step_loss: 0.1166
step 78390; step_loss: 0.1288
step 78400; step_loss: 0.1254
step 78410; step_loss: 0.1812
step 78420; step_loss: 0.1147
step 78430; step_loss: 0.1132
step 78440; step_loss: 0.1343
step 78450; step_loss: 0.1116
step 78460; step_loss: 0.1200
step 78470; step_loss: 0.1111
step 78480; step_loss: 0.1463
step 78490; step_loss: 0.1384
step 78500; step_loss: 0.1086
step 78510; step_loss: 0.1001
step 78520; step_loss: 0.1459
step 78530; step_loss: 0.1009
step 78540; step_loss: 0.0876
step 78550; step_loss: 0.1209
step 78560; step_loss: 0.1190
step 78570; step_loss: 0.0914
step 78580; step_loss: 0.1067
step 78590; step_loss: 0.1237
step 78600; step_loss: 0.0997
step 78610; step_loss: 0.1172
step 78620; step_loss: 0.0994
step 78630; step_loss: 0.1108
step 78640; step_loss: 0.1032
step 78650; step_loss: 0.1636
step 78660; step_loss: 0.1339
step 78670; step_loss: 0.1442
step 78680; step_loss: 0.1088
step 78690; step_loss: 0.1398
step 78700; step_loss: 0.1242
step 78710; step_loss: 0.1643
step 78720; step_loss: 0.1318
step 78730; step_loss: 0.1578
step 78740; step_loss: 0.0669
step 78750; step_loss: 0.1203
step 78760; step_loss: 0.1773
step 78770; step_loss: 0.1335
step 78780; step_loss: 0.1421
step 78790; step_loss: 0.1828
step 78800; step_loss: 0.1078
step 78810; step_loss: 0.1054
step 78820; step_loss: 0.1397
step 78830; step_loss: 0.1290
step 78840; step_loss: 0.1043
step 78850; step_loss: 0.1303
step 78860; step_loss: 0.1478
step 78870; step_loss: 0.1143
step 78880; step_loss: 0.1210
step 78890; step_loss: 0.0883
step 78900; step_loss: 0.1793
step 78910; step_loss: 0.0895
step 78920; step_loss: 0.1511
step 78930; step_loss: 0.1532
step 78940; step_loss: 0.1114
step 78950; step_loss: 0.1135
step 78960; step_loss: 0.1104
step 78970; step_loss: 0.1333
step 78980; step_loss: 0.1094
step 78990; step_loss: 0.1158

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 0.970 | 1.127 | 1.423 | 1.540 | 1.721 | 1.978 |

============================
Global step:         79000
Learning rate:       0.0035
Step-time (ms):     35.8142
Train loss avg:      0.1248
--------------------------
Val loss:            0.8868
srnn loss:           0.7977
============================

Saving the model...
done in 401.35 ms
step 79000; step_loss: 0.1245
step 79010; step_loss: 0.1548
step 79020; step_loss: 0.1175
step 79030; step_loss: 0.1736
step 79040; step_loss: 0.1338
step 79050; step_loss: 0.1325
step 79060; step_loss: 0.1293
step 79070; step_loss: 0.1141
step 79080; step_loss: 0.1161
step 79090; step_loss: 0.1035
step 79100; step_loss: 0.1236
step 79110; step_loss: 0.1367
step 79120; step_loss: 0.0966
step 79130; step_loss: 0.1145
step 79140; step_loss: 0.1057
step 79150; step_loss: 0.1238
step 79160; step_loss: 0.1129
step 79170; step_loss: 0.1378
step 79180; step_loss: 0.1495
step 79190; step_loss: 0.1452
step 79200; step_loss: 0.0862
step 79210; step_loss: 0.1087
step 79220; step_loss: 0.0969
step 79230; step_loss: 0.1479
step 79240; step_loss: 0.1057
step 79250; step_loss: 0.0989
step 79260; step_loss: 0.1193
step 79270; step_loss: 0.1038
step 79280; step_loss: 0.1269
step 79290; step_loss: 0.1452
step 79300; step_loss: 0.1061
step 79310; step_loss: 0.1337
step 79320; step_loss: 0.1387
step 79330; step_loss: 0.1602
step 79340; step_loss: 0.1724
step 79350; step_loss: 0.1523
step 79360; step_loss: 0.0840
step 79370; step_loss: 0.0890
step 79380; step_loss: 0.0998
step 79390; step_loss: 0.1387
step 79400; step_loss: 0.1386
step 79410; step_loss: 0.1151
step 79420; step_loss: 0.1115
step 79430; step_loss: 0.1261
step 79440; step_loss: 0.1126
step 79450; step_loss: 0.1325
step 79460; step_loss: 0.0989
step 79470; step_loss: 0.1263
step 79480; step_loss: 0.1387
step 79490; step_loss: 0.1187
step 79500; step_loss: 0.1374
step 79510; step_loss: 0.1219
step 79520; step_loss: 0.1578
step 79530; step_loss: 0.1137
step 79540; step_loss: 0.1842
step 79550; step_loss: 0.1149
step 79560; step_loss: 0.1298
step 79570; step_loss: 0.1511
step 79580; step_loss: 0.1213
step 79590; step_loss: 0.1147
step 79600; step_loss: 0.1133
step 79610; step_loss: 0.0997
step 79620; step_loss: 0.1146
step 79630; step_loss: 0.1386
step 79640; step_loss: 0.0841
step 79650; step_loss: 0.0999
step 79660; step_loss: 0.0996
step 79670; step_loss: 0.1552
step 79680; step_loss: 0.1050
step 79690; step_loss: 0.1190
step 79700; step_loss: 0.1144
step 79710; step_loss: 0.1516
step 79720; step_loss: 0.1818
step 79730; step_loss: 0.1632
step 79740; step_loss: 0.1405
step 79750; step_loss: 0.0926
step 79760; step_loss: 0.1057
step 79770; step_loss: 0.0732
step 79780; step_loss: 0.1294
step 79790; step_loss: 0.1055
step 79800; step_loss: 0.0799
step 79810; step_loss: 0.1614
step 79820; step_loss: 0.1501
step 79830; step_loss: 0.1543
step 79840; step_loss: 0.1053
step 79850; step_loss: 0.1542
step 79860; step_loss: 0.1677
step 79870; step_loss: 0.1469
step 79880; step_loss: 0.1228
step 79890; step_loss: 0.1001
step 79900; step_loss: 0.1460
step 79910; step_loss: 0.1275
step 79920; step_loss: 0.1009
step 79930; step_loss: 0.1299
step 79940; step_loss: 0.1406
step 79950; step_loss: 0.1051
step 79960; step_loss: 0.1067
step 79970; step_loss: 0.1244
step 79980; step_loss: 0.1271
step 79990; step_loss: 0.1353

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 0.967 | 1.121 | 1.412 | 1.528 | 1.704 | 1.955 |

============================
Global step:         80000
Learning rate:       0.0033
Step-time (ms):     35.7179
Train loss avg:      0.1231
--------------------------
Val loss:            0.8664
srnn loss:           0.7884
============================

Saving the model...
done in 434.35 ms
step 80000; step_loss: 0.1015
step 80010; step_loss: 0.1608
step 80020; step_loss: 0.1533
step 80030; step_loss: 0.0941
step 80040; step_loss: 0.1003
step 80050; step_loss: 0.1008
step 80060; step_loss: 0.1265
step 80070; step_loss: 0.1301
step 80080; step_loss: 0.1054
step 80090; step_loss: 0.1388
step 80100; step_loss: 0.1262
step 80110; step_loss: 0.1035
step 80120; step_loss: 0.0915
step 80130; step_loss: 0.1221
step 80140; step_loss: 0.1168
step 80150; step_loss: 0.1451
step 80160; step_loss: 0.1473
step 80170; step_loss: 0.1150
step 80180; step_loss: 0.1488
step 80190; step_loss: 0.1055
step 80200; step_loss: 0.1208
step 80210; step_loss: 0.1021
step 80220; step_loss: 0.0987
step 80230; step_loss: 0.1398
step 80240; step_loss: 0.1027
step 80250; step_loss: 0.1145
step 80260; step_loss: 0.1168
step 80270; step_loss: 0.1172
step 80280; step_loss: 0.1344
step 80290; step_loss: 0.1551
step 80300; step_loss: 0.1168
step 80310; step_loss: 0.1238
step 80320; step_loss: 0.1158
step 80330; step_loss: 0.1302
step 80340; step_loss: 0.1014
step 80350; step_loss: 0.1382
step 80360; step_loss: 0.1187
step 80370; step_loss: 0.1055
step 80380; step_loss: 0.1712
step 80390; step_loss: 0.0706
step 80400; step_loss: 0.0944
step 80410; step_loss: 0.1226
step 80420; step_loss: 0.1040
step 80430; step_loss: 0.1045
step 80440; step_loss: 0.1299
step 80450; step_loss: 0.1300
step 80460; step_loss: 0.0794
step 80470; step_loss: 0.1004
step 80480; step_loss: 0.1177
step 80490; step_loss: 0.1526
step 80500; step_loss: 0.0777
step 80510; step_loss: 0.1187
step 80520; step_loss: 0.1134
step 80530; step_loss: 0.1048
step 80540; step_loss: 0.1209
step 80550; step_loss: 0.0946
step 80560; step_loss: 0.0868
step 80570; step_loss: 0.1269
step 80580; step_loss: 0.0919
step 80590; step_loss: 0.1006
step 80600; step_loss: 0.1131
step 80610; step_loss: 0.1561
step 80620; step_loss: 0.1192
step 80630; step_loss: 0.1227
step 80640; step_loss: 0.1224
step 80650; step_loss: 0.1065
step 80660; step_loss: 0.1190
step 80670; step_loss: 0.1352
step 80680; step_loss: 0.1062
step 80690; step_loss: 0.1247
step 80700; step_loss: 0.1025
step 80710; step_loss: 0.1299
step 80720; step_loss: 0.1234
step 80730; step_loss: 0.1063
step 80740; step_loss: 0.0905
step 80750; step_loss: 0.1372
step 80760; step_loss: 0.1028
step 80770; step_loss: 0.1236
step 80780; step_loss: 0.1274
step 80790; step_loss: 0.1205
step 80800; step_loss: 0.1030
step 80810; step_loss: 0.1138
step 80820; step_loss: 0.1045
step 80830; step_loss: 0.1460
step 80840; step_loss: 0.1066
step 80850; step_loss: 0.1134
step 80860; step_loss: 0.1278
step 80870; step_loss: 0.1514
step 80880; step_loss: 0.1252
step 80890; step_loss: 0.1715
step 80900; step_loss: 0.1183
step 80910; step_loss: 0.1057
step 80920; step_loss: 0.1631
step 80930; step_loss: 0.1427
step 80940; step_loss: 0.1802
step 80950; step_loss: 0.1123
step 80960; step_loss: 0.0986
step 80970; step_loss: 0.1203
step 80980; step_loss: 0.1365
step 80990; step_loss: 0.1398

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 0.965 | 1.120 | 1.414 | 1.531 | 1.709 | 1.957 |

============================
Global step:         81000
Learning rate:       0.0033
Step-time (ms):     35.8320
Train loss avg:      0.1215
--------------------------
Val loss:            0.9435
srnn loss:           0.7877
============================

Saving the model...
done in 400.15 ms
step 81000; step_loss: 0.0867
step 81010; step_loss: 0.1350
step 81020; step_loss: 0.1265
step 81030; step_loss: 0.1615
step 81040; step_loss: 0.1320
step 81050; step_loss: 0.0839
step 81060; step_loss: 0.1532
step 81070; step_loss: 0.1239
step 81080; step_loss: 0.0986
step 81090; step_loss: 0.1220
step 81100; step_loss: 0.1592
step 81110; step_loss: 0.1198
step 81120; step_loss: 0.1010
step 81130; step_loss: 0.1502
step 81140; step_loss: 0.1569
step 81150; step_loss: 0.1152
step 81160; step_loss: 0.1578
step 81170; step_loss: 0.0943
step 81180; step_loss: 0.1505
step 81190; step_loss: 0.1403
step 81200; step_loss: 0.1154
step 81210; step_loss: 0.1301
step 81220; step_loss: 0.0949
step 81230; step_loss: 0.0986
step 81240; step_loss: 0.1262
step 81250; step_loss: 0.1001
step 81260; step_loss: 0.1375
step 81270; step_loss: 0.1365
step 81280; step_loss: 0.1131
step 81290; step_loss: 0.1181
step 81300; step_loss: 0.0981
step 81310; step_loss: 0.0980
step 81320; step_loss: 0.1160
step 81330; step_loss: 0.0976
step 81340; step_loss: 0.1299
step 81350; step_loss: 0.1636
step 81360; step_loss: 0.1109
step 81370; step_loss: 0.1322
step 81380; step_loss: 0.1116
step 81390; step_loss: 0.1440
step 81400; step_loss: 0.1269
step 81410; step_loss: 0.1059
step 81420; step_loss: 0.1185
step 81430; step_loss: 0.1553
step 81440; step_loss: 0.0958
step 81450; step_loss: 0.1183
step 81460; step_loss: 0.1223
step 81470; step_loss: 0.1386
step 81480; step_loss: 0.1077
step 81490; step_loss: 0.1282
step 81500; step_loss: 0.1185
step 81510; step_loss: 0.1311
step 81520; step_loss: 0.0950
step 81530; step_loss: 0.1027
step 81540; step_loss: 0.1282
step 81550; step_loss: 0.1025
step 81560; step_loss: 0.1367
step 81570; step_loss: 0.1110
step 81580; step_loss: 0.1392
step 81590; step_loss: 0.1418
step 81600; step_loss: 0.1151
step 81610; step_loss: 0.0865
step 81620; step_loss: 0.1711
step 81630; step_loss: 0.1026
step 81640; step_loss: 0.0974
step 81650; step_loss: 0.0964
step 81660; step_loss: 0.0903
step 81670; step_loss: 0.1314
step 81680; step_loss: 0.1332
step 81690; step_loss: 0.1248
step 81700; step_loss: 0.1515
step 81710; step_loss: 0.1256
step 81720; step_loss: 0.1259
step 81730; step_loss: 0.1221
step 81740; step_loss: 0.1631
step 81750; step_loss: 0.1176
step 81760; step_loss: 0.0988
step 81770; step_loss: 0.1303
step 81780; step_loss: 0.1168
step 81790; step_loss: 0.1171
step 81800; step_loss: 0.1045
step 81810; step_loss: 0.1676
step 81820; step_loss: 0.1814
step 81830; step_loss: 0.1272
step 81840; step_loss: 0.1200
step 81850; step_loss: 0.1046
step 81860; step_loss: 0.1032
step 81870; step_loss: 0.1058
step 81880; step_loss: 0.1420
step 81890; step_loss: 0.0988
step 81900; step_loss: 0.1689
step 81910; step_loss: 0.1398
step 81920; step_loss: 0.1109
step 81930; step_loss: 0.0949
step 81940; step_loss: 0.1190
step 81950; step_loss: 0.0943
step 81960; step_loss: 0.1064
step 81970; step_loss: 0.1409
step 81980; step_loss: 0.1222
step 81990; step_loss: 0.0890

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 0.966 | 1.125 | 1.422 | 1.542 | 1.724 | 1.999 |

============================
Global step:         82000
Learning rate:       0.0033
Step-time (ms):     35.7663
Train loss avg:      0.1217
--------------------------
Val loss:            0.9588
srnn loss:           0.8027
============================

Saving the model...
done in 433.86 ms
step 82000; step_loss: 0.1192
step 82010; step_loss: 0.2365
step 82020; step_loss: 0.1153
step 82030; step_loss: 0.1022
step 82040; step_loss: 0.0838
step 82050; step_loss: 0.1180
step 82060; step_loss: 0.1666
step 82070; step_loss: 0.0982
step 82080; step_loss: 0.1370
step 82090; step_loss: 0.1094
step 82100; step_loss: 0.1386
step 82110; step_loss: 0.1367
step 82120; step_loss: 0.1052
step 82130; step_loss: 0.1363
step 82140; step_loss: 0.0941
step 82150; step_loss: 0.2241
step 82160; step_loss: 0.0951
step 82170; step_loss: 0.0852
step 82180; step_loss: 0.0957
step 82190; step_loss: 0.1189
step 82200; step_loss: 0.1543
step 82210; step_loss: 0.1037
step 82220; step_loss: 0.1551
step 82230; step_loss: 0.0874
step 82240; step_loss: 0.1068
step 82250; step_loss: 0.1033
step 82260; step_loss: 0.1039
step 82270; step_loss: 0.0984
step 82280; step_loss: 0.1685
step 82290; step_loss: 0.1523
step 82300; step_loss: 0.1258
step 82310; step_loss: 0.1341
step 82320; step_loss: 0.1189
step 82330; step_loss: 0.1289
step 82340; step_loss: 0.1348
step 82350; step_loss: 0.0946
step 82360; step_loss: 0.0934
step 82370; step_loss: 0.1205
step 82380; step_loss: 0.1483
step 82390; step_loss: 0.1367
step 82400; step_loss: 0.1044
step 82410; step_loss: 0.1166
step 82420; step_loss: 0.1303
step 82430; step_loss: 0.0891
step 82440; step_loss: 0.0997
step 82450; step_loss: 0.1039
step 82460; step_loss: 0.1044
step 82470; step_loss: 0.1927
step 82480; step_loss: 0.1116
step 82490; step_loss: 0.1348
step 82500; step_loss: 0.1059
step 82510; step_loss: 0.0973
step 82520; step_loss: 0.1118
step 82530; step_loss: 0.1448
step 82540; step_loss: 0.1274
step 82550; step_loss: 0.1215
step 82560; step_loss: 0.1142
step 82570; step_loss: 0.0991
step 82580; step_loss: 0.1717
step 82590; step_loss: 0.0994
step 82600; step_loss: 0.1676
step 82610; step_loss: 0.1173
step 82620; step_loss: 0.1068
step 82630; step_loss: 0.1010
step 82640; step_loss: 0.1487
step 82650; step_loss: 0.1291
step 82660; step_loss: 0.1213
step 82670; step_loss: 0.0914
step 82680; step_loss: 0.1282
step 82690; step_loss: 0.0796
step 82700; step_loss: 0.1553
step 82710; step_loss: 0.0910
step 82720; step_loss: 0.1030
step 82730; step_loss: 0.0992
step 82740; step_loss: 0.1075
step 82750; step_loss: 0.0727
step 82760; step_loss: 0.1111
step 82770; step_loss: 0.1164
step 82780; step_loss: 0.1545
step 82790; step_loss: 0.1563
step 82800; step_loss: 0.1132
step 82810; step_loss: 0.1631
step 82820; step_loss: 0.1163
step 82830; step_loss: 0.1269
step 82840; step_loss: 0.1763
step 82850; step_loss: 0.1428
step 82860; step_loss: 0.1180
step 82870; step_loss: 0.0840
step 82880; step_loss: 0.1183
step 82890; step_loss: 0.0914
step 82900; step_loss: 0.1542
step 82910; step_loss: 0.1429
step 82920; step_loss: 0.1098
step 82930; step_loss: 0.1244
step 82940; step_loss: 0.1232
step 82950; step_loss: 0.1001
step 82960; step_loss: 0.1166
step 82970; step_loss: 0.1276
step 82980; step_loss: 0.1376
step 82990; step_loss: 0.0988

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 0.958 | 1.113 | 1.405 | 1.521 | 1.699 | 1.961 |

============================
Global step:         83000
Learning rate:       0.0033
Step-time (ms):     35.9013
Train loss avg:      0.1202
--------------------------
Val loss:            0.8624
srnn loss:           0.7995
============================

Saving the model...
done in 403.99 ms
step 83000; step_loss: 0.1201
step 83010; step_loss: 0.1207
step 83020; step_loss: 0.1274
step 83030; step_loss: 0.1432
step 83040; step_loss: 0.0856
step 83050; step_loss: 0.1458
step 83060; step_loss: 0.0698
step 83070; step_loss: 0.1083
step 83080; step_loss: 0.0929
step 83090; step_loss: 0.1939
step 83100; step_loss: 0.1531
step 83110; step_loss: 0.1034
step 83120; step_loss: 0.1021
step 83130; step_loss: 0.1009
step 83140; step_loss: 0.1726
step 83150; step_loss: 0.0784
step 83160; step_loss: 0.1196
step 83170; step_loss: 0.0985
step 83180; step_loss: 0.0977
step 83190; step_loss: 0.1159
step 83200; step_loss: 0.1393
step 83210; step_loss: 0.1169
step 83220; step_loss: 0.1484
step 83230; step_loss: 0.1515
step 83240; step_loss: 0.1283
step 83250; step_loss: 0.0961
step 83260; step_loss: 0.1161
step 83270; step_loss: 0.1152
step 83280; step_loss: 0.1365
step 83290; step_loss: 0.1077
step 83300; step_loss: 0.0945
step 83310; step_loss: 0.1041
step 83320; step_loss: 0.1539
step 83330; step_loss: 0.1390
step 83340; step_loss: 0.1128
step 83350; step_loss: 0.1086
step 83360; step_loss: 0.1103
step 83370; step_loss: 0.1452
step 83380; step_loss: 0.1384
step 83390; step_loss: 0.1313
step 83400; step_loss: 0.0773
step 83410; step_loss: 0.1103
step 83420; step_loss: 0.1403
step 83430; step_loss: 0.0782
step 83440; step_loss: 0.1071
step 83450; step_loss: 0.1658
step 83460; step_loss: 0.1294
step 83470; step_loss: 0.1107
step 83480; step_loss: 0.0987
step 83490; step_loss: 0.1015
step 83500; step_loss: 0.0932
step 83510; step_loss: 0.1330
step 83520; step_loss: 0.0837
step 83530; step_loss: 0.0963
step 83540; step_loss: 0.1446
step 83550; step_loss: 0.1647
step 83560; step_loss: 0.1036
step 83570; step_loss: 0.1392
step 83580; step_loss: 0.0941
step 83590; step_loss: 0.0909
step 83600; step_loss: 0.1116
step 83610; step_loss: 0.1202
step 83620; step_loss: 0.1094
step 83630; step_loss: 0.0942
step 83640; step_loss: 0.1032
step 83650; step_loss: 0.1372
step 83660; step_loss: 0.1175
step 83670; step_loss: 0.1467
step 83680; step_loss: 0.0880
step 83690; step_loss: 0.1469
step 83700; step_loss: 0.1041
step 83710; step_loss: 0.0709
step 83720; step_loss: 0.1238
step 83730; step_loss: 0.0940
step 83740; step_loss: 0.0984
step 83750; step_loss: 0.1197
step 83760; step_loss: 0.1368
step 83770; step_loss: 0.1617
step 83780; step_loss: 0.1308
step 83790; step_loss: 0.1105
step 83800; step_loss: 0.1426
step 83810; step_loss: 0.0994
step 83820; step_loss: 0.0932
step 83830; step_loss: 0.1363
step 83840; step_loss: 0.1911
step 83850; step_loss: 0.1386
step 83860; step_loss: 0.1322
step 83870; step_loss: 0.1328
step 83880; step_loss: 0.0965
step 83890; step_loss: 0.1040
step 83900; step_loss: 0.1388
step 83910; step_loss: 0.1319
step 83920; step_loss: 0.1062
step 83930; step_loss: 0.1168
step 83940; step_loss: 0.1007
step 83950; step_loss: 0.1197
step 83960; step_loss: 0.1213
step 83970; step_loss: 0.0960
step 83980; step_loss: 0.1329
step 83990; step_loss: 0.1021

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 0.963 | 1.121 | 1.419 | 1.538 | 1.717 | 1.976 |

============================
Global step:         84000
Learning rate:       0.0033
Step-time (ms):     35.8576
Train loss avg:      0.1219
--------------------------
Val loss:            0.9362
srnn loss:           0.8056
============================

Saving the model...
done in 419.26 ms
step 84000; step_loss: 0.0908
step 84010; step_loss: 0.1067
step 84020; step_loss: 0.1073
step 84030; step_loss: 0.1299
step 84040; step_loss: 0.1506
step 84050; step_loss: 0.0924
step 84060; step_loss: 0.1134
step 84070; step_loss: 0.1590
step 84080; step_loss: 0.0965
step 84090; step_loss: 0.1419
step 84100; step_loss: 0.1073
step 84110; step_loss: 0.0936
step 84120; step_loss: 0.1324
step 84130; step_loss: 0.1094
step 84140; step_loss: 0.1105
step 84150; step_loss: 0.1212
step 84160; step_loss: 0.1672
step 84170; step_loss: 0.1150
step 84180; step_loss: 0.1101
step 84190; step_loss: 0.0987
step 84200; step_loss: 0.1234
step 84210; step_loss: 0.1056
step 84220; step_loss: 0.1117
step 84230; step_loss: 0.0861
step 84240; step_loss: 0.0826
step 84250; step_loss: 0.1409
step 84260; step_loss: 0.0953
step 84270; step_loss: 0.1222
step 84280; step_loss: 0.1460
step 84290; step_loss: 0.0948
step 84300; step_loss: 0.0995
step 84310; step_loss: 0.1285
step 84320; step_loss: 0.0838
step 84330; step_loss: 0.1084
step 84340; step_loss: 0.1570
step 84350; step_loss: 0.1524
step 84360; step_loss: 0.0946
step 84370; step_loss: 0.1107
step 84380; step_loss: 0.1076
step 84390; step_loss: 0.1221
step 84400; step_loss: 0.1039
step 84410; step_loss: 0.0989
step 84420; step_loss: 0.1088
step 84430; step_loss: 0.1103
step 84440; step_loss: 0.0959
step 84450; step_loss: 0.1049
step 84460; step_loss: 0.1399
step 84470; step_loss: 0.1446
step 84480; step_loss: 0.1231
step 84490; step_loss: 0.1534
step 84500; step_loss: 0.0960
step 84510; step_loss: 0.1481
step 84520; step_loss: 0.1028
step 84530; step_loss: 0.1631
step 84540; step_loss: 0.0909
step 84550; step_loss: 0.1254
step 84560; step_loss: 0.1446
step 84570; step_loss: 0.1410
step 84580; step_loss: 0.1369
step 84590; step_loss: 0.1509
step 84600; step_loss: 0.1137
step 84610; step_loss: 0.0875
step 84620; step_loss: 0.1300
step 84630; step_loss: 0.0972
step 84640; step_loss: 0.1569
step 84650; step_loss: 0.1002
step 84660; step_loss: 0.1431
step 84670; step_loss: 0.1609
step 84680; step_loss: 0.1028
step 84690; step_loss: 0.1462
step 84700; step_loss: 0.1544
step 84710; step_loss: 0.1283
step 84720; step_loss: 0.1110
step 84730; step_loss: 0.0956
step 84740; step_loss: 0.1146
step 84750; step_loss: 0.0751
step 84760; step_loss: 0.1128
step 84770; step_loss: 0.1144
step 84780; step_loss: 0.1330
step 84790; step_loss: 0.0757
step 84800; step_loss: 0.1352
step 84810; step_loss: 0.1292
step 84820; step_loss: 0.1153
step 84830; step_loss: 0.0995
step 84840; step_loss: 0.1203
step 84850; step_loss: 0.1473
step 84860; step_loss: 0.1428
step 84870; step_loss: 0.1479
step 84880; step_loss: 0.1110
step 84890; step_loss: 0.1156
step 84900; step_loss: 0.1139
step 84910; step_loss: 0.1566
step 84920; step_loss: 0.1274
step 84930; step_loss: 0.1380
step 84940; step_loss: 0.1072
step 84950; step_loss: 0.1349
step 84960; step_loss: 0.1204
step 84970; step_loss: 0.1204
step 84980; step_loss: 0.1083
step 84990; step_loss: 0.1145

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 0.959 | 1.118 | 1.422 | 1.545 | 1.727 | 1.995 |

============================
Global step:         85000
Learning rate:       0.0033
Step-time (ms):     35.9419
Train loss avg:      0.1194
--------------------------
Val loss:            0.9154
srnn loss:           0.8134
============================

Saving the model...
done in 404.21 ms
step 85000; step_loss: 0.1484
step 85010; step_loss: 0.0933
step 85020; step_loss: 0.1107
step 85030; step_loss: 0.1145
step 85040; step_loss: 0.1166
step 85050; step_loss: 0.1292
step 85060; step_loss: 0.1029
step 85070; step_loss: 0.0973
step 85080; step_loss: 0.1075
step 85090; step_loss: 0.0901
step 85100; step_loss: 0.1189
step 85110; step_loss: 0.1330
step 85120; step_loss: 0.1389
step 85130; step_loss: 0.1374
step 85140; step_loss: 0.0888
step 85150; step_loss: 0.1612
step 85160; step_loss: 0.1082
step 85170; step_loss: 0.1352
step 85180; step_loss: 0.1528
step 85190; step_loss: 0.0697
step 85200; step_loss: 0.1013
step 85210; step_loss: 0.1145
step 85220; step_loss: 0.1538
step 85230; step_loss: 0.0740
step 85240; step_loss: 0.1267
step 85250; step_loss: 0.0902
step 85260; step_loss: 0.1067
step 85270; step_loss: 0.1406
step 85280; step_loss: 0.1333
step 85290; step_loss: 0.1705
step 85300; step_loss: 0.1260
step 85310; step_loss: 0.1383
step 85320; step_loss: 0.1307
step 85330; step_loss: 0.1633
step 85340; step_loss: 0.1129
step 85350; step_loss: 0.1116
step 85360; step_loss: 0.1506
step 85370; step_loss: 0.0995
step 85380; step_loss: 0.1004
step 85390; step_loss: 0.1144
step 85400; step_loss: 0.1449
step 85410; step_loss: 0.0853
step 85420; step_loss: 0.1415
step 85430; step_loss: 0.0746
step 85440; step_loss: 0.1304
step 85450; step_loss: 0.1101
step 85460; step_loss: 0.0884
step 85470; step_loss: 0.1131
step 85480; step_loss: 0.0913
step 85490; step_loss: 0.1134
step 85500; step_loss: 0.1254
step 85510; step_loss: 0.0876
step 85520; step_loss: 0.1385
step 85530; step_loss: 0.1235
step 85540; step_loss: 0.1187
step 85550; step_loss: 0.1578
step 85560; step_loss: 0.1293
step 85570; step_loss: 0.0876
step 85580; step_loss: 0.1101
step 85590; step_loss: 0.0988
step 85600; step_loss: 0.0892
step 85610; step_loss: 0.1297
step 85620; step_loss: 0.1321
step 85630; step_loss: 0.1568
step 85640; step_loss: 0.1182
step 85650; step_loss: 0.0817
step 85660; step_loss: 0.1367
step 85670; step_loss: 0.0885
step 85680; step_loss: 0.1355
step 85690; step_loss: 0.1158
step 85700; step_loss: 0.1164
step 85710; step_loss: 0.1200
step 85720; step_loss: 0.1201
step 85730; step_loss: 0.1048
step 85740; step_loss: 0.1240
step 85750; step_loss: 0.1274
step 85760; step_loss: 0.1074
step 85770; step_loss: 0.1195
step 85780; step_loss: 0.0997
step 85790; step_loss: 0.1155
step 85800; step_loss: 0.0805
step 85810; step_loss: 0.1117
step 85820; step_loss: 0.1304
step 85830; step_loss: 0.1124
step 85840; step_loss: 0.1574
step 85850; step_loss: 0.1041
step 85860; step_loss: 0.1341
step 85870; step_loss: 0.1012
step 85880; step_loss: 0.1209
step 85890; step_loss: 0.0871
step 85900; step_loss: 0.0993
step 85910; step_loss: 0.1955
step 85920; step_loss: 0.0921
step 85930; step_loss: 0.1259
step 85940; step_loss: 0.1019
step 85950; step_loss: 0.1250
step 85960; step_loss: 0.0970
step 85970; step_loss: 0.1306
step 85980; step_loss: 0.1354
step 85990; step_loss: 0.1401

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 0.957 | 1.115 | 1.414 | 1.535 | 1.710 | 1.970 |

============================
Global step:         86000
Learning rate:       0.0033
Step-time (ms):     35.9355
Train loss avg:      0.1179
--------------------------
Val loss:            0.9720
srnn loss:           0.8060
============================

Saving the model...
done in 444.39 ms
step 86000; step_loss: 0.0933
step 86010; step_loss: 0.1154
step 86020; step_loss: 0.1360
step 86030; step_loss: 0.2170
step 86040; step_loss: 0.1055
step 86050; step_loss: 0.1092
step 86060; step_loss: 0.1072
step 86070; step_loss: 0.1223
step 86080; step_loss: 0.1105
step 86090; step_loss: 0.0943
step 86100; step_loss: 0.1374
step 86110; step_loss: 0.1447
step 86120; step_loss: 0.1301
step 86130; step_loss: 0.0887
step 86140; step_loss: 0.0885
step 86150; step_loss: 0.1424
step 86160; step_loss: 0.0848
step 86170; step_loss: 0.1233
step 86180; step_loss: 0.1296
step 86190; step_loss: 0.1014
step 86200; step_loss: 0.1174
step 86210; step_loss: 0.0848
step 86220; step_loss: 0.1200
step 86230; step_loss: 0.1020
step 86240; step_loss: 0.1254
step 86250; step_loss: 0.1173
step 86260; step_loss: 0.1069
step 86270; step_loss: 0.1530
step 86280; step_loss: 0.0915
step 86290; step_loss: 0.0975
step 86300; step_loss: 0.1445
step 86310; step_loss: 0.1052
step 86320; step_loss: 0.1695
step 86330; step_loss: 0.0857
step 86340; step_loss: 0.0783
step 86350; step_loss: 0.1253
step 86360; step_loss: 0.1512
step 86370; step_loss: 0.0591
step 86380; step_loss: 0.2112
step 86390; step_loss: 0.1137
step 86400; step_loss: 0.1139
step 86410; step_loss: 0.0995
step 86420; step_loss: 0.0938
step 86430; step_loss: 0.1220
step 86440; step_loss: 0.1568
step 86450; step_loss: 0.1336
step 86460; step_loss: 0.1253
step 86470; step_loss: 0.0995
step 86480; step_loss: 0.1094
step 86490; step_loss: 0.1197
step 86500; step_loss: 0.1155
step 86510; step_loss: 0.1181
step 86520; step_loss: 0.1095
step 86530; step_loss: 0.1112
step 86540; step_loss: 0.0989
step 86550; step_loss: 0.1228
step 86560; step_loss: 0.1068
step 86570; step_loss: 0.1726
step 86580; step_loss: 0.1049
step 86590; step_loss: 0.1430
step 86600; step_loss: 0.1697
step 86610; step_loss: 0.1057
step 86620; step_loss: 0.0925
step 86630; step_loss: 0.1558
step 86640; step_loss: 0.1392
step 86650; step_loss: 0.0956
step 86660; step_loss: 0.0876
step 86670; step_loss: 0.1379
step 86680; step_loss: 0.1199
step 86690; step_loss: 0.0824
step 86700; step_loss: 0.1008
step 86710; step_loss: 0.1340
step 86720; step_loss: 0.1249
step 86730; step_loss: 0.1022
step 86740; step_loss: 0.0832
step 86750; step_loss: 0.1335
step 86760; step_loss: 0.1315
step 86770; step_loss: 0.0994
step 86780; step_loss: 0.0944
step 86790; step_loss: 0.1227
step 86800; step_loss: 0.1583
step 86810; step_loss: 0.0869
step 86820; step_loss: 0.1189
step 86830; step_loss: 0.1159
step 86840; step_loss: 0.1528
step 86850; step_loss: 0.0796
step 86860; step_loss: 0.1003
step 86870; step_loss: 0.1184
step 86880; step_loss: 0.1108
step 86890; step_loss: 0.1616
step 86900; step_loss: 0.0938
step 86910; step_loss: 0.1292
step 86920; step_loss: 0.1116
step 86930; step_loss: 0.1249
step 86940; step_loss: 0.1581
step 86950; step_loss: 0.0752
step 86960; step_loss: 0.1273
step 86970; step_loss: 0.1125
step 86980; step_loss: 0.1041
step 86990; step_loss: 0.1290

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 0.955 | 1.111 | 1.402 | 1.519 | 1.693 | 1.945 |

============================
Global step:         87000
Learning rate:       0.0033
Step-time (ms):     35.8136
Train loss avg:      0.1189
--------------------------
Val loss:            0.9419
srnn loss:           0.7974
============================

Saving the model...
done in 398.27 ms
step 87000; step_loss: 0.1394
step 87010; step_loss: 0.0906
step 87020; step_loss: 0.1068
step 87030; step_loss: 0.0892
step 87040; step_loss: 0.1036
step 87050; step_loss: 0.1414
step 87060; step_loss: 0.0956
step 87070; step_loss: 0.1383
step 87080; step_loss: 0.1473
step 87090; step_loss: 0.1428
step 87100; step_loss: 0.1062
step 87110; step_loss: 0.1154
step 87120; step_loss: 0.0889
step 87130; step_loss: 0.1477
step 87140; step_loss: 0.0823
step 87150; step_loss: 0.0877
step 87160; step_loss: 0.0766
step 87170; step_loss: 0.0936
step 87180; step_loss: 0.1499
step 87190; step_loss: 0.1462
step 87200; step_loss: 0.0781
step 87210; step_loss: 0.0789
step 87220; step_loss: 0.1828
step 87230; step_loss: 0.1065
step 87240; step_loss: 0.1401
step 87250; step_loss: 0.1093
step 87260; step_loss: 0.1551
step 87270; step_loss: 0.1252
step 87280; step_loss: 0.1043
step 87290; step_loss: 0.1516
step 87300; step_loss: 0.1174
step 87310; step_loss: 0.1706
step 87320; step_loss: 0.1577
step 87330; step_loss: 0.1339
step 87340; step_loss: 0.1234
step 87350; step_loss: 0.1073
step 87360; step_loss: 0.1474
step 87370; step_loss: 0.1693
step 87380; step_loss: 0.0893
step 87390; step_loss: 0.0886
step 87400; step_loss: 0.1175
step 87410; step_loss: 0.1168
step 87420; step_loss: 0.1183
step 87430; step_loss: 0.0963
step 87440; step_loss: 0.1000
step 87450; step_loss: 0.1459
step 87460; step_loss: 0.0974
step 87470; step_loss: 0.1112
step 87480; step_loss: 0.1424
step 87490; step_loss: 0.1264
step 87500; step_loss: 0.1276
step 87510; step_loss: 0.0946
step 87520; step_loss: 0.1049
step 87530; step_loss: 0.1486
step 87540; step_loss: 0.1631
step 87550; step_loss: 0.1097
step 87560; step_loss: 0.1058
step 87570; step_loss: 0.0929
step 87580; step_loss: 0.1236
step 87590; step_loss: 0.1146
step 87600; step_loss: 0.1366
step 87610; step_loss: 0.1007
step 87620; step_loss: 0.2039
step 87630; step_loss: 0.0897
step 87640; step_loss: 0.1148
step 87650; step_loss: 0.1068
step 87660; step_loss: 0.1334
step 87670; step_loss: 0.1134
step 87680; step_loss: 0.0911
step 87690; step_loss: 0.1800
step 87700; step_loss: 0.0814
step 87710; step_loss: 0.1510
step 87720; step_loss: 0.0917
step 87730; step_loss: 0.1183
step 87740; step_loss: 0.1216
step 87750; step_loss: 0.1236
step 87760; step_loss: 0.1051
step 87770; step_loss: 0.1084
step 87780; step_loss: 0.1928
step 87790; step_loss: 0.0937
step 87800; step_loss: 0.1292
step 87810; step_loss: 0.0760
step 87820; step_loss: 0.1274
step 87830; step_loss: 0.0947
step 87840; step_loss: 0.1734
step 87850; step_loss: 0.1278
step 87860; step_loss: 0.1101
step 87870; step_loss: 0.0854
step 87880; step_loss: 0.1172
step 87890; step_loss: 0.1051
step 87900; step_loss: 0.1149
step 87910; step_loss: 0.0955
step 87920; step_loss: 0.1575
step 87930; step_loss: 0.0897
step 87940; step_loss: 0.1381
step 87950; step_loss: 0.1340
step 87960; step_loss: 0.0929
step 87970; step_loss: 0.1152
step 87980; step_loss: 0.1281
step 87990; step_loss: 0.1386

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 0.959 | 1.119 | 1.426 | 1.549 | 1.732 | 1.999 |

============================
Global step:         88000
Learning rate:       0.0033
Step-time (ms):     35.6748
Train loss avg:      0.1186
--------------------------
Val loss:            0.8763
srnn loss:           0.8144
============================

Saving the model...
done in 451.78 ms
step 88000; step_loss: 0.0770
step 88010; step_loss: 0.1795
step 88020; step_loss: 0.0698
step 88030; step_loss: 0.1486
step 88040; step_loss: 0.1048
step 88050; step_loss: 0.1235
step 88060; step_loss: 0.1363
step 88070; step_loss: 0.1526
step 88080; step_loss: 0.0805
step 88090; step_loss: 0.1391
step 88100; step_loss: 0.1132
step 88110; step_loss: 0.1185
step 88120; step_loss: 0.1630
step 88130; step_loss: 0.1229
step 88140; step_loss: 0.1407
step 88150; step_loss: 0.1344
step 88160; step_loss: 0.1282
step 88170; step_loss: 0.0913
step 88180; step_loss: 0.1235
step 88190; step_loss: 0.0960
step 88200; step_loss: 0.1012
step 88210; step_loss: 0.0779
step 88220; step_loss: 0.0963
step 88230; step_loss: 0.1107
step 88240; step_loss: 0.0946
step 88250; step_loss: 0.1073
step 88260; step_loss: 0.0967
step 88270; step_loss: 0.1639
step 88280; step_loss: 0.0810
step 88290; step_loss: 0.0992
step 88300; step_loss: 0.0754
step 88310; step_loss: 0.1499
step 88320; step_loss: 0.1406
step 88330; step_loss: 0.1602
step 88340; step_loss: 0.1044
step 88350; step_loss: 0.1642
step 88360; step_loss: 0.0973
step 88370; step_loss: 0.1015
step 88380; step_loss: 0.0912
step 88390; step_loss: 0.0902
step 88400; step_loss: 0.1091
step 88410; step_loss: 0.0693
step 88420; step_loss: 0.0839
step 88430; step_loss: 0.1017
step 88440; step_loss: 0.1281
step 88450; step_loss: 0.1094
step 88460; step_loss: 0.1179
step 88470; step_loss: 0.1208
step 88480; step_loss: 0.0938
step 88490; step_loss: 0.0984
step 88500; step_loss: 0.1257
step 88510; step_loss: 0.1233
step 88520; step_loss: 0.0962
step 88530; step_loss: 0.1241
step 88540; step_loss: 0.1209
step 88550; step_loss: 0.0928
step 88560; step_loss: 0.1138
step 88570; step_loss: 0.1174
step 88580; step_loss: 0.0840
step 88590; step_loss: 0.0926
step 88600; step_loss: 0.0848
step 88610; step_loss: 0.1283
step 88620; step_loss: 0.1096
step 88630; step_loss: 0.0796
step 88640; step_loss: 0.1182
step 88650; step_loss: 0.0944
step 88660; step_loss: 0.1226
step 88670; step_loss: 0.1029
step 88680; step_loss: 0.0965
step 88690; step_loss: 0.0943
step 88700; step_loss: 0.1079
step 88710; step_loss: 0.1349
step 88720; step_loss: 0.1424
step 88730; step_loss: 0.0988
step 88740; step_loss: 0.1198
step 88750; step_loss: 0.0914
step 88760; step_loss: 0.1258
step 88770; step_loss: 0.1798
step 88780; step_loss: 0.0772
step 88790; step_loss: 0.1040
step 88800; step_loss: 0.1129
step 88810; step_loss: 0.1457
step 88820; step_loss: 0.1369
step 88830; step_loss: 0.0948
step 88840; step_loss: 0.1116
step 88850; step_loss: 0.1070
step 88860; step_loss: 0.0956
step 88870; step_loss: 0.1353
step 88880; step_loss: 0.1009
step 88890; step_loss: 0.0927
step 88900; step_loss: 0.0940
step 88910; step_loss: 0.1304
step 88920; step_loss: 0.1318
step 88930; step_loss: 0.1609
step 88940; step_loss: 0.1306
step 88950; step_loss: 0.1082
step 88960; step_loss: 0.0775
step 88970; step_loss: 0.0913
step 88980; step_loss: 0.1756
step 88990; step_loss: 0.1153

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 0.957 | 1.119 | 1.424 | 1.547 | 1.730 | 2.005 |

============================
Global step:         89000
Learning rate:       0.0033
Step-time (ms):     35.7489
Train loss avg:      0.1158
--------------------------
Val loss:            0.9230
srnn loss:           0.8248
============================

Saving the model...
done in 404.08 ms
step 89000; step_loss: 0.1900
step 89010; step_loss: 0.0732
step 89020; step_loss: 0.0931
step 89030; step_loss: 0.0845
step 89040; step_loss: 0.0857
step 89050; step_loss: 0.1167
step 89060; step_loss: 0.1135
step 89070; step_loss: 0.1173
step 89080; step_loss: 0.1329
step 89090; step_loss: 0.0866
step 89100; step_loss: 0.1620
step 89110; step_loss: 0.1480
step 89120; step_loss: 0.0931
step 89130; step_loss: 0.1201
step 89140; step_loss: 0.1186
step 89150; step_loss: 0.1044
step 89160; step_loss: 0.0904
step 89170; step_loss: 0.1391
step 89180; step_loss: 0.1219
step 89190; step_loss: 0.1420
step 89200; step_loss: 0.1094
step 89210; step_loss: 0.1165
step 89220; step_loss: 0.1092
step 89230; step_loss: 0.1214
step 89240; step_loss: 0.0931
step 89250; step_loss: 0.1156
step 89260; step_loss: 0.1270
step 89270; step_loss: 0.1019
step 89280; step_loss: 0.1309
step 89290; step_loss: 0.1176
step 89300; step_loss: 0.1331
step 89310; step_loss: 0.0950
step 89320; step_loss: 0.1295
step 89330; step_loss: 0.1139
step 89340; step_loss: 0.1287
step 89350; step_loss: 0.1031
step 89360; step_loss: 0.0901
step 89370; step_loss: 0.0862
step 89380; step_loss: 0.1096
step 89390; step_loss: 0.1011
step 89400; step_loss: 0.1164
step 89410; step_loss: 0.1138
step 89420; step_loss: 0.1306
step 89430; step_loss: 0.1068
step 89440; step_loss: 0.1373
step 89450; step_loss: 0.1258
step 89460; step_loss: 0.1368
step 89470; step_loss: 0.1420
step 89480; step_loss: 0.1046
step 89490; step_loss: 0.0921
step 89500; step_loss: 0.0821
step 89510; step_loss: 0.0992
step 89520; step_loss: 0.1274
step 89530; step_loss: 0.1199
step 89540; step_loss: 0.1027
step 89550; step_loss: 0.0915
step 89560; step_loss: 0.1165
step 89570; step_loss: 0.1089
step 89580; step_loss: 0.1126
step 89590; step_loss: 0.1141
step 89600; step_loss: 0.1031
step 89610; step_loss: 0.1101
step 89620; step_loss: 0.1151
step 89630; step_loss: 0.0876
step 89640; step_loss: 0.1107
step 89650; step_loss: 0.1232
step 89660; step_loss: 0.1023
step 89670; step_loss: 0.1001
step 89680; step_loss: 0.0897
step 89690; step_loss: 0.1353
step 89700; step_loss: 0.0987
step 89710; step_loss: 0.1135
step 89720; step_loss: 0.1453
step 89730; step_loss: 0.1175
step 89740; step_loss: 0.1159
step 89750; step_loss: 0.1001
step 89760; step_loss: 0.1386
step 89770; step_loss: 0.1135
step 89780; step_loss: 0.1456
step 89790; step_loss: 0.1055
step 89800; step_loss: 0.1228
step 89810; step_loss: 0.1381
step 89820; step_loss: 0.1267
step 89830; step_loss: 0.0935
step 89840; step_loss: 0.0659
step 89850; step_loss: 0.1109
step 89860; step_loss: 0.1081
step 89870; step_loss: 0.1088
step 89880; step_loss: 0.1201
step 89890; step_loss: 0.0973
step 89900; step_loss: 0.1480
step 89910; step_loss: 0.1376
step 89920; step_loss: 0.0915
step 89930; step_loss: 0.0977
step 89940; step_loss: 0.1409
step 89950; step_loss: 0.1124
step 89960; step_loss: 0.1028
step 89970; step_loss: 0.1325
step 89980; step_loss: 0.1558
step 89990; step_loss: 0.0916

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 0.957 | 1.117 | 1.421 | 1.544 | 1.725 | 1.983 |

============================
Global step:         90000
Learning rate:       0.0032
Step-time (ms):     35.8196
Train loss avg:      0.1149
--------------------------
Val loss:            0.8900
srnn loss:           0.8164
============================

Saving the model...
done in 401.33 ms
step 90000; step_loss: 0.0810
step 90010; step_loss: 0.1118
step 90020; step_loss: 0.1090
step 90030; step_loss: 0.1385
step 90040; step_loss: 0.1108
step 90050; step_loss: 0.1339
step 90060; step_loss: 0.1179
step 90070; step_loss: 0.1214
step 90080; step_loss: 0.1276
step 90090; step_loss: 0.1209
step 90100; step_loss: 0.1665
step 90110; step_loss: 0.0936
step 90120; step_loss: 0.1136
step 90130; step_loss: 0.0692
step 90140; step_loss: 0.1241
step 90150; step_loss: 0.0996
step 90160; step_loss: 0.1011
step 90170; step_loss: 0.1126
step 90180; step_loss: 0.1045
step 90190; step_loss: 0.1379
step 90200; step_loss: 0.1589
step 90210; step_loss: 0.1328
step 90220; step_loss: 0.1105
step 90230; step_loss: 0.1646
step 90240; step_loss: 0.1302
step 90250; step_loss: 0.1007
step 90260; step_loss: 0.1784
step 90270; step_loss: 0.1067
step 90280; step_loss: 0.1220
step 90290; step_loss: 0.0910
step 90300; step_loss: 0.1585
step 90310; step_loss: 0.0757
step 90320; step_loss: 0.0765
step 90330; step_loss: 0.0940
step 90340; step_loss: 0.1269
step 90350; step_loss: 0.1096
step 90360; step_loss: 0.0806
step 90370; step_loss: 0.1494
step 90380; step_loss: 0.0888
step 90390; step_loss: 0.1313
step 90400; step_loss: 0.0897
step 90410; step_loss: 0.1100
step 90420; step_loss: 0.1133
step 90430; step_loss: 0.1202
step 90440; step_loss: 0.0869
step 90450; step_loss: 0.1062
step 90460; step_loss: 0.1068
step 90470; step_loss: 0.1049
step 90480; step_loss: 0.1061
step 90490; step_loss: 0.1098
step 90500; step_loss: 0.1606
step 90510; step_loss: 0.1134
step 90520; step_loss: 0.1775
step 90530; step_loss: 0.1253
step 90540; step_loss: 0.1217
step 90550; step_loss: 0.1330
step 90560; step_loss: 0.1062
step 90570; step_loss: 0.1150
step 90580; step_loss: 0.0937
step 90590; step_loss: 0.0898
step 90600; step_loss: 0.1095
step 90610; step_loss: 0.1008
step 90620; step_loss: 0.0947
step 90630; step_loss: 0.1095
step 90640; step_loss: 0.0773
step 90650; step_loss: 0.1229
step 90660; step_loss: 0.1114
step 90670; step_loss: 0.0699
step 90680; step_loss: 0.1340
step 90690; step_loss: 0.0982
step 90700; step_loss: 0.1108
step 90710; step_loss: 0.1172
step 90720; step_loss: 0.0742
step 90730; step_loss: 0.1146
step 90740; step_loss: 0.1243
step 90750; step_loss: 0.1313
step 90760; step_loss: 0.0872
step 90770; step_loss: 0.1018
step 90780; step_loss: 0.1272
step 90790; step_loss: 0.1227
step 90800; step_loss: 0.0904
step 90810; step_loss: 0.0978
step 90820; step_loss: 0.0914
step 90830; step_loss: 0.0897
step 90840; step_loss: 0.1189
step 90850; step_loss: 0.0945
step 90860; step_loss: 0.1018
step 90870; step_loss: 0.1301
step 90880; step_loss: 0.0933
step 90890; step_loss: 0.1135
step 90900; step_loss: 0.1271
step 90910; step_loss: 0.2034
step 90920; step_loss: 0.0933
step 90930; step_loss: 0.1106
step 90940; step_loss: 0.1070
step 90950; step_loss: 0.1182
step 90960; step_loss: 0.1282
step 90970; step_loss: 0.1086
step 90980; step_loss: 0.0784
step 90990; step_loss: 0.0874

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 0.957 | 1.118 | 1.421 | 1.543 | 1.724 | 1.991 |

============================
Global step:         91000
Learning rate:       0.0032
Step-time (ms):     35.9047
Train loss avg:      0.1156
--------------------------
Val loss:            0.9816
srnn loss:           0.8277
============================

Saving the model...
done in 450.00 ms
step 91000; step_loss: 0.1379
step 91010; step_loss: 0.1014
step 91020; step_loss: 0.0828
step 91030; step_loss: 0.1433
step 91040; step_loss: 0.1239
step 91050; step_loss: 0.1435
step 91060; step_loss: 0.1118
step 91070; step_loss: 0.1094
step 91080; step_loss: 0.1650
step 91090; step_loss: 0.1113
step 91100; step_loss: 0.1275
step 91110; step_loss: 0.1289
step 91120; step_loss: 0.1003
step 91130; step_loss: 0.1253
step 91140; step_loss: 0.1293
step 91150; step_loss: 0.1397
step 91160; step_loss: 0.1001
step 91170; step_loss: 0.1037
step 91180; step_loss: 0.0686
step 91190; step_loss: 0.1159
step 91200; step_loss: 0.1166
step 91210; step_loss: 0.1518
step 91220; step_loss: 0.1236
step 91230; step_loss: 0.1032
step 91240; step_loss: 0.1019
step 91250; step_loss: 0.0842
step 91260; step_loss: 0.1206
step 91270; step_loss: 0.0895
step 91280; step_loss: 0.0900
step 91290; step_loss: 0.1137
step 91300; step_loss: 0.0846
step 91310; step_loss: 0.1106
step 91320; step_loss: 0.1083
step 91330; step_loss: 0.1290
step 91340; step_loss: 0.1070
step 91350; step_loss: 0.1705
step 91360; step_loss: 0.1328
step 91370; step_loss: 0.1007
step 91380; step_loss: 0.0935
step 91390; step_loss: 0.1103
step 91400; step_loss: 0.0985
step 91410; step_loss: 0.1171
step 91420; step_loss: 0.0849
step 91430; step_loss: 0.0869
step 91440; step_loss: 0.0985
step 91450; step_loss: 0.1228
step 91460; step_loss: 0.1115
step 91470; step_loss: 0.0870
step 91480; step_loss: 0.0726
step 91490; step_loss: 0.0988
step 91500; step_loss: 0.1046
step 91510; step_loss: 0.1466
step 91520; step_loss: 0.0797
step 91530; step_loss: 0.1121
step 91540; step_loss: 0.1018
step 91550; step_loss: 0.0981
step 91560; step_loss: 0.1155
step 91570; step_loss: 0.1325
step 91580; step_loss: 0.0999
step 91590; step_loss: 0.1024
step 91600; step_loss: 0.1410
step 91610; step_loss: 0.1424
step 91620; step_loss: 0.1044
step 91630; step_loss: 0.0970
step 91640; step_loss: 0.1256
step 91650; step_loss: 0.1015
step 91660; step_loss: 0.0919
step 91670; step_loss: 0.1092
step 91680; step_loss: 0.1542
step 91690; step_loss: 0.1126
step 91700; step_loss: 0.1333
step 91710; step_loss: 0.1245
step 91720; step_loss: 0.1177
step 91730; step_loss: 0.1306
step 91740; step_loss: 0.0920
step 91750; step_loss: 0.1020
step 91760; step_loss: 0.1066
step 91770; step_loss: 0.1446
step 91780; step_loss: 0.0908
step 91790; step_loss: 0.0966
step 91800; step_loss: 0.1377
step 91810; step_loss: 0.1171
step 91820; step_loss: 0.1083
step 91830; step_loss: 0.0789
step 91840; step_loss: 0.1207
step 91850; step_loss: 0.1107
step 91860; step_loss: 0.1367
step 91870; step_loss: 0.1105
step 91880; step_loss: 0.1033
step 91890; step_loss: 0.0860
step 91900; step_loss: 0.0923
step 91910; step_loss: 0.0942
step 91920; step_loss: 0.1098
step 91930; step_loss: 0.1286
step 91940; step_loss: 0.1119
step 91950; step_loss: 0.1328
step 91960; step_loss: 0.1386
step 91970; step_loss: 0.1143
step 91980; step_loss: 0.1191
step 91990; step_loss: 0.1342

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 0.960 | 1.121 | 1.423 | 1.545 | 1.722 | 1.966 |

============================
Global step:         92000
Learning rate:       0.0032
Step-time (ms):     35.8190
Train loss avg:      0.1153
--------------------------
Val loss:            0.9795
srnn loss:           0.8134
============================

Saving the model...
done in 401.76 ms
step 92000; step_loss: 0.0988
step 92010; step_loss: 0.1305
step 92020; step_loss: 0.0853
step 92030; step_loss: 0.0919
step 92040; step_loss: 0.0870
step 92050; step_loss: 0.0763
step 92060; step_loss: 0.1070
step 92070; step_loss: 0.1173
step 92080; step_loss: 0.1469
step 92090; step_loss: 0.1196
step 92100; step_loss: 0.1015
step 92110; step_loss: 0.1361
step 92120; step_loss: 0.1046
step 92130; step_loss: 0.1179
step 92140; step_loss: 0.1073
step 92150; step_loss: 0.0848
step 92160; step_loss: 0.1416
step 92170; step_loss: 0.0941
step 92180; step_loss: 0.1075
step 92190; step_loss: 0.1148
step 92200; step_loss: 0.1027
step 92210; step_loss: 0.1088
step 92220; step_loss: 0.1016
step 92230; step_loss: 0.1348
step 92240; step_loss: 0.1147
step 92250; step_loss: 0.1332
step 92260; step_loss: 0.1154
step 92270; step_loss: 0.1072
step 92280; step_loss: 0.1204
step 92290; step_loss: 0.1013
step 92300; step_loss: 0.0995
step 92310; step_loss: 0.1041
step 92320; step_loss: 0.1385
step 92330; step_loss: 0.1229
step 92340; step_loss: 0.1125
step 92350; step_loss: 0.1365
step 92360; step_loss: 0.1848
step 92370; step_loss: 0.0776
step 92380; step_loss: 0.0901
step 92390; step_loss: 0.1072
step 92400; step_loss: 0.0971
step 92410; step_loss: 0.1234
step 92420; step_loss: 0.0868
step 92430; step_loss: 0.1022
step 92440; step_loss: 0.1629
step 92450; step_loss: 0.1046
step 92460; step_loss: 0.0816
step 92470; step_loss: 0.1112
step 92480; step_loss: 0.1253
step 92490; step_loss: 0.0941
step 92500; step_loss: 0.1133
step 92510; step_loss: 0.0912
step 92520; step_loss: 0.0937
step 92530; step_loss: 0.1001
step 92540; step_loss: 0.1306
step 92550; step_loss: 0.1144
step 92560; step_loss: 0.1132
step 92570; step_loss: 0.0914
step 92580; step_loss: 0.1153
step 92590; step_loss: 0.1156
step 92600; step_loss: 0.0858
step 92610; step_loss: 0.1517
step 92620; step_loss: 0.1028
step 92630; step_loss: 0.0959
step 92640; step_loss: 0.1111
step 92650; step_loss: 0.1198
step 92660; step_loss: 0.1536
step 92670; step_loss: 0.1340
step 92680; step_loss: 0.0945
step 92690; step_loss: 0.1226
step 92700; step_loss: 0.1016
step 92710; step_loss: 0.1165
step 92720; step_loss: 0.1750
step 92730; step_loss: 0.1140
step 92740; step_loss: 0.1160
step 92750; step_loss: 0.0824
step 92760; step_loss: 0.1427
step 92770; step_loss: 0.0962
step 92780; step_loss: 0.1081
step 92790; step_loss: 0.1780
step 92800; step_loss: 0.1266
step 92810; step_loss: 0.1180
step 92820; step_loss: 0.1294
step 92830; step_loss: 0.0876
step 92840; step_loss: 0.1426
step 92850; step_loss: 0.1084
step 92860; step_loss: 0.1433
step 92870; step_loss: 0.1168
step 92880; step_loss: 0.0786
step 92890; step_loss: 0.1048
step 92900; step_loss: 0.1283
step 92910; step_loss: 0.1015
step 92920; step_loss: 0.1405
step 92930; step_loss: 0.0818
step 92940; step_loss: 0.1029
step 92950; step_loss: 0.1121
step 92960; step_loss: 0.1457
step 92970; step_loss: 0.0915
step 92980; step_loss: 0.0914
step 92990; step_loss: 0.1377

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 0.952 | 1.111 | 1.411 | 1.532 | 1.709 | 1.965 |

============================
Global step:         93000
Learning rate:       0.0032
Step-time (ms):     35.9014
Train loss avg:      0.1141
--------------------------
Val loss:            0.9501
srnn loss:           0.8120
============================

Saving the model...
done in 433.59 ms
step 93000; step_loss: 0.0863
step 93010; step_loss: 0.0896
step 93020; step_loss: 0.1244
step 93030; step_loss: 0.0818
step 93040; step_loss: 0.0868
step 93050; step_loss: 0.0977
step 93060; step_loss: 0.1086
step 93070; step_loss: 0.1049
step 93080; step_loss: 0.1228
step 93090; step_loss: 0.1532
step 93100; step_loss: 0.0960
step 93110; step_loss: 0.1077
step 93120; step_loss: 0.1172
step 93130; step_loss: 0.1011
step 93140; step_loss: 0.1023
step 93150; step_loss: 0.0951
step 93160; step_loss: 0.1071
step 93170; step_loss: 0.0855
step 93180; step_loss: 0.0791
step 93190; step_loss: 0.1174
step 93200; step_loss: 0.1133
step 93210; step_loss: 0.1083
step 93220; step_loss: 0.1349
step 93230; step_loss: 0.1210
step 93240; step_loss: 0.1245
step 93250; step_loss: 0.1220
step 93260; step_loss: 0.1079
step 93270; step_loss: 0.0851
step 93280; step_loss: 0.1321
step 93290; step_loss: 0.1403
step 93300; step_loss: 0.1175
step 93310; step_loss: 0.0984
step 93320; step_loss: 0.1225
step 93330; step_loss: 0.0874
step 93340; step_loss: 0.1449
step 93350; step_loss: 0.0962
step 93360; step_loss: 0.0983
step 93370; step_loss: 0.0925
step 93380; step_loss: 0.1042
step 93390; step_loss: 0.1149
step 93400; step_loss: 0.1214
step 93410; step_loss: 0.1264
step 93420; step_loss: 0.1154
step 93430; step_loss: 0.0958
step 93440; step_loss: 0.0978
step 93450; step_loss: 0.1367
step 93460; step_loss: 0.1300
step 93470; step_loss: 0.0845
step 93480; step_loss: 0.1041
step 93490; step_loss: 0.0975
step 93500; step_loss: 0.1146
step 93510; step_loss: 0.0870
step 93520; step_loss: 0.1053
step 93530; step_loss: 0.1186
step 93540; step_loss: 0.1075
step 93550; step_loss: 0.1003
step 93560; step_loss: 0.1065
step 93570; step_loss: 0.1022
step 93580; step_loss: 0.1367
step 93590; step_loss: 0.1197
step 93600; step_loss: 0.1025
step 93610; step_loss: 0.1079
step 93620; step_loss: 0.1401
step 93630; step_loss: 0.1371
step 93640; step_loss: 0.1181
step 93650; step_loss: 0.0876
step 93660; step_loss: 0.1571
step 93670; step_loss: 0.1242
step 93680; step_loss: 0.1072
step 93690; step_loss: 0.0738
step 93700; step_loss: 0.1740
step 93710; step_loss: 0.1310
step 93720; step_loss: 0.1323
step 93730; step_loss: 0.1114
step 93740; step_loss: 0.0727
step 93750; step_loss: 0.0893
step 93760; step_loss: 0.1184
step 93770; step_loss: 0.0800
step 93780; step_loss: 0.1139
step 93790; step_loss: 0.1094
step 93800; step_loss: 0.1031
step 93810; step_loss: 0.0909
step 93820; step_loss: 0.1376
step 93830; step_loss: 0.1102
step 93840; step_loss: 0.1493
step 93850; step_loss: 0.1497
step 93860; step_loss: 0.1070
step 93870; step_loss: 0.1279
step 93880; step_loss: 0.1417
step 93890; step_loss: 0.1206
step 93900; step_loss: 0.1186
step 93910; step_loss: 0.1198
step 93920; step_loss: 0.1231
step 93930; step_loss: 0.1072
step 93940; step_loss: 0.1459
step 93950; step_loss: 0.1396
step 93960; step_loss: 0.1352
step 93970; step_loss: 0.1251
step 93980; step_loss: 0.1291
step 93990; step_loss: 0.1394

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 0.956 | 1.118 | 1.421 | 1.543 | 1.719 | 1.970 |

============================
Global step:         94000
Learning rate:       0.0032
Step-time (ms):     35.7011
Train loss avg:      0.1133
--------------------------
Val loss:            0.9327
srnn loss:           0.8212
============================

Saving the model...
done in 418.14 ms
step 94000; step_loss: 0.1108
step 94010; step_loss: 0.1160
step 94020; step_loss: 0.1075
step 94030; step_loss: 0.0906
step 94040; step_loss: 0.1068
step 94050; step_loss: 0.1388
step 94060; step_loss: 0.0894
step 94070; step_loss: 0.0859
step 94080; step_loss: 0.1372
step 94090; step_loss: 0.1040
step 94100; step_loss: 0.1443
step 94110; step_loss: 0.0981
step 94120; step_loss: 0.1334
step 94130; step_loss: 0.1284
step 94140; step_loss: 0.1502
step 94150; step_loss: 0.1330
step 94160; step_loss: 0.0951
step 94170; step_loss: 0.1106
step 94180; step_loss: 0.1454
step 94190; step_loss: 0.1140
step 94200; step_loss: 0.1401
step 94210; step_loss: 0.0863
step 94220; step_loss: 0.0799
step 94230; step_loss: 0.1068
step 94240; step_loss: 0.1394
step 94250; step_loss: 0.1245
step 94260; step_loss: 0.1153
step 94270; step_loss: 0.1457
step 94280; step_loss: 0.1945
step 94290; step_loss: 0.1023
step 94300; step_loss: 0.1119
step 94310; step_loss: 0.0929
step 94320; step_loss: 0.1031
step 94330; step_loss: 0.0899
step 94340; step_loss: 0.1852
step 94350; step_loss: 0.1249
step 94360; step_loss: 0.1050
step 94370; step_loss: 0.1418
step 94380; step_loss: 0.1313
step 94390; step_loss: 0.1169
step 94400; step_loss: 0.1238
step 94410; step_loss: 0.0970
step 94420; step_loss: 0.0840
step 94430; step_loss: 0.1199
step 94440; step_loss: 0.1204
step 94450; step_loss: 0.1008
step 94460; step_loss: 0.1113
step 94470; step_loss: 0.0904
step 94480; step_loss: 0.1373
step 94490; step_loss: 0.1126
step 94500; step_loss: 0.1075
step 94510; step_loss: 0.1518
step 94520; step_loss: 0.1064
step 94530; step_loss: 0.1106
step 94540; step_loss: 0.1106
step 94550; step_loss: 0.1012
step 94560; step_loss: 0.1045
step 94570; step_loss: 0.0686
step 94580; step_loss: 0.0936
step 94590; step_loss: 0.0931
step 94600; step_loss: 0.1067
step 94610; step_loss: 0.0904
step 94620; step_loss: 0.0884
step 94630; step_loss: 0.0763
step 94640; step_loss: 0.1123
step 94650; step_loss: 0.1965
step 94660; step_loss: 0.0936
step 94670; step_loss: 0.0975
step 94680; step_loss: 0.1234
step 94690; step_loss: 0.1107
step 94700; step_loss: 0.1014
step 94710; step_loss: 0.1361
step 94720; step_loss: 0.1111
step 94730; step_loss: 0.1462
step 94740; step_loss: 0.1018
step 94750; step_loss: 0.0990
step 94760; step_loss: 0.0942
step 94770; step_loss: 0.0972
step 94780; step_loss: 0.1147
step 94790; step_loss: 0.1103
step 94800; step_loss: 0.1328
step 94810; step_loss: 0.1103
step 94820; step_loss: 0.0992
step 94830; step_loss: 0.1224
step 94840; step_loss: 0.0974
step 94850; step_loss: 0.0886
step 94860; step_loss: 0.1037
step 94870; step_loss: 0.1118
step 94880; step_loss: 0.0875
step 94890; step_loss: 0.0899
step 94900; step_loss: 0.1013
step 94910; step_loss: 0.1434
step 94920; step_loss: 0.0828
step 94930; step_loss: 0.1324
step 94940; step_loss: 0.1502
step 94950; step_loss: 0.1232
step 94960; step_loss: 0.0886
step 94970; step_loss: 0.1475
step 94980; step_loss: 0.0963
step 94990; step_loss: 0.1398

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 0.956 | 1.119 | 1.423 | 1.544 | 1.719 | 1.957 |

============================
Global step:         95000
Learning rate:       0.0032
Step-time (ms):     35.7937
Train loss avg:      0.1127
--------------------------
Val loss:            1.0166
srnn loss:           0.8156
============================

Saving the model...
done in 426.97 ms
step 95000; step_loss: 0.1031
step 95010; step_loss: 0.1020
step 95020; step_loss: 0.1072
step 95030; step_loss: 0.0857
step 95040; step_loss: 0.0982
step 95050; step_loss: 0.0830
step 95060; step_loss: 0.0963
step 95070; step_loss: 0.1451
step 95080; step_loss: 0.1406
step 95090; step_loss: 0.1234
step 95100; step_loss: 0.0756
step 95110; step_loss: 0.1097
step 95120; step_loss: 0.1026
step 95130; step_loss: 0.0876
step 95140; step_loss: 0.1097
step 95150; step_loss: 0.1183
step 95160; step_loss: 0.1266
step 95170; step_loss: 0.1463
step 95180; step_loss: 0.1309
step 95190; step_loss: 0.1111
step 95200; step_loss: 0.1402
step 95210; step_loss: 0.0885
step 95220; step_loss: 0.0868
step 95230; step_loss: 0.1519
step 95240; step_loss: 0.1013
step 95250; step_loss: 0.1155
step 95260; step_loss: 0.1422
step 95270; step_loss: 0.0970
step 95280; step_loss: 0.1223
step 95290; step_loss: 0.1221
step 95300; step_loss: 0.0850
step 95310; step_loss: 0.1064
step 95320; step_loss: 0.0961
step 95330; step_loss: 0.1229
step 95340; step_loss: 0.1110
step 95350; step_loss: 0.1464
step 95360; step_loss: 0.1136
step 95370; step_loss: 0.1194
step 95380; step_loss: 0.1105
step 95390; step_loss: 0.1768
step 95400; step_loss: 0.1093
step 95410; step_loss: 0.1081
step 95420; step_loss: 0.1113
step 95430; step_loss: 0.1023
step 95440; step_loss: 0.1247
step 95450; step_loss: 0.0790
step 95460; step_loss: 0.0894
step 95470; step_loss: 0.1454
step 95480; step_loss: 0.0990
step 95490; step_loss: 0.1371
step 95500; step_loss: 0.1173
step 95510; step_loss: 0.1017
step 95520; step_loss: 0.0951
step 95530; step_loss: 0.0935
step 95540; step_loss: 0.0977
step 95550; step_loss: 0.1249
step 95560; step_loss: 0.0848
step 95570; step_loss: 0.0843
step 95580; step_loss: 0.1092
step 95590; step_loss: 0.0884
step 95600; step_loss: 0.1286
step 95610; step_loss: 0.0897
step 95620; step_loss: 0.1111
step 95630; step_loss: 0.0785
step 95640; step_loss: 0.1218
step 95650; step_loss: 0.1188
step 95660; step_loss: 0.0968
step 95670; step_loss: 0.0973
step 95680; step_loss: 0.0741
step 95690; step_loss: 0.1133
step 95700; step_loss: 0.0712
step 95710; step_loss: 0.1111
step 95720; step_loss: 0.1349
step 95730; step_loss: 0.0776
step 95740; step_loss: 0.1214
step 95750; step_loss: 0.1004
step 95760; step_loss: 0.1404
step 95770; step_loss: 0.0854
step 95780; step_loss: 0.0841
step 95790; step_loss: 0.1035
step 95800; step_loss: 0.1320
step 95810; step_loss: 0.1023
step 95820; step_loss: 0.1261
step 95830; step_loss: 0.1207
step 95840; step_loss: 0.0946
step 95850; step_loss: 0.1211
step 95860; step_loss: 0.1319
step 95870; step_loss: 0.0918
step 95880; step_loss: 0.1371
step 95890; step_loss: 0.0820
step 95900; step_loss: 0.1278
step 95910; step_loss: 0.0712
step 95920; step_loss: 0.0940
step 95930; step_loss: 0.0821
step 95940; step_loss: 0.1391
step 95950; step_loss: 0.1281
step 95960; step_loss: 0.1158
step 95970; step_loss: 0.1492
step 95980; step_loss: 0.1054
step 95990; step_loss: 0.1009

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 0.953 | 1.114 | 1.415 | 1.533 | 1.703 | 1.940 |

============================
Global step:         96000
Learning rate:       0.0032
Step-time (ms):     35.7055
Train loss avg:      0.1123
--------------------------
Val loss:            0.8792
srnn loss:           0.8098
============================

Saving the model...
done in 403.76 ms
step 96000; step_loss: 0.1242
step 96010; step_loss: 0.1398
step 96020; step_loss: 0.0941
step 96030; step_loss: 0.1161
step 96040; step_loss: 0.0793
step 96050; step_loss: 0.0972
step 96060; step_loss: 0.1795
step 96070; step_loss: 0.1265
step 96080; step_loss: 0.1042
step 96090; step_loss: 0.1290
step 96100; step_loss: 0.1264
step 96110; step_loss: 0.0832
step 96120; step_loss: 0.1782
step 96130; step_loss: 0.1106
step 96140; step_loss: 0.0939
step 96150; step_loss: 0.1028
step 96160; step_loss: 0.1157
step 96170; step_loss: 0.1214
step 96180; step_loss: 0.0878
step 96190; step_loss: 0.1024
step 96200; step_loss: 0.1189
step 96210; step_loss: 0.1186
step 96220; step_loss: 0.1084
step 96230; step_loss: 0.1017
step 96240; step_loss: 0.1079
step 96250; step_loss: 0.1351
step 96260; step_loss: 0.0947
step 96270; step_loss: 0.0792
step 96280; step_loss: 0.1211
step 96290; step_loss: 0.1034
step 96300; step_loss: 0.0935
step 96310; step_loss: 0.1272
step 96320; step_loss: 0.1016
step 96330; step_loss: 0.1026
step 96340; step_loss: 0.0957
step 96350; step_loss: 0.0777
step 96360; step_loss: 0.0837
step 96370; step_loss: 0.1063
step 96380; step_loss: 0.1213
step 96390; step_loss: 0.1015
step 96400; step_loss: 0.1134
step 96410; step_loss: 0.1271
step 96420; step_loss: 0.1183
step 96430; step_loss: 0.0973
step 96440; step_loss: 0.0736
step 96450; step_loss: 0.1182
step 96460; step_loss: 0.1209
step 96470; step_loss: 0.1248
step 96480; step_loss: 0.1153
step 96490; step_loss: 0.1619
step 96500; step_loss: 0.1251
step 96510; step_loss: 0.1247
step 96520; step_loss: 0.0963
step 96530; step_loss: 0.1025
step 96540; step_loss: 0.0962
step 96550; step_loss: 0.0870
step 96560; step_loss: 0.1269
step 96570; step_loss: 0.0918
step 96580; step_loss: 0.1138
step 96590; step_loss: 0.1114
step 96600; step_loss: 0.0811
step 96610; step_loss: 0.1048
step 96620; step_loss: 0.1071
step 96630; step_loss: 0.1362
step 96640; step_loss: 0.1075
step 96650; step_loss: 0.1197
step 96660; step_loss: 0.1165
step 96670; step_loss: 0.0962
step 96680; step_loss: 0.1018
step 96690; step_loss: 0.1034
step 96700; step_loss: 0.1432
step 96710; step_loss: 0.1140
step 96720; step_loss: 0.1029
step 96730; step_loss: 0.1175
step 96740; step_loss: 0.1281
step 96750; step_loss: 0.1152
step 96760; step_loss: 0.1123
step 96770; step_loss: 0.0795
step 96780; step_loss: 0.0848
step 96790; step_loss: 0.1257
step 96800; step_loss: 0.0938
step 96810; step_loss: 0.1165
step 96820; step_loss: 0.0899
step 96830; step_loss: 0.0956
step 96840; step_loss: 0.0884
step 96850; step_loss: 0.1158
step 96860; step_loss: 0.1276
step 96870; step_loss: 0.0806
step 96880; step_loss: 0.1375
step 96890; step_loss: 0.1393
step 96900; step_loss: 0.0970
step 96910; step_loss: 0.0829
step 96920; step_loss: 0.0862
step 96930; step_loss: 0.0996
step 96940; step_loss: 0.1092
step 96950; step_loss: 0.1059
step 96960; step_loss: 0.1007
step 96970; step_loss: 0.0856
step 96980; step_loss: 0.0836
step 96990; step_loss: 0.1530

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 0.955 | 1.121 | 1.431 | 1.555 | 1.731 | 1.983 |

============================
Global step:         97000
Learning rate:       0.0032
Step-time (ms):     35.7962
Train loss avg:      0.1140
--------------------------
Val loss:            1.0194
srnn loss:           0.8273
============================

Saving the model...
done in 438.01 ms
step 97000; step_loss: 0.1043
step 97010; step_loss: 0.1406
step 97020; step_loss: 0.1499
step 97030; step_loss: 0.1346
step 97040; step_loss: 0.1103
step 97050; step_loss: 0.1154
step 97060; step_loss: 0.1327
step 97070; step_loss: 0.1070
step 97080; step_loss: 0.1140
step 97090; step_loss: 0.1392
step 97100; step_loss: 0.0925
step 97110; step_loss: 0.1067
step 97120; step_loss: 0.1223
step 97130; step_loss: 0.1183
step 97140; step_loss: 0.0750
step 97150; step_loss: 0.1136
step 97160; step_loss: 0.1256
step 97170; step_loss: 0.0786
step 97180; step_loss: 0.1188
step 97190; step_loss: 0.1239
step 97200; step_loss: 0.1028
step 97210; step_loss: 0.1102
step 97220; step_loss: 0.1091
step 97230; step_loss: 0.0978
step 97240; step_loss: 0.1413
step 97250; step_loss: 0.0782
step 97260; step_loss: 0.1316
step 97270; step_loss: 0.1799
step 97280; step_loss: 0.1253
step 97290; step_loss: 0.1367
step 97300; step_loss: 0.1110
step 97310; step_loss: 0.1040
step 97320; step_loss: 0.1503
step 97330; step_loss: 0.1090
step 97340; step_loss: 0.1296
step 97350; step_loss: 0.0851
step 97360; step_loss: 0.1359
step 97370; step_loss: 0.0992
step 97380; step_loss: 0.1167
step 97390; step_loss: 0.1084
step 97400; step_loss: 0.1018
step 97410; step_loss: 0.1082
step 97420; step_loss: 0.0858
step 97430; step_loss: 0.1369
step 97440; step_loss: 0.0980
step 97450; step_loss: 0.0874
step 97460; step_loss: 0.0898
step 97470; step_loss: 0.1081
step 97480; step_loss: 0.1095
step 97490; step_loss: 0.1264
step 97500; step_loss: 0.1617
step 97510; step_loss: 0.1111
step 97520; step_loss: 0.1153
step 97530; step_loss: 0.1179
step 97540; step_loss: 0.1627
step 97550; step_loss: 0.0860
step 97560; step_loss: 0.0946
step 97570; step_loss: 0.1162
step 97580; step_loss: 0.1054
step 97590; step_loss: 0.1026
step 97600; step_loss: 0.0759
step 97610; step_loss: 0.0857
step 97620; step_loss: 0.1032
step 97630; step_loss: 0.0912
step 97640; step_loss: 0.1047
step 97650; step_loss: 0.0559
step 97660; step_loss: 0.1172
step 97670; step_loss: 0.1103
step 97680; step_loss: 0.1016
step 97690; step_loss: 0.1616
step 97700; step_loss: 0.1070
step 97710; step_loss: 0.0902
step 97720; step_loss: 0.0943
step 97730; step_loss: 0.0964
step 97740; step_loss: 0.1035
step 97750; step_loss: 0.1015
step 97760; step_loss: 0.0944
step 97770; step_loss: 0.0855
step 97780; step_loss: 0.1329
step 97790; step_loss: 0.1083
step 97800; step_loss: 0.1175
step 97810; step_loss: 0.1023
step 97820; step_loss: 0.1404
step 97830; step_loss: 0.1331
step 97840; step_loss: 0.1583
step 97850; step_loss: 0.1484
step 97860; step_loss: 0.0886
step 97870; step_loss: 0.0805
step 97880; step_loss: 0.1243
step 97890; step_loss: 0.1276
step 97900; step_loss: 0.1335
step 97910; step_loss: 0.1117
step 97920; step_loss: 0.0948
step 97930; step_loss: 0.1138
step 97940; step_loss: 0.1090
step 97950; step_loss: 0.1282
step 97960; step_loss: 0.1015
step 97970; step_loss: 0.1100
step 97980; step_loss: 0.1850
step 97990; step_loss: 0.1549

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 0.957 | 1.123 | 1.431 | 1.555 | 1.736 | 1.993 |

============================
Global step:         98000
Learning rate:       0.0032
Step-time (ms):     35.7956
Train loss avg:      0.1116
--------------------------
Val loss:            0.9827
srnn loss:           0.8389
============================

Saving the model...
done in 408.01 ms
step 98000; step_loss: 0.1316
step 98010; step_loss: 0.1093
step 98020; step_loss: 0.1005
step 98030; step_loss: 0.1194
step 98040; step_loss: 0.1256
step 98050; step_loss: 0.1191
step 98060; step_loss: 0.1155
step 98070; step_loss: 0.1115
step 98080; step_loss: 0.1130
step 98090; step_loss: 0.1813
step 98100; step_loss: 0.0837
step 98110; step_loss: 0.0717
step 98120; step_loss: 0.2102
step 98130; step_loss: 0.1114
step 98140; step_loss: 0.0995
step 98150; step_loss: 0.0819
step 98160; step_loss: 0.1136
step 98170; step_loss: 0.0817
step 98180; step_loss: 0.0955
step 98190; step_loss: 0.1198
step 98200; step_loss: 0.0752
step 98210; step_loss: 0.0991
step 98220; step_loss: 0.1001
step 98230; step_loss: 0.0848
step 98240; step_loss: 0.1046
step 98250; step_loss: 0.0919
step 98260; step_loss: 0.1718
step 98270; step_loss: 0.1271
step 98280; step_loss: 0.1516
step 98290; step_loss: 0.1151
step 98300; step_loss: 0.0738
step 98310; step_loss: 0.0831
step 98320; step_loss: 0.1077
step 98330; step_loss: 0.1183
step 98340; step_loss: 0.1029
step 98350; step_loss: 0.1521
step 98360; step_loss: 0.1017
step 98370; step_loss: 0.1363
step 98380; step_loss: 0.1106
step 98390; step_loss: 0.1470
step 98400; step_loss: 0.1001
step 98410; step_loss: 0.1148
step 98420; step_loss: 0.1275
step 98430; step_loss: 0.1478
step 98440; step_loss: 0.1204
step 98450; step_loss: 0.0789
step 98460; step_loss: 0.0999
step 98470; step_loss: 0.1420
step 98480; step_loss: 0.1128
step 98490; step_loss: 0.1438
step 98500; step_loss: 0.1063
step 98510; step_loss: 0.1028
step 98520; step_loss: 0.1065
step 98530; step_loss: 0.0807
step 98540; step_loss: 0.1048
step 98550; step_loss: 0.1164
step 98560; step_loss: 0.1296
step 98570; step_loss: 0.1319
step 98580; step_loss: 0.0884
step 98590; step_loss: 0.1014
step 98600; step_loss: 0.1028
step 98610; step_loss: 0.1023
step 98620; step_loss: 0.1034
step 98630; step_loss: 0.0920
step 98640; step_loss: 0.1028
step 98650; step_loss: 0.1254
step 98660; step_loss: 0.1144
step 98670; step_loss: 0.1026
step 98680; step_loss: 0.0908
step 98690; step_loss: 0.1090
step 98700; step_loss: 0.1335
step 98710; step_loss: 0.1130
step 98720; step_loss: 0.1171
step 98730; step_loss: 0.1266
step 98740; step_loss: 0.1236
step 98750; step_loss: 0.1237
step 98760; step_loss: 0.0946
step 98770; step_loss: 0.1037
step 98780; step_loss: 0.1146
step 98790; step_loss: 0.1551
step 98800; step_loss: 0.1006
step 98810; step_loss: 0.1008
step 98820; step_loss: 0.0976
step 98830; step_loss: 0.1267
step 98840; step_loss: 0.1038
step 98850; step_loss: 0.1352
step 98860; step_loss: 0.0926
step 98870; step_loss: 0.1154
step 98880; step_loss: 0.0980
step 98890; step_loss: 0.1392
step 98900; step_loss: 0.1201
step 98910; step_loss: 0.1025
step 98920; step_loss: 0.1089
step 98930; step_loss: 0.1068
step 98940; step_loss: 0.0909
step 98950; step_loss: 0.1253
step 98960; step_loss: 0.1211
step 98970; step_loss: 0.0909
step 98980; step_loss: 0.1065
step 98990; step_loss: 0.0990

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 0.949 | 1.112 | 1.417 | 1.539 | 1.715 | 1.960 |

============================
Global step:         99000
Learning rate:       0.0032
Step-time (ms):     35.8772
Train loss avg:      0.1112
--------------------------
Val loss:            0.8869
srnn loss:           0.8336
============================

Saving the model...
done in 388.52 ms
step 99000; step_loss: 0.0908
step 99010; step_loss: 0.0990
step 99020; step_loss: 0.1315
step 99030; step_loss: 0.0947
step 99040; step_loss: 0.0879
step 99050; step_loss: 0.1118
step 99060; step_loss: 0.0903
step 99070; step_loss: 0.1150
step 99080; step_loss: 0.0978
step 99090; step_loss: 0.0951
step 99100; step_loss: 0.0798
step 99110; step_loss: 0.0841
step 99120; step_loss: 0.1114
step 99130; step_loss: 0.0791
step 99140; step_loss: 0.1169
step 99150; step_loss: 0.1167
step 99160; step_loss: 0.0945
step 99170; step_loss: 0.1058
step 99180; step_loss: 0.0812
step 99190; step_loss: 0.0869
step 99200; step_loss: 0.1039
step 99210; step_loss: 0.1639
step 99220; step_loss: 0.1098
step 99230; step_loss: 0.1115
step 99240; step_loss: 0.1132
step 99250; step_loss: 0.1034
step 99260; step_loss: 0.1168
step 99270; step_loss: 0.0904
step 99280; step_loss: 0.1142
step 99290; step_loss: 0.0997
step 99300; step_loss: 0.1236
step 99310; step_loss: 0.0929
step 99320; step_loss: 0.1130
step 99330; step_loss: 0.1169
step 99340; step_loss: 0.0717
step 99350; step_loss: 0.0814
step 99360; step_loss: 0.0976
step 99370; step_loss: 0.0926
step 99380; step_loss: 0.0631
step 99390; step_loss: 0.1041
step 99400; step_loss: 0.1336
step 99410; step_loss: 0.0899
step 99420; step_loss: 0.0887
step 99430; step_loss: 0.1855
step 99440; step_loss: 0.1080
step 99450; step_loss: 0.0948
step 99460; step_loss: 0.1386
step 99470; step_loss: 0.1143
step 99480; step_loss: 0.1086
step 99490; step_loss: 0.1184
step 99500; step_loss: 0.1188
step 99510; step_loss: 0.1079
step 99520; step_loss: 0.1187
step 99530; step_loss: 0.1075
step 99540; step_loss: 0.1126
step 99550; step_loss: 0.1410
step 99560; step_loss: 0.1448
step 99570; step_loss: 0.0856
step 99580; step_loss: 0.1245
step 99590; step_loss: 0.1377
step 99600; step_loss: 0.0787
step 99610; step_loss: 0.1109
step 99620; step_loss: 0.0767
step 99630; step_loss: 0.0829
step 99640; step_loss: 0.1773
step 99650; step_loss: 0.1826
step 99660; step_loss: 0.1277
step 99670; step_loss: 0.1013
step 99680; step_loss: 0.0926
step 99690; step_loss: 0.1023
step 99700; step_loss: 0.1259
step 99710; step_loss: 0.1302
step 99720; step_loss: 0.1315
step 99730; step_loss: 0.1159
step 99740; step_loss: 0.0939
step 99750; step_loss: 0.0931
step 99760; step_loss: 0.1287
step 99770; step_loss: 0.1480
step 99780; step_loss: 0.0810
step 99790; step_loss: 0.1189
step 99800; step_loss: 0.0858
step 99810; step_loss: 0.0954
step 99820; step_loss: 0.1079
step 99830; step_loss: 0.0954
step 99840; step_loss: 0.1132
step 99850; step_loss: 0.1468
step 99860; step_loss: 0.1129
step 99870; step_loss: 0.1629
step 99880; step_loss: 0.1126
step 99890; step_loss: 0.1021
step 99900; step_loss: 0.0760
step 99910; step_loss: 0.1386
step 99920; step_loss: 0.1431
step 99930; step_loss: 0.1286
step 99940; step_loss: 0.0963
step 99950; step_loss: 0.1051
step 99960; step_loss: 0.0959
step 99970; step_loss: 0.1294
step 99980; step_loss: 0.1249
step 99990; step_loss: 0.0989

milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |
walking          | 0.952 | 1.117 | 1.428 | 1.552 | 1.729 | 1.990 |

============================
Global step:         100000
Learning rate:       0.0030
Step-time (ms):     35.8473
Train loss avg:      0.1100
--------------------------
Val loss:            0.9117
srnn loss:           0.8382
============================

Saving the model...
done in 419.02 ms